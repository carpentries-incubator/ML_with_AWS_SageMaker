<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Intro to AWS SageMaker for Predictive ML/AI: Training Models in SageMaker: Intro</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Learn how to run predictive AI/ML procedures (train, tune, etc.) using AWS SageMaker. These examples focus on narrow &amp;quot;predictive ML/AI&amp;quot; cases, where models are trained to perform a single function (contrasing with &amp;quot;foundation&amp;quot; model use via AWS Bedrock). These materials are directed towards participants of the 2024 Machine Learning Marathon, and some instructions may pertain only to that group. A more general purpose version of this workshop will be made available in future months." src="../assets/images/incubator-logo.svg"><span class="badge text-bg-warning">
          <abbr title="This lesson is in the alpha phase, which means that it has been taught once and lesson authors are iterating on feedback.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-triangle" style="border-radius: 5px"></i>
              Alpha
            </a>
            <span class="visually-hidden">This lesson is in the alpha phase, which means that it has been taught once and lesson authors are iterating on feedback.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../Training-models-in-SageMaker-notebooks.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Learn how to run predictive AI/ML procedures (train, tune, etc.) using AWS SageMaker. These examples focus on narrow &amp;quot;predictive ML/AI&amp;quot; cases, where models are trained to perform a single function (contrasing with &amp;quot;foundation&amp;quot; model use via AWS Bedrock). These materials are directed towards participants of the 2024 Machine Learning Marathon, and some instructions may pertain only to that group. A more general purpose version of this workshop will be made available in future months." src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Intro to AWS SageMaker for Predictive ML/AI
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Intro to AWS SageMaker for Predictive ML/AI
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Glossary</a></li><li><a class="dropdown-item" href="instances-for-ML.html">Instances for ML</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Intro to AWS SageMaker for Predictive ML/AI
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 45%" class="percentage">
    45%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 45%" aria-valuenow="45" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../Training-models-in-SageMaker-notebooks.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="SageMaker-overview.html">1. Overview of Amazon SageMaker</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="Data-storage-setting-up-S3.html">2. Data Storage: Setting up S3</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="SageMaker-notebooks-as-controllers.html">3. Notebooks as Controllers</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="Accessing-S3-via-SageMaker-notebooks.html">4. Accessing and Managing Data in S3 with SageMaker Notebooks</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="Interacting-with-code-repo.html">5. Using a GitHub Personal Access Token (PAT) to Push/Pull from a SageMaker Notebook</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        6. Training Models in SageMaker: Intro
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#initial-setup">Initial setup</a></li>
<li><a href="#testing-train.py-on-this-notebooks-instance">Testing train.py on this notebook’s instance</a></li>
<li><a href="#training-via-sagemaker-using-notebook-as-controller---custom-train.py-script">Training via SageMaker (using notebook as controller) - custom
train.py script</a></li>
<li><a href="#training-with-sagemakers-built-in-xgboost-image">Training with SageMaker’s Built-in XGBoost Image</a></li>
<li><a href="#monitoring-training">Monitoring training</a></li>
<li><a href="#when-training-takes-too-long">When training takes too long</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="Training-models-in-SageMaker-notebooks-part2.html">7. Training Models in SageMaker: PyTorch Example</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="Hyperparameter-tuning.html">8. Hyperparameter Tuning in SageMaker: Neural Network Example</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="Resource-management-cleanup.html">9. Resource Management and Monitoring</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Glossary</a></li><li><a class="dropdown-item" href="instances-for-ML.html">Instances for ML</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/Interacting-with-code-repo.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/Training-models-in-SageMaker-notebooks-part2.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/Interacting-with-code-repo.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Using a GitHub
        </a>
        <a class="chapter-link float-end" href="../instructor/Training-models-in-SageMaker-notebooks-part2.html" rel="next">
          Next: Training Models in...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Training Models in SageMaker: Intro</h1>
        <p>Last updated on 2025-03-12 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/Training-models-in-SageMaker-notebooks.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What are the differences between local training and
SageMaker-managed training?</li>
<li>How do Estimator classes in SageMaker streamline the training
process for various frameworks?</li>
<li>How does SageMaker handle data and model parallelism, and when
should each be considered?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand the difference between training locally in a SageMaker
notebook and using SageMaker’s managed infrastructure.</li>
<li>Learn to configure and use SageMaker’s Estimator classes for
different frameworks (e.g., XGBoost, PyTorch, SKLearn).</li>
<li>Understand data and model parallelism options in SageMaker,
including when to use each for efficient training.</li>
<li>Compare performance, cost, and setup between custom scripts and
built-in images in SageMaker.</li>
<li>Conduct training with data stored in S3 and monitor training job
status using the SageMaker console.</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="initial-setup">Initial setup<a class="anchor" aria-label="anchor" href="#initial-setup"></a></h2>
<hr class="half-width"><div class="section level4">
<h4 id="open-a-new--ipynb-notebook">1. Open a new .ipynb notebook<a class="anchor" aria-label="anchor" href="#open-a-new--ipynb-notebook"></a></h4>
<p>Open a fresh .ipynb notebook (“Jupyter notebook”), and select the
conda_pytorch_p310 environment. This will save us the trouble of having
to install pytorch in this notebook. You can name your Jupyter notebook
something along the lines of, <code>Training-models.ipynb</code>.</p>
</div>
<div class="section level4">
<h4 id="cd-to-instance-home-directory">2. CD to instance home directory<a class="anchor" aria-label="anchor" href="#cd-to-instance-home-directory"></a></h4>
<p>So we all can reference the helper functions using the same path, CD
to…</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="op">%</span>cd <span class="op">/</span>home<span class="op">/</span>ec2<span class="op">-</span>user<span class="op">/</span>SageMaker<span class="op">/</span></span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="initialize-sagemaker-environment">3. Initialize SageMaker environment<a class="anchor" aria-label="anchor" href="#initialize-sagemaker-environment"></a></h4>
<p>This code initializes the AWS SageMaker environment by defining the
SageMaker role, session, and S3 client. It also specifies the S3 bucket
and key for accessing the Titanic training dataset stored in an S3
bucket.</p>
</div>
<div class="section level4">
<h4 id="boto3-api">Boto3 API<a class="anchor" aria-label="anchor" href="#boto3-api"></a></h4>
<blockquote>
<p>Boto3 is the official AWS SDK for Python, allowing developers to
interact programmatically with AWS services like S3, EC2, and Lambda. It
provides both high-level and low-level APIs, making it easy to manage
AWS resources and automate tasks. With built-in support for paginators,
waiters, and session management, Boto3 simplifies working with AWS
credentials, regions, and IAM permissions. It’s ideal for automating
cloud operations and integrating AWS services into Python
applications.</p>
</blockquote>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> boto3</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">import</span> sagemaker</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">from</span> sagemaker <span class="im">import</span> get_execution_role</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Initialize the SageMaker role (will reflect notebook instance's policy)</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>role <span class="op">=</span> sagemaker.get_execution_role()</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'role = </span><span class="sc">{</span>role<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># Create a SageMaker session to manage interactions with Amazon SageMaker, such as training jobs, model deployments, and data input/output.</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>session <span class="op">=</span> sagemaker.Session()</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co"># Initialize an S3 client to interact with Amazon S3, allowing operations like uploading, downloading, and managing objects and buckets.</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>s3 <span class="op">=</span> boto3.client(<span class="st">'s3'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co"># Define the S3 bucket that we will load from</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>bucket_name <span class="op">=</span> <span class="st">'doejohn-titanic-s3'</span>  <span class="co"># replace with your S3 bucket name</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co"># Define train/test filenames</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>train_filename <span class="op">=</span> <span class="st">'titanic_train.csv'</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>test_filename <span class="op">=</span> <span class="st">'titanic_test.csv'</span></span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="get-code-from-git-repo-skip-if-completed-already-from-earlier-episodes">4. Get code from git repo (skip if completed already from earlier
episodes)<a class="anchor" aria-label="anchor" href="#get-code-from-git-repo-skip-if-completed-already-from-earlier-episodes"></a></h4>
<p>If you didn’t complete the earlier episodes, you’ll need to clone our
code repo before moving forward. Check to make sure we’re in our EC2
root folder first (<code>/home/ec2-user/SageMaker</code>).</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="op">%</span>cd <span class="op">/</span>home<span class="op">/</span>ec2<span class="op">-</span>user<span class="op">/</span>SageMaker<span class="op">/</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># uncomment below line only if you still need to download the code repo (replace username with your GitHub usernanme)</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="co">#!git clone https://github.com/username/AWS_helpers.git </span></span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="testing-train-py-on-this-notebooks-instance">Testing train.py on this notebook’s instance<a class="anchor" aria-label="anchor" href="#testing-train-py-on-this-notebooks-instance"></a></h2>
<hr class="half-width"><p>In this next section, we will learn how to take a model training
script that was written/designed to run locally, and deploy it to more
powerful instances (or many instances) using SageMaker. This is helpful
for machine learning jobs that require extra power, GPUs, or benefit
from parallelization. However, before we try exploiting this extra
power, it is essential that we test our code thoroughly! We don’t want
to waste unnecessary compute cycles and resources on jobs that produce
bugs rather than insights.</p>
<div class="section level3">
<h3 id="general-guidelines-for-testing-ml-pipelines-before-scaling">General guidelines for testing ML pipelines before scaling<a class="anchor" aria-label="anchor" href="#general-guidelines-for-testing-ml-pipelines-before-scaling"></a></h3>
<ul><li>
<strong>Run tests locally first</strong> (if feasible) to avoid
unnecessary AWS charges. Here, we assume that local tests are not
feasible due to limited local resources. Instead, we use our SageMaker
instance to test our script on a minimally sized EC2 instance.</li>
<li>
<strong>Use a small dataset subset</strong> (e.g., 1-5% of data) to
catch issues early and speed up tests.</li>
<li>
<strong>Start with a small/cheap instance</strong> before committing
to larger resources. Visit the <a href="https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/instances-for-ML.html" class="external-link">Instances
for ML page</a> for guidance.</li>
<li>
<strong>Log everything</strong> to track training times, errors, and
key metrics.</li>
<li>
<strong>Verify correctness first</strong> before optimizing
hyperparameters or scaling.</li>
</ul><div id="what-tests-should-we-do-before-scaling" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="what-tests-should-we-do-before-scaling" class="callout-inner">
<h3 class="callout-title">What tests should we do before scaling?</h3>
<div class="callout-content">
<p>Before scaling to mutliple or more powerful instances (e.g., training
on larger/multiple datsets in parallel or tuning hyperparameters in
parallel), it’s important to run a few quick sanity checks to catch
potential issues early. <strong>In your group, discuss:</strong></p>
<ul><li>Which checks do you think are most critical before scaling up?<br></li>
<li>What potential issues might we miss if we skip this step?</li>
</ul></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Which checks do you think are most critical before scaling up?</p>
<ul><li>
<strong>Data loads correctly</strong> – Ensure the dataset loads
without errors, expected columns exist, and missing values are handled
properly.<br></li>
<li>
<strong>Overfitting check</strong> – Train on a small dataset (e.g.,
100 rows). If it doesn’t overfit, there may be a data or model setup
issue.<br></li>
<li>
<strong>Loss behavior check</strong> – Verify that training loss
decreases over time and doesn’t diverge.<br></li>
<li>
<strong>Training time estimate</strong> – Run on a small subset to
estimate how long full training will take.</li>
<li>
<strong>Memory estimate</strong> - Estimate the memory needs of the
algorithm/model you’re using, and understand how this scales with input
size.</li>
<li>
<strong>Save &amp; reload test</strong> – Ensure the trained model
can be saved, reloaded, and used for inference without errors.</li>
</ul><p>What potential issues might we miss if we skip the above checks?</p>
<ul><li>
<strong>Silent data issues</strong> – Missing values, unexpected
distributions, or incorrect labels could degrade model
performance.<br></li>
<li>
<strong>Code bugs at scale</strong> – Small logic errors might not
break on small tests but could fail with larger datasets.<br></li>
<li>
<strong>Inefficient training runs</strong> – Without estimating
runtime, jobs may take far longer than expected, wasting AWS
resources.<br></li>
<li>
<strong>Memory or compute failures</strong> – Large datasets might
exceed instance memory limits, causing crashes or slowdowns.<br></li>
<li>
<strong>Model performance issues</strong> – If a model doesn’t
overfit a small dataset, there may be problems with features, training
logic, or hyperparameters.</li>
</ul></div>
</div>
</div>
</div>
<div id="know-your-data-before-modeling" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="know-your-data-before-modeling" class="callout-inner">
<h3 class="callout-title"><strong>Know Your Data Before
Modeling</strong></h3>
<div class="callout-content">
<p>The sanity checks above focus on validating the code, but a model is
only as good as the data it’s trained on. A deeper look at feature
distributions, correlations, and potential biases is critical before
scaling up. We won’t cover that here, but it’s essential to keep in mind
for any ML/AI practitioner.</p>
</div>
</div>
</div>
<div id="understanding-the-xgboost-training-script" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="understanding-the-xgboost-training-script" class="callout-inner">
<h3 class="callout-title">Understanding the XGBoost Training Script</h3>
<div class="callout-content">
<p>Take a moment to review the <code>AWS_helpers/train_xgboost.py</code>
script we just cloned into our notebook. This script handles
preprocessing, training, and saving an XGBoost model, while also
adapting to both local and SageMaker-managed environments.</p>
<p>Try answering the following questions:</p>
<ol style="list-style-type: decimal"><li><p><strong>Data Preprocessing</strong>: What transformations are
applied to the dataset before training?</p></li>
<li><p><strong>Training Function</strong>: What does the
<code>train_model()</code> function do? Why do we print the training
time?</p></li>
<li><p><strong>Command-Line Arguments</strong>: What is the purpose of
<code>argparse</code> in this script? How would you modify the script if
you wanted to change the number of training rounds?</p></li>
<li><p><strong>Handling Local vs. SageMaker Runs</strong>: How does the
script determine whether it is running in a SageMaker training job or
locally (within this notebook’s instance)?</p></li>
<li><p><strong>Training and Saving the Model</strong>: What format is
the dataset converted to before training, and why? How is the trained
model saved, and where will it be stored?</p></li>
</ol><p>After reviewing, discuss any questions or observations with your
group.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li><p><strong>Data Preprocessing</strong>: The script fills missing
values (<code>Age</code> with median, <code>Embarked</code> with mode),
converts categorical variables (<code>Sex</code> and
<code>Embarked</code>) to numerical values, and removes columns that
don’t contribute to prediction (<code>Name</code>, <code>Ticket</code>,
<code>Cabin</code>).</p></li>
<li><p><strong>Training Function</strong>: The
<code>train_model()</code> function takes the training dataset
(<code>dtrain</code>), applies XGBoost training with the specified
hyperparameters, and prints the training time. Printing training time
helps compare different runs and ensures that scaling decisions are
based on performance metrics.</p></li>
<li><p><strong>Command-Line Arguments</strong>: <code>argparse</code>
allows passing parameters like <code>max_depth</code>, <code>eta</code>,
<code>num_round</code>, etc., at runtime without modifying the script.
To change the number of training rounds, you would update the
<code>--num_round</code> argument when running the script:
<code>python train_xgboost.py --num_round 200</code></p></li>
<li><p><strong>Handling Local vs. SageMaker Runs</strong>: The script
uses <code>os.environ.get("SM_CHANNEL_TRAIN", ".")</code> and
<code>os.environ.get("SM_MODEL_DIR", ".")</code> to detect whether it’s
running in SageMaker. <code>SM_CHANNEL_TRAIN</code> is the directory
where SageMaker stores input training data, and
<code>SM_MODEL_DIR</code> is the directory where trained models should
be saved. If these environment variables are <em>not set</em> (e.g.,
running locally), the script defaults to <code>"."</code> (current
directory).</p></li>
<li><p><strong>Training and Saving the Model</strong>: The dataset is
converted into <strong>XGBoost’s <code>DMatrix</code> format</strong>,
which is optimized for memory and computation efficiency. The trained
model is saved using <code>joblib.dump()</code> to
<code>xgboost-model</code>, stored either in the SageMaker
<code>SM_MODEL_DIR</code> (if running in SageMaker) or in the local
directory.</p></li>
</ol></div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="download-data-into-notebook-environment">Download data into notebook environment<a class="anchor" aria-label="anchor" href="#download-data-into-notebook-environment"></a></h3>
<p>It can be convenient to have a copy of the data (i.e., one that you
store in your notebook’s instance) to allow us to test our code before
scaling things up.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout</h3>
<div class="callout-content">
<p>While we demonstrate how to download data into the notebook
environment for testing our code (previously setup for local ML
pipelines), keep in mind that S3 is the preferred location for dataset
storage in a scalable ML pipeline.</p>
</div>
</div>
</div>
<p>Run the next code chunk to download data from S3 to notebook
environment. You may need to hit refresh on the file explorer panel to
the left to see this file. If you get any permission issues…</p>
<ul><li>check that you have selected the appropriate policy for this
notebook</li>
<li>check that your bucket has the appropriate policy permissions</li>
</ul><div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Define the S3 bucket and file location</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>file_key <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>train_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Path to your file in the S3 bucket</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>local_file_path <span class="op">=</span> <span class="ss">f"./</span><span class="sc">{</span>train_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Local path to save the file</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co"># Download the file using the s3 client variable we initialized earlier</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>s3.download_file(bucket_name, file_key, local_file_path)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"File downloaded:"</span>, local_file_path)</span></code></pre>
</div>
<p>We can do the same for the test set.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Define the S3 bucket and file location</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>file_key <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>test_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Path to your file in the S3 bucket. W</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>local_file_path <span class="op">=</span> <span class="ss">f"./</span><span class="sc">{</span>test_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Local path to save the file</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="co"># Initialize the S3 client and download the file</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>s3.download_file(bucket_name, file_key, local_file_path)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"File downloaded:"</span>, local_file_path)</span></code></pre>
</div>
<div class="section level4">
<h4 id="logging-runtime-instance-info">Logging runtime &amp; instance info<a class="anchor" aria-label="anchor" href="#logging-runtime-instance-info"></a></h4>
<p>To compare our local runtime with future experiments, we’ll need to
know what instance was used, as this will greatly impact runtime in many
cases. We can extract the instance name for this notebook using…</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Replace with your notebook instance name.</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># This does NOT refer to specific ipynb files, but to the SageMaker notebook instance.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>notebook_instance_name <span class="op">=</span> <span class="st">'DoeJohn-ExploreSageMaker'</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># Initialize SageMaker client</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>sagemaker_client <span class="op">=</span> boto3.client(<span class="st">'sagemaker'</span>)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Describe the notebook instance</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>response <span class="op">=</span> sagemaker_client.describe_notebook_instance(NotebookInstanceName<span class="op">=</span>notebook_instance_name)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="co"># Display the status and instance type</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Notebook Instance '</span><span class="sc">{</span>notebook_instance_name<span class="sc">}</span><span class="ss">' status: </span><span class="sc">{</span>response[<span class="st">'NotebookInstanceStatus'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>local_instance <span class="op">=</span> response[<span class="st">'InstanceType'</span>]</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Instance Type: </span><span class="sc">{</span>local_instance<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="helper-get_notebook_instance_info">Helper: <code>get_notebook_instance_info()</code>
<a class="anchor" aria-label="anchor" href="#helper-get_notebook_instance_info"></a></h4>
<p>You can also use the <code>get_notebook_instance_info()</code>
function found in <code>AWS_helpers.py</code> to retrieve this info for
your own project.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> AWS_helpers.helpers <span class="im">as</span> helpers</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>helpers.get_notebook_instance_info(notebook_instance_name)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="ex">{</span><span class="st">'Status'</span><span class="ex">:</span> <span class="st">'InService'</span>, <span class="st">'InstanceType'</span>: <span class="st">'ml.t3.medium'</span>}</span></code></pre>
</div>
<p>Test train.py on this notebook’s instance (or when possible, on your
own machine) before doing anything more complicated (e.g.,
hyperparameter tuning on multiple instances)</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="op">!</span>pip install xgboost <span class="co"># need to add this to environment to run train.py</span></span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="local-test">Local test<a class="anchor" aria-label="anchor" href="#local-test"></a></h3>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> time <span class="im">as</span> t <span class="co"># we'll use the time package to measure runtime</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>start_time <span class="op">=</span> t.time()</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># Define your parameters. These python vars wil be passed as input args to our train_xgboost.py script using %run</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>max_depth <span class="op">=</span> <span class="dv">3</span> <span class="co"># Sets the maximum depth of each tree in the model to 3. Limiting tree depth helps control model complexity and can reduce overfitting, especially on small datasets.</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.1</span> <span class="co">#  Sets the learning rate to 0.1, which scales the contribution of each tree to the final model. A smaller learning rate often requires more rounds to converge but can lead to better performance.</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>subsample <span class="op">=</span> <span class="fl">0.8</span> <span class="co"># Specifies that 80% of the training data will be randomly sampled to build each tree. Subsampling can help with model robustness by preventing overfitting and increasing variance.</span></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>colsample_bytree <span class="op">=</span> <span class="fl">0.8</span> <span class="co"># Specifies that 80% of the features will be randomly sampled for each tree, enhancing the model's ability to generalize by reducing feature reliance.</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>num_round <span class="op">=</span> <span class="dv">100</span> <span class="co"># Sets the number of boosting rounds (trees) to 100. More rounds typically allow for a more refined model, but too many rounds can lead to overfitting.</span></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>train_file <span class="op">=</span> <span class="st">'titanic_train.csv'</span> <span class="co">#  Points to the location of the training data</span></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a><span class="co"># Use f-strings to format the command with your variables</span></span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a><span class="op">%</span>run AWS_helpers<span class="op">/</span>train_xgboost.py <span class="op">--</span>max_depth {max_depth} <span class="op">--</span>eta {eta} <span class="op">--</span>subsample {subsample} <span class="op">--</span>colsample_bytree {colsample_bytree} <span class="op">--</span>num_round {num_round} <span class="op">--</span>train {train_file}</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a><span class="co"># Measure and print the time taken</span></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total local runtime: </span><span class="sc">{</span>t<span class="sc">.</span>time() <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type = </span><span class="sc">{</span>local_instance<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>Training on this relatively small dataset should take less than a
minute, but as we scale up with larger datasets and more complex models
in SageMaker, tracking both training time and total runtime becomes
essential for efficient debugging and resource management.</p>
<p><strong>Note</strong>: Our code above includes print statements to
monitor dataset size, training time, and total runtime, which provides
insights into resource usage for model development. We recommend
incorporating similar logging to track not only training time but also
total runtime, which includes additional steps like data loading,
evaluation, and saving results. Tracking both can help you pinpoint
bottlenecks and optimize your workflow as projects grow in size and
complexity, especially when scaling with SageMaker’s distributed
resources.</p>
</div>
<div class="section level3">
<h3 id="sanity-check-quick-evaluation-on-test-set">Sanity check: Quick evaluation on test set<a class="anchor" aria-label="anchor" href="#sanity-check-quick-evaluation-on-test-set"></a></h3>
<p>This next section isn’t SageMaker specific, but it does serve as a
good sanity check to ensure our model is training properly. Here’s how
you would apply the outputted model to your test set using your local
notebook instance.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="im">from</span> AWS_helpers.train_xgboost <span class="im">import</span> preprocess_data</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co"># Load the test data</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>test_data <span class="op">=</span> pd.read_csv(<span class="st">'./titanic_test.csv'</span>)</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="co"># Preprocess the test data using the imported preprocess_data function</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>X_test, y_test <span class="op">=</span> preprocess_data(test_data)</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a><span class="co"># Convert the test features to DMatrix for XGBoost</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>dtest <span class="op">=</span> xgb.DMatrix(X_test)</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a><span class="co"># Load the trained model from the saved file</span></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>model <span class="op">=</span> joblib.load(<span class="st">'./xgboost-model'</span>)</span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>preds <span class="op">=</span> model.predict(dtest)</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a>predictions <span class="op">=</span> np.<span class="bu">round</span>(preds)  <span class="co"># Round predictions to 0 or 1 for binary classification</span></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a><span class="co"># Calculate and print the accuracy of the model on the test data</span></span>
<span id="cb12-25"><a href="#cb12-25" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, predictions)</span>
<span id="cb12-26"><a href="#cb12-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Set Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>A reasonably high test set accuracy suggests our code/model is
working correctly.</p>
</div>
</section><section><h2 class="section-heading" id="training-via-sagemaker-using-notebook-as-controller---custom-train-py-script">Training via SageMaker (using notebook as controller) - custom
train.py script<a class="anchor" aria-label="anchor" href="#training-via-sagemaker-using-notebook-as-controller---custom-train-py-script"></a></h2>
<hr class="half-width"><p>Unlike “local” training (using this notebook), this next approach
leverages SageMaker’s managed infrastructure to handle resources,
parallelism, and scalability. By specifying instance parameters, such as
instance_count and instance_type, you can control the resources
allocated for training.</p>
<div class="section level3">
<h3 id="which-instance-to-start-with">Which instance to start with?<a class="anchor" aria-label="anchor" href="#which-instance-to-start-with"></a></h3>
<p>In this example, we start with one ml.m5.large instance, which is
suitable for small- to medium-sized datasets and simpler models. Using a
single instance is often cost-effective and sufficient for initial
testing, allowing for straightforward scaling up to more powerful
instance types or multiple instances if training takes too long. See
here for further guidance on selecting an appropriate instance for your
data/model: <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">EC2
Instances for ML</a></p>
</div>
<div class="section level3">
<h3 id="overview-of-estimator-classes-in-sagemaker">Overview of Estimator classes in SageMaker<a class="anchor" aria-label="anchor" href="#overview-of-estimator-classes-in-sagemaker"></a></h3>
<p>To launch this training “job”, we’ll use the XGBoost “Estimator. In
SageMaker, Estimator classes streamline the configuration and training
of models on managed instances. Each Estimator can work with custom
scripts and be enhanced with additional dependencies by specifying a
<code>requirements.txt</code> file, which is automatically installed at
the start of training. Here’s a breakdown of some commonly used
Estimator classes in SageMaker:</p>
<div class="section level4">
<h4 id="estimator-base-class">1. <strong><code>Estimator</code> (Base Class)</strong>
<a class="anchor" aria-label="anchor" href="#estimator-base-class"></a></h4>
<ul><li>
<strong>Purpose</strong>: General-purpose for custom Docker
containers or defining an image URI directly.</li>
<li>
<strong>Configuration</strong>: Requires specifying an
<code>image_uri</code> and custom entry points.</li>
<li>
<strong>Dependencies</strong>: You can use
<code>requirements.txt</code> to install Python packages or configure a
custom Docker container with pre-baked dependencies.</li>
<li>
<strong>Ideal Use Cases</strong>: Custom algorithms or models that
need tailored environments not covered by built-in containers.</li>
</ul></div>
<div class="section level4">
<h4 id="xgboost-estimator">2. <strong><code>XGBoost</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#xgboost-estimator"></a></h4>
<ul><li>
<strong>Purpose</strong>: Provides an optimized container
specifically for XGBoost models.</li>
<li>
<strong>Configuration</strong>:
<ul><li>
<code>entry_point</code>: Path to a custom script, useful for
additional preprocessing or unique training workflows.</li>
<li>
<code>framework_version</code>: Select XGBoost version, e.g.,
<code>"1.5-1"</code>.</li>
<li>
<code>dependencies</code>: Specify additional packages through
<code>requirements.txt</code> to enhance preprocessing capabilities or
incorporate auxiliary libraries.</li>
</ul></li>
<li>
<strong>Ideal Use Cases</strong>: Tabular data modeling using
gradient-boosted trees; cases requiring custom preprocessing or tuning
logic.</li>
</ul></div>
<div class="section level4">
<h4 id="pytorch-estimator">3. <strong><code>PyTorch</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#pytorch-estimator"></a></h4>
<ul><li>
<strong>Purpose</strong>: Configures training jobs with PyTorch for
deep learning tasks.</li>
<li>
<strong>Configuration</strong>:
<ul><li>
<code>entry_point</code>: Training script with model architecture
and training loop.</li>
<li>
<code>instance_type</code>: e.g., <code>ml.p3.2xlarge</code> for GPU
acceleration.</li>
<li>
<code>framework_version</code> and <code>py_version</code>: Define
specific versions.</li>
<li>
<code>dependencies</code>: Install any required packages via
<code>requirements.txt</code> to support advanced data processing, data
augmentation, or custom layer implementations.</li>
</ul></li>
<li>
<strong>Ideal Use Cases</strong>: Deep learning models, particularly
complex networks requiring GPUs and custom layers.</li>
</ul></div>
<div class="section level4">
<h4 id="sklearn-estimator">4. <strong><code>SKLearn</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#sklearn-estimator"></a></h4>
<ul><li>
<strong>Purpose</strong>: Supports scikit-learn workflows for data
preprocessing and classical machine learning.</li>
<li>
<strong>Configuration</strong>:
<ul><li>
<code>entry_point</code>: Python script to handle feature
engineering, preprocessing, or training.</li>
<li>
<code>framework_version</code>: Version of scikit-learn, e.g.,
<code>"1.0-1"</code>.</li>
<li>
<code>dependencies</code>: Use <code>requirements.txt</code> to
install any additional Python packages required by the training
script.</li>
</ul></li>
<li>
<strong>Ideal Use Cases</strong>: Classical ML workflows, extensive
preprocessing, or cases where additional libraries (e.g., pandas, numpy)
are essential.</li>
</ul></div>
<div class="section level4">
<h4 id="tensorflow-estimator">5. <strong><code>TensorFlow</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#tensorflow-estimator"></a></h4>
<ul><li>
<strong>Purpose</strong>: Designed for training and deploying
TensorFlow models.</li>
<li>
<strong>Configuration</strong>:
<ul><li>
<code>entry_point</code>: Script for model definition and training
process.</li>
<li>
<code>instance_type</code>: Select based on dataset size and
computational needs.</li>
<li>
<code>dependencies</code>: Additional dependencies can be listed in
<code>requirements.txt</code> to install TensorFlow add-ons, custom
layers, or preprocessing libraries.</li>
</ul></li>
<li>
<strong>Ideal Use Cases</strong>: NLP, computer vision, and transfer
learning applications in TensorFlow.</li>
</ul><div id="configuring-custom-environments-with-requirements.txt" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="configuring-custom-environments-with-requirements.txt" class="callout-inner">
<h3 class="callout-title">Configuring custom environments with
<code>requirements.txt</code>
</h3>
<div class="callout-content">
<p>For all these Estimators, adding a <code>requirements.txt</code> file
under <code>dependencies</code> ensures that additional packages are
installed before training begins. This approach allows the use of
specific libraries that may be critical for custom preprocessing,
feature engineering, or model modifications. Here’s how to include
it:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Customizing estimator using requirements.txt</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">from</span> sagemaker.sklearn.estimator <span class="im">import</span> SKLearn</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>sklearn_estimator <span class="op">=</span> SKLearn(</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">"train_script.py"</span>,</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    instance_count<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span>,</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>    output_path<span class="op">=</span><span class="st">"s3://your-bucket/output"</span>,</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.0-1"</span>,</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>    dependencies<span class="op">=</span>[<span class="st">'requirements.txt'</span>],  <span class="co"># Adding custom dependencies</span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>        <span class="st">"max_depth"</span>: <span class="dv">5</span>,</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>        <span class="st">"eta"</span>: <span class="fl">0.1</span>,</span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>        <span class="st">"subsample"</span>: <span class="fl">0.8</span>,</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>        <span class="st">"num_round"</span>: <span class="dv">100</span></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>    }</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>)</span></code></pre>
</div>
<p>This setup simplifies training, allowing you to maintain custom
environments directly within SageMaker’s managed containers, without
needing to build and manage your own Docker images. The <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html" class="external-link">AWS
SageMaker Documentation</a> provides lists of pre-built container images
for each framework and their standard libraries, including details on
pre-installed packages.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="deploying-to-other-instances">Deploying to other instances<a class="anchor" aria-label="anchor" href="#deploying-to-other-instances"></a></h3>
<p>For this deployment, we configure the “XGBoost” estimator with a
custom training script, train_xgboost.py, and define hyperparameters
directly within the SageMaker setup. Here’s the full code, with some
additional explanation following the code.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> sagemaker.inputs <span class="im">import</span> TrainingInput</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="im">from</span> sagemaker.xgboost.estimator <span class="im">import</span> XGBoost</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="co"># Define instance type/count we'll use for training</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>instance_count<span class="op">=</span><span class="dv">1</span> <span class="co"># always start with 1. Rarely is parallelized training justified with data &lt; 50 GB. More on this later.</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="co"># Define S3 paths for input and output</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>train_s3_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket_name<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>train_filename<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a><span class="co"># we'll store all results in a subfolder called xgboost on our bucket. This folder will automatically be created if it doesn't exist already.</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>output_folder <span class="op">=</span> <span class="st">'xgboost'</span></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>output_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket_name<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>output_folder<span class="sc">}</span><span class="ss">/'</span> </span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a><span class="co"># Set up the SageMaker XGBoost Estimator with custom script</span></span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>xgboost_estimator <span class="op">=</span> XGBoost(</span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">'train_xgboost.py'</span>,      <span class="co"># Custom script path</span></span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>    source_dir<span class="op">=</span><span class="st">'AWS_helpers'</span>,               <span class="co"># Directory where your script is located</span></span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,</span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.5-1"</span>,           <span class="co"># Use latest supported version for better compatibility</span></span>
<span id="cb14-25"><a href="#cb14-25" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb14-26"><a href="#cb14-26" tabindex="-1"></a>        <span class="st">'train'</span>: train_file,</span>
<span id="cb14-27"><a href="#cb14-27" tabindex="-1"></a>        <span class="st">'max_depth'</span>: max_depth,</span>
<span id="cb14-28"><a href="#cb14-28" tabindex="-1"></a>        <span class="st">'eta'</span>: eta,</span>
<span id="cb14-29"><a href="#cb14-29" tabindex="-1"></a>        <span class="st">'subsample'</span>: subsample,</span>
<span id="cb14-30"><a href="#cb14-30" tabindex="-1"></a>        <span class="st">'colsample_bytree'</span>: colsample_bytree,</span>
<span id="cb14-31"><a href="#cb14-31" tabindex="-1"></a>        <span class="st">'num_round'</span>: num_round</span>
<span id="cb14-32"><a href="#cb14-32" tabindex="-1"></a>    }</span>
<span id="cb14-33"><a href="#cb14-33" tabindex="-1"></a>)</span>
<span id="cb14-34"><a href="#cb14-34" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" tabindex="-1"></a><span class="co"># Define input data</span></span>
<span id="cb14-36"><a href="#cb14-36" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(train_s3_path, content_type<span class="op">=</span><span class="st">'csv'</span>)</span>
<span id="cb14-37"><a href="#cb14-37" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" tabindex="-1"></a><span class="co"># Measure and start training time</span></span>
<span id="cb14-39"><a href="#cb14-39" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb14-40"><a href="#cb14-40" tabindex="-1"></a>xgboost_estimator.fit({<span class="st">'train'</span>: train_input})</span>
<span id="cb14-41"><a href="#cb14-41" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb14-42"><a href="#cb14-42" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>When running longer training jobs, you can check on their status
periodically from the AWS SageMaker Console (where we originally
launched our Notebook instance) on left side panel under “Training”.</p>
<div class="section level4">
<h4 id="hyperparameters">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a></h4>
<p>The <code>hyperparameters</code> section in this code defines the
input arguments of train_XGBoost.py. The first is the name of the
training input file, and the others are hyperparameters for the XGBoost
model, such as <code>max_depth</code>, <code>eta</code>,
<code>subsample</code>, <code>colsample_bytree</code>, and
<code>num_round</code>.</p>
</div>
<div class="section level4">
<h4 id="traininginput">TrainingInput<a class="anchor" aria-label="anchor" href="#traininginput"></a></h4>
<p>Additionally, we define a TrainingInput object containing the
training data’s S3 path, to pass to
<code>.fit({'train': train_input})</code>. SageMaker uses
<code>TrainingInput</code> to download your dataset from S3 to a
temporary location on the training instance. This location is mounted
and managed by SageMaker and can be accessed by the training job if/when
needed.</p>
</div>
<div class="section level4">
<h4 id="model-results">Model results<a class="anchor" aria-label="anchor" href="#model-results"></a></h4>
<p>With this code, the training results and model artifacts are saved in
a subfolder called <code>xgboost</code> in your specified S3 bucket.
This folder (<code>s3://{bucket_name}/xgboost/</code>) will be
automatically created if it doesn’t already exist, and will contain:</p>
<ol style="list-style-type: decimal"><li>
<strong>Model “artifacts”</strong>: The trained model file (often a
<code>.tar.gz</code> file) that SageMaker saves in the
<code>output_path</code>.</li>
<li>
<strong>Logs and metrics</strong>: Any metrics and logs related to
the training job, stored in the same <code>xgboost</code> folder.</li>
</ol><p>This setup allows for convenient access to both the trained model and
related output for later evaluation or deployment.</p>
</div>
</div>
<div class="section level3">
<h3 id="extracting-trained-model-from-s3-for-final-evaluation">Extracting trained model from S3 for final evaluation<a class="anchor" aria-label="anchor" href="#extracting-trained-model-from-s3-for-final-evaluation"></a></h3>
<p>To evaluate the model on a test set after training, we’ll go through
these steps:</p>
<ol style="list-style-type: decimal"><li>
<strong>Download the trained model from S3</strong>.</li>
<li>
<strong>Load and preprocess</strong> the test dataset.</li>
<li>
<strong>Evaluate</strong> the model on the test data.</li>
</ol><p>Here’s how you can implement this in your SageMaker notebook. The
following code will:</p>
<ul><li>Download the <code>model.tar.gz</code> file containing the trained
model from S3.</li>
<li>Load the <code>test.csv</code> data from S3 and preprocess it as
needed.</li>
<li>Use the XGBoost model to make predictions on the test set and then
compute accuracy or other metrics on the results.</li>
</ul><p>If additional metrics or custom evaluation steps are needed, you can
add them in place of or alongside accuracy.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># Model results are saved in auto-generated folders. Use xgboost_estimator.latest_training_job.name to get the folder name</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>model_s3_path <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>output_folder<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>xgboost_estimator<span class="sc">.</span>latest_training_job<span class="sc">.</span>name<span class="sc">}</span><span class="ss">/output/model.tar.gz'</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="bu">print</span>(model_s3_path)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>local_model_path <span class="op">=</span> <span class="st">'model.tar.gz'</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co"># Download the trained model from S3</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>s3.download_file(bucket_name, model_s3_path, local_model_path)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="co"># Extract the model file</span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a><span class="im">import</span> tarfile</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a><span class="cf">with</span> tarfile.<span class="bu">open</span>(local_model_path) <span class="im">as</span> tar:</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>    tar.extractall()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="im">from</span> AWS_helpers.train_xgboost <span class="im">import</span> preprocess_data</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a><span class="co"># Load the test data</span></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>test_data <span class="op">=</span> pd.read_csv(<span class="st">'./titanic_test.csv'</span>)</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a><span class="co"># Preprocess the test data using the imported preprocess_data function</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>X_test, y_test <span class="op">=</span> preprocess_data(test_data)</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a><span class="co"># Convert the test features to DMatrix for XGBoost</span></span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>dtest <span class="op">=</span> xgb.DMatrix(X_test)</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a><span class="co"># Load the trained model from the saved file</span></span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>model <span class="op">=</span> joblib.load(<span class="st">'./xgboost-model'</span>)</span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a>preds <span class="op">=</span> model.predict(dtest)</span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a>predictions <span class="op">=</span> np.<span class="bu">round</span>(preds)  <span class="co"># Round predictions to 0 or 1 for binary classification</span></span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" tabindex="-1"></a><span class="co"># Calculate and print the accuracy of the model on the test data</span></span>
<span id="cb16-25"><a href="#cb16-25" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, predictions)</span>
<span id="cb16-26"><a href="#cb16-26" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Set Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>Now that we’ve covered training using a custom script with the
<code>XGBoost</code> estimator, let’s examine the built-in image-based
approach. Using SageMaker’s pre-configured XGBoost image streamlines the
setup by eliminating the need to manage custom scripts for common
workflows, and it can also provide optimization advantages. Below, we’ll
discuss both the code and pros and cons of the image-based setup
compared to the custom script approach.</p>
</div>
</section><section><h2 class="section-heading" id="training-with-sagemakers-built-in-xgboost-image">Training with SageMaker’s Built-in XGBoost Image<a class="anchor" aria-label="anchor" href="#training-with-sagemakers-built-in-xgboost-image"></a></h2>
<hr class="half-width"><p>With the SageMaker-provided XGBoost container, you can bypass custom
script configuration if your workflow aligns with standard XGBoost
training. This setup is particularly useful when you need quick, default
configurations without custom preprocessing or additional libraries.</p>
<div class="section level3">
<h3 id="comparison-custom-script-vs--built-in-image">Comparison: Custom Script vs. Built-in Image<a class="anchor" aria-label="anchor" href="#comparison-custom-script-vs--built-in-image"></a></h3>
<table class="table"><colgroup><col width="20%"><col width="41%"><col width="38%"></colgroup><thead><tr class="header"><th>Feature</th>
<th>Custom Script (<code>XGBoost</code> with
<code>entry_point</code>)</th>
<th>Built-in XGBoost Image</th>
</tr></thead><tbody><tr class="odd"><td><strong>Flexibility</strong></td>
<td>Allows for custom preprocessing, data transformation, or advanced
parameterization. Requires a Python script and custom dependencies can
be added through <code>requirements.txt</code>.</td>
<td>Limited to XGBoost’s built-in functionality, no custom preprocessing
steps without additional customization.</td>
</tr><tr class="even"><td><strong>Simplicity</strong></td>
<td>Requires setting up a script with <code>entry_point</code> and
managing dependencies. Ideal for specific needs but requires
configuration.</td>
<td>Streamlined for fast deployment without custom code. Simple setup
and no need for custom scripts.</td>
</tr><tr class="odd"><td><strong>Performance</strong></td>
<td>Similar performance, though potential for overhead with additional
preprocessing.</td>
<td>Optimized for typical XGBoost tasks with faster startup. May offer
marginally faster time-to-first-train.</td>
</tr><tr class="even"><td><strong>Use Cases</strong></td>
<td>Ideal for complex workflows requiring unique preprocessing steps or
when adding specific libraries or functionalities.</td>
<td>Best for quick experiments, standard workflows, or initial testing
on datasets without complex preprocessing.</td>
</tr></tbody></table><p><strong>When to use each approach</strong>:</p>
<ul><li>
<strong>Custom script</strong>: Recommended if you need to implement
custom data preprocessing, advanced feature engineering, or specific
workflow steps that require more control over training.</li>
<li>
<strong>Built-in image</strong>: Ideal when running standard XGBoost
training, especially for quick experiments or production deployments
where default configurations suffice.</li>
</ul><p>Both methods offer powerful and flexible approaches to model training
on SageMaker, allowing you to select the approach best suited to your
needs. Below is an example of training using the built-in XGBoost
Image.</p>
<div class="section level4">
<h4 id="setting-up-the-data-path">Setting up the data path<a class="anchor" aria-label="anchor" href="#setting-up-the-data-path"></a></h4>
<p>In this approach, using <code>TrainingInput</code> directly with
SageMaker’s built-in XGBoost container contrasts with our previous
method, where we specified a custom script with argument inputs
(specified in hyperparameters) for data paths and settings. Here, we use
hyperparameters only to specify the model’s hyperparameters.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> sagemaker.estimator <span class="im">import</span> Estimator <span class="co"># when using images, we use the general Estimator class</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="co"># Define instance type/count we'll use for training</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>instance_count<span class="op">=</span><span class="dv">1</span> <span class="co"># always start with 1. Rarely is parallelized training justified with data &lt; 50 GB. More on this later.</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="co"># Use Estimator directly for built-in container without specifying entry_point</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>xgboost_estimator_builtin <span class="op">=</span> Estimator(</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    image_uri<span class="op">=</span>sagemaker.image_uris.retrieve(<span class="st">"xgboost"</span>, session.boto_region_name, version<span class="op">=</span><span class="st">"1.5-1"</span>),</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a>        <span class="st">'max_depth'</span>: max_depth,</span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>        <span class="st">'eta'</span>: eta,</span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>        <span class="st">'subsample'</span>: subsample,</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a>        <span class="st">'colsample_bytree'</span>: colsample_bytree,</span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a>        <span class="st">'num_round'</span>: num_round</span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>    }</span>
<span id="cb17-22"><a href="#cb17-22" tabindex="-1"></a>)</span>
<span id="cb17-23"><a href="#cb17-23" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" tabindex="-1"></a><span class="co"># Define input data</span></span>
<span id="cb17-25"><a href="#cb17-25" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(train_s3_path, content_type<span class="op">=</span><span class="st">"csv"</span>)</span>
<span id="cb17-26"><a href="#cb17-26" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" tabindex="-1"></a><span class="co"># Measure and start training time</span></span>
<span id="cb17-28"><a href="#cb17-28" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb17-29"><a href="#cb17-29" tabindex="-1"></a>xgboost_estimator_builtin.fit({<span class="st">'train'</span>: train_input})</span>
<span id="cb17-30"><a href="#cb17-30" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb17-31"><a href="#cb17-31" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="monitoring-training">Monitoring training<a class="anchor" aria-label="anchor" href="#monitoring-training"></a></h2>
<hr class="half-width"><p>To view and monitor your SageMaker training job, follow these steps
in the AWS Management Console. Since training jobs may be visible to
multiple users in your account, it’s essential to confirm that you’re
interacting with your own job before making any changes.</p>
<ol style="list-style-type: decimal"><li>
<strong>Navigate to the SageMaker Console</strong>
<ul><li>Go to the AWS Management Console and open the
<strong>SageMaker</strong> service (can search for it)</li>
</ul></li>
<li>
<strong>View training jobs</strong>
<ul><li>In the left-hand navigation menu, select <strong>Training
jobs</strong>. You’ll see a list of recent training jobs, which may
include jobs from other users in the account.</li>
</ul></li>
<li>
<strong>Verify your training Job</strong>
<ul><li>Identify your job by looking for the specific name format (e.g.,
<code>sagemaker-xgboost-YYYY-MM-DD-HH-MM-SS-XXX</code>) generated when
you launched the job. Click on its name to access detailed information.
Cross-check the job details, such as the <strong>Instance Type</strong>
and <strong>Input data configuration</strong>, with the parameters you
set in your script.</li>
</ul></li>
<li>
<strong>Monitor the job status</strong>
<ul><li>Once you’ve verified the correct job, click on its name to access
detailed information:
<ul><li>
<strong>Status</strong>: Confirms whether the job is
<code>InProgress</code>, <code>Completed</code>, or
<code>Failed</code>.</li>
<li>
<strong>Logs</strong>: Review CloudWatch Logs and Job Metrics for
real-time updates.</li>
<li>
<strong>Output Data</strong>: Shows the S3 location with the trained
model artifacts.</li>
</ul></li>
</ul></li>
<li>
<strong>Stopping a training job</strong>
<ul><li>Before stopping a job, ensure you’ve selected the correct one by
verifying job details as outlined above.</li>
<li>If you’re certain it’s your job, go to <strong>Training
jobs</strong> in the SageMaker Console, select the job, and choose
<strong>Stop</strong> from the <strong>Actions</strong> menu. Confirm
your selection, as this action will halt the job and release any
associated resources.</li>
<li>
<strong>Important</strong>: Avoid stopping jobs you don’t own, as
this could disrupt other users’ work and may have unintended
consequences.</li>
</ul></li>
</ol><p>Following these steps helps ensure you only interact with and modify
jobs you own, reducing the risk of impacting other users’ training
processes.</p>
</section><section><h2 class="section-heading" id="when-training-takes-too-long">When training takes too long<a class="anchor" aria-label="anchor" href="#when-training-takes-too-long"></a></h2>
<hr class="half-width"><p>When training time becomes excessive, two main options can improve
efficiency in SageMaker:</p>
<ul><li>
<strong>Option 1: Upgrading to a more powerful
instance</strong><br></li>
<li><strong>Option 2: Using multiple instances for distributed
training</strong></li>
</ul><p>Generally, Option 1 is the preferred approach and should be explored
first.</p>
<div class="section level3">
<h3 id="option-1-upgrade-to-a-more-powerful-instance-preferred-starting-point">Option 1: Upgrade to a more powerful instance (preferred starting
point)<a class="anchor" aria-label="anchor" href="#option-1-upgrade-to-a-more-powerful-instance-preferred-starting-point"></a></h3>
<p>Upgrading to a more capable instance, particularly one with GPU
capabilities, is often the simplest and most cost-effective way to speed
up training. Check the <a href="https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/instances-for-ML.html" class="external-link">Instances
for ML page</a> for guidance.</p>
<p>When to use a single instance upgrade:<br>
- Dataset size – The dataset is small to moderate (e.g., &lt;10 GB),
fitting comfortably within memory.<br>
- Model complexity – XGBoost models are typically small enough to fit in
memory.<br>
- Training time – If training completes in a few hours but could be
faster, upgrading may help.</p>
<p>Upgrading a single instance is usually the most efficient option. It
avoids the communication overhead of multi-instance setups and works
well for small to medium datasets.</p>
</div>
<div class="section level3">
<h3 id="option-2-use-multiple-instances-for-distributed-training">Option 2: Use multiple instances for distributed training<a class="anchor" aria-label="anchor" href="#option-2-use-multiple-instances-for-distributed-training"></a></h3>
<p>If upgrading a single instance doesn’t sufficiently reduce training
time, distributed training across multiple instances may be a viable
alternative. For XGBoost, SageMaker applies only data parallelism (not
model parallelism).</p>
<div class="section level4">
<h4 id="xgboost-uses-data-parallelism-not-model-parallelism">XGBoost uses data parallelism, not model parallelism<a class="anchor" aria-label="anchor" href="#xgboost-uses-data-parallelism-not-model-parallelism"></a></h4>
<ul><li>Data parallelism – The dataset is split across multiple instances,
with each instance training on a portion of the data. The gradient
updates are then synchronized and aggregated.<br></li>
<li>Why not model parallelism? – Unlike deep learning models, XGBoost
decision trees are small enough to fit in memory, so there’s no need to
split the model itself across multiple instances.</li>
</ul></div>
<div class="section level4">
<h4 id="how-sagemaker-implements-data-parallelism-for-xgboost">How SageMaker implements data parallelism for XGBoost<a class="anchor" aria-label="anchor" href="#how-sagemaker-implements-data-parallelism-for-xgboost"></a></h4>
<ul><li>When <code>instance_count &gt; 1</code>, SageMaker automatically
splits the dataset across instances.<br></li>
<li>Each instance trains on a subset of the data, computing gradient
updates in parallel.<br></li>
<li>Gradient updates are synchronized across instances before the next
iteration.<br></li>
<li>The final trained model is assembled as if it had been trained on
the full dataset.</li>
</ul></div>
</div>
<div class="section level3">
<h3 id="when-to-consider-multiple-instances">When to consider multiple instances<a class="anchor" aria-label="anchor" href="#when-to-consider-multiple-instances"></a></h3>
<p>Using multiple instances makes sense when:<br>
- Dataset size – The dataset is large and doesn’t fit comfortably in
memory.<br>
- Expected training time – A single instance takes too long (e.g.,
&gt;10 hours).<br>
- Need for faster training – Parallelization can speed up training but
introduces communication overhead.</p>
<p>If scaling to multiple instances, monitoring training time and
efficiency is critical. In many cases, a single, more powerful instance
may be more cost-effective than multiple smaller ones.</p>
</div>
<div class="section level3">
<h3 id="implementing-distributed-training-with-xgboost-in-sagemaker">Implementing distributed training with XGBoost in SageMaker<a class="anchor" aria-label="anchor" href="#implementing-distributed-training-with-xgboost-in-sagemaker"></a></h3>
<p>In SageMaker, setting up distributed training for XGBoost can offer
significant time savings as dataset sizes and computational requirements
increase. Here’s how you can configure it:</p>
<ol style="list-style-type: decimal"><li>
<strong>Select multiple instances</strong>: Specify
<code>instance_count &gt; 1</code> in the SageMaker
<code>Estimator</code> to enable distributed training.</li>
<li>
<strong>Optimize instance type</strong>: Choose an instance type
suitable for your dataset size and XGBoost requirements</li>
<li>
<strong>Monitor for speed improvements</strong>: With larger
datasets, distributed training can yield time savings by scaling
horizontally. However, gains may vary depending on the dataset and
computation per instance.</li>
</ol><div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># Define instance type/count we'll use for training</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>instance_count<span class="op">=</span><span class="dv">1</span> <span class="co"># always start with 1. Rarely is parallelized training justified with data &lt; 50 GB.</span></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a><span class="co"># Define the XGBoost estimator for distributed training</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>xgboost_estimator <span class="op">=</span> Estimator(</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>    image_uri<span class="op">=</span>sagemaker.image_uris.retrieve(<span class="st">"xgboost"</span>, session.boto_region_name, version<span class="op">=</span><span class="st">"1.5-1"</span>),</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,  <span class="co"># Start with 1 instance for baseline</span></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>)</span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a><span class="co"># Set hyperparameters</span></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>xgboost_estimator.set_hyperparameters(</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>    eta<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>    colsample_bytree<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>    num_round<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>)</span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a><span class="co"># Specify input data from S3</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(train_s3_path, content_type<span class="op">=</span><span class="st">"csv"</span>)</span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a><span class="co"># Run with 1 instance</span></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>start1 <span class="op">=</span> t.time()</span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>xgboost_estimator.fit({<span class="st">"train"</span>: train_input})</span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a>end1 <span class="op">=</span> t.time()</span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a><span class="co"># Now run with 2 instances to observe speedup</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>xgboost_estimator.instance_count <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a>start2 <span class="op">=</span> t.time()</span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>xgboost_estimator.fit({<span class="st">"train"</span>: train_input})</span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a>end2 <span class="op">=</span> t.time()</span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end1 <span class="op">-</span> start1<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-40"><a href="#cb18-40" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end2 <span class="op">-</span> start2<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>xgboost_estimator<span class="sc">.</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="why-scaling-instances-might-not-show-speedup-here">Why scaling instances might not show speedup here<a class="anchor" aria-label="anchor" href="#why-scaling-instances-might-not-show-speedup-here"></a></h3>
<ul><li><p>Small dataset: With only 892 rows, the dataset might be too small
to benefit from distributed training. Distributing small datasets often
adds overhead (like network communication between instances), which
outweighs the parallel processing benefits.</p></li>
<li><p>Distributed overhead: Distributed training introduces
coordination steps that can add latency. For very short training jobs,
this overhead can become a larger portion of the total training time,
reducing the benefit of additional instances.</p></li>
<li><p>Tree-based models: Tree-based models, like those in XGBoost,
don’t benefit from distributed scaling as much as deep learning models
when datasets are small. For large datasets, distributed XGBoost can
still offer speedups, but this effect is generally less than with neural
networks, where parallel gradient updates across multiple instances
become efficient.</p></li>
</ul></div>
<div class="section level3">
<h3 id="when-multi-instance-training-helps">When multi-instance training helps<a class="anchor" aria-label="anchor" href="#when-multi-instance-training-helps"></a></h3>
<ul><li><p>Larger datasets: Multi-instance training shines with larger
datasets, where splitting the data across instances and processing it in
parallel can significantly reduce the training time.</p></li>
<li><p>Complex models: For highly complex models with many parameters
(like deep learning models or large XGBoost ensembles) and long training
times, distributing the training can help speed up the process as each
instance contributes to the gradient calculation and optimization
steps.</p></li>
<li><p>Distributed algorithms: XGBoost has a built-in distributed
training capability, but models that perform gradient descent, like deep
neural networks, gain more obvious benefits because each instance can
compute gradients for a batch of data simultaneously, allowing faster
convergence.</p></li>
</ul></div>
<div class="section level3">
<h3 id="for-cost-optimization">For cost optimization<a class="anchor" aria-label="anchor" href="#for-cost-optimization"></a></h3>
<ul><li>Single-instance training is typically more cost-effective for small
or moderately sized datasets, while multi-instance setups can reduce
wall-clock time for larger datasets and complex models, at a higher
instance cost.</li>
<li>Increase instance count only if training time becomes prohibitive
even with more powerful single instances, while being mindful of
communication overhead and scaling efficiency.</li>
</ul><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul><li>
<strong>Environment initialization</strong>: Setting up a SageMaker
session, defining roles, and specifying the S3 bucket are essential for
managing data and running jobs in SageMaker.</li>
<li>
<strong>Local vs. managed training</strong>: Always test your code
locally (on a smaller scale) before scaling things up. This avoids
wasting resources on buggy code that doesn’t produce reliable
results.</li>
<li>
<strong>Estimator classes</strong>: SageMaker provides
framework-specific Estimator classes (e.g., XGBoost, PyTorch, SKLearn)
to streamline training setups, each suited to different model types and
workflows.</li>
<li>
<strong>Custom scripts vs. built-in images</strong>: Custom training
scripts offer flexibility with preprocessing and custom logic, while
built-in images are optimized for rapid deployment and simpler
setups.</li>
<li>
<strong>Training data channels</strong>: Using
<code>TrainingInput</code> ensures SageMaker manages data efficiently,
especially for distributed setups where data needs to be synchronized
across multiple instances.</li>
<li>
<strong>Distributed training options</strong>: Data parallelism
(splitting data across instances) is common for many models, while model
parallelism (splitting the model across instances) is useful for very
large models that exceed instance memory.</li>
</ul></div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/Interacting-with-code-repo.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/Training-models-in-SageMaker-notebooks-part2.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/Interacting-with-code-repo.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Using a GitHub
        </a>
        <a class="chapter-link float-end" href="../instructor/Training-models-in-SageMaker-notebooks-part2.html" rel="next">
          Next: Training Models in...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/Training-models-in-SageMaker-notebooks.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/" class="external-link">Source</a></p>
				<p><a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:endemann@wisc.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.12" class="external-link">sandpaper (0.16.12)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.6" class="external-link">varnish (1.0.6)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://UW-Madison-DataScience.github.io/ml-with-aws-sagemaker/instructor/Training-models-in-SageMaker-notebooks.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "AWS, SageMaker, Cloud Computing, Machine Learning, AI",
  "name": "Training Models in SageMaker: Intro",
  "creativeWorkStatus": "active",
  "url": "https://UW-Madison-DataScience.github.io/ml-with-aws-sagemaker/instructor/Training-models-in-SageMaker-notebooks.html",
  "identifier": "https://UW-Madison-DataScience.github.io/ml-with-aws-sagemaker/instructor/Training-models-in-SageMaker-notebooks.html",
  "dateCreated": "2024-10-31",
  "dateModified": "2025-03-12",
  "datePublished": "2025-08-05"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

