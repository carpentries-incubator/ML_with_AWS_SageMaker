{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef53255",
   "metadata": {},
   "source": [
    "# Episode 03 – Evaluating WattBot RAG with Amazon Bedrock\n",
    "\n",
    "In this episode, we re-run the WattBot evaluation pipeline using a **hosted LLM on Amazon Bedrock** instead of:\n",
    "\n",
    "- a powerful notebook instance (Episode 01), or  \n",
    "- a SageMaker Processing job (Episode 02).\n",
    "\n",
    "The **retrieval and evaluation logic stays the same**. The only major change is how we generate answers:\n",
    "\n",
    "- Before: we loaded an open‑source model (Qwen) on our own GPU instance.\n",
    "- Now: we send prompts to a managed model on Amazon Bedrock (for example, Claude 3 Haiku or Sonnet) and pay **per token**.\n",
    "\n",
    "This gives you a template for plugging WattBot (or other RAG systems) into hosted frontier models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d7f19",
   "metadata": {},
   "source": [
    "## Why use a hosted LLM for WattBot?\n",
    "\n",
    "Amazon Bedrock exposes popular, high‑capacity models from providers such as **Anthropic (Claude)**, **Meta (Llama)**, **Mistral**, **Cohere**, and **Amazon Titan**.  \n",
    "From a WattBot perspective, the pattern is similar to using other hosted APIs (including OpenAI APIs): you send a prompt and receive a completion, and you are **billed per token**, not per GPU‑hour.\n",
    "\n",
    "There are real trade‑offs here:\n",
    "\n",
    "- **Pros of hosted LLMs (Bedrock / OpenAI‑style APIs)**  \n",
    "  - You can use state‑of‑the‑art models without provisioning or patching GPU instances.  \n",
    "  - Scaling up/down is handled for you by the provider.  \n",
    "  - For **small or one‑off evaluations** (like a single WattBot run over a modest question set), token‑based pricing is often cheaper and much simpler to budget.\n",
    "\n",
    "- **Pros of running your own model on GPU instances (Episodes 01–02)**  \n",
    "  - You have full control over which model you run (including custom fine‑tunes).  \n",
    "  - If you run **many large batch jobs** or keep GPUs busy most of the time, paying by instance‑hour can be comparable or cheaper than token‑based APIs.  \n",
    "  - There are no provider‑level rate limits beyond what your infrastructure can handle.\n",
    "\n",
    "In practice, the choice depends on:\n",
    "\n",
    "- How many questions you need to answer.  \n",
    "- How large the model is and how many tokens you expect per question.  \n",
    "- How often you will repeat this evaluation.  \n",
    "- Whether your team is comfortable managing GPU infrastructure.\n",
    "\n",
    "In this notebook we’ll keep the **RAG evaluation strategy identical** to earlier episodes and simply swap in a Bedrock model for generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c83d3",
   "metadata": {},
   "source": [
    "## Setup: libraries, configuration, and Bedrock client\n",
    "\n",
    "This notebook assumes you:\n",
    "\n",
    "- Already ran Episodes 01–02 and uploaded the following artifacts to S3:\n",
    "  - `wattbot_chunks.jsonl` (RAG chunks)\n",
    "  - `embeddings.npy` (chunk embeddings)\n",
    "  - `train_QA.csv` (WattBot training questions)\n",
    "  - `metadata.csv` (document‑level metadata)\n",
    "- Have **Amazon Bedrock** enabled in your account and an Anthropic Claude 3 model available.\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/model-catalog/serverless/anthropic.claude-haiku-4-5-20251001-v1:0\n",
    "\n",
    "You can adjust the S3 keys and model ID below to match your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3b41ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# ---- AWS configuration ----\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Claude 3 Haiku is a good starting point for batch evaluation.\n",
    "# Swap for Sonnet/Opus if you have access and want higher quality.\n",
    "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "\n",
    "# S3 bucket + keys where Episode 02 wrote the artifacts.\n",
    "# TODO: Update these keys to match your pipeline.\n",
    "bucket_name = \"chris-rag\"  # <-- change to your bucket\n",
    "chunks_key = \"wattbot_chunks.jsonl\"\n",
    "embeddings_key = \"embeddings/embeddings.npy\"\n",
    "train_key = \"train_QA.csv\"\n",
    "metadata_key = \"metadata.csv\"\n",
    "\n",
    "# Local working directory for downloaded artifacts\n",
    "local_data_dir = \"bedrock\"\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "# AWS clients\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbce0d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading s3://chris-rag/wattbot_chunks.jsonl -> bedrock/wattbot_chunks.jsonl\n",
      "Downloading s3://chris-rag/embeddings/embeddings.npy -> bedrock/embeddings.npy\n",
      "Downloading s3://chris-rag/train_QA.csv -> bedrock/train_QA.csv\n",
      "Downloading s3://chris-rag/metadata.csv -> bedrock/metadata.csv\n",
      "Chunks: 2874\n",
      "Train QAs: 41\n",
      "Embeddings shape: (2874, 1024)\n"
     ]
    }
   ],
   "source": [
    "def download_from_s3(key: str, local_name: str) -> str:\n",
    "    \"\"\"Download a file from S3 to local_data_dir and return the local path.\"\"\"\n",
    "    local_path = os.path.join(local_data_dir, local_name)\n",
    "    print(f\"Downloading s3://{bucket_name}/{key} -> {local_path}\")\n",
    "    s3.download_file(bucket_name, key, local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "chunks_path = download_from_s3(chunks_key, \"wattbot_chunks.jsonl\")\n",
    "emb_path = download_from_s3(embeddings_key, \"embeddings.npy\")\n",
    "train_qa_path = download_from_s3(train_key, \"train_QA.csv\")\n",
    "metadata_path = download_from_s3(metadata_key, \"metadata.csv\")\n",
    "\n",
    "# Load artifacts\n",
    "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunked_docs = [json.loads(line) for line in f]\n",
    "\n",
    "chunk_embeddings = np.load(emb_path)\n",
    "train_df = pd.read_csv(train_qa_path)\n",
    "\n",
    "# Robust metadata load: handle possible non-UTF-8 characters\n",
    "try:\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "except UnicodeDecodeError:\n",
    "    metadata_df = pd.read_csv(metadata_path, encoding=\"latin1\")\n",
    "\n",
    "print(f\"Chunks: {len(chunked_docs)}\")\n",
    "print(f\"Train QAs: {len(train_df)}\")\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380ee16b-1cc1-4c0e-8cc8-47677569c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997ead87-85cd-4e2e-a39a-f3811f976fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882dbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata doc URLs: 32 entries\n"
     ]
    }
   ],
   "source": [
    "# Build doc_id -> url mapping from metadata\n",
    "docid_to_url: Dict[str, str] = {}\n",
    "for _, row in metadata_df.iterrows():\n",
    "    doc_id = str(row.get(\"id\", \"\")).strip()\n",
    "    url = row.get(\"url\", \"\")\n",
    "    if doc_id and isinstance(url, str) and url.strip():\n",
    "        docid_to_url[doc_id] = url.strip()\n",
    "\n",
    "print(f\"Metadata doc URLs: {len(docid_to_url)} entries\")\n",
    "\n",
    "# Load the same embedding model we used earlier\n",
    "embedding_model_id = \"thenlper/gte-large\"\n",
    "embedder = SentenceTransformer(embedding_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "325f97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- similarity + retrieval ----------------------\n",
    "\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Cosine similarity between two sets of vectors.\"\"\"\n",
    "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return np.matmul(a_norm, b_norm.T)\n",
    "\n",
    "\n",
    "def retrieve_top_k(\n",
    "    query_embedding: np.ndarray,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    k: int = 8,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return the top–k chunks for a single query embedding.\"\"\"\n",
    "    query = query_embedding.reshape(1, -1)\n",
    "    sims = cosine_similarity_matrix(query, chunk_embeddings)[0]\n",
    "\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        ch = chunked_docs[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"text\": ch[\"text\"],\n",
    "                \"doc_id\": ch.get(\"doc_id\", \"\"),\n",
    "                \"title\": ch.get(\"title\", \"\"),\n",
    "                \"url\": ch.get(\"url\", \"\"),\n",
    "                \"page_num\": ch.get(\"page_num\", None),\n",
    "                \"page_label\": ch.get(\"page_label\", None),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def format_context_for_prompt(retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Turn retrieved chunk dicts into a compact context string for the LLM.\"\"\"\n",
    "    lines = []\n",
    "    for i, ch in enumerate(retrieved_chunks, start=1):\n",
    "        label = ch.get(\"doc_id\", f\"chunk_{i}\")\n",
    "        page = ch.get(\"page_label\", ch.get(\"page_num\", \"\"))\n",
    "        header = f\"[{label}, page {page}]\".strip()\n",
    "        txt = ch[\"text\"].replace(\"\\n\", \" \")\n",
    "        lines.append(f\"{header}: {txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def retrieve_context_for_question(\n",
    "    question: str,\n",
    "    embedder: SentenceTransformer,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    return retrieved, q_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b86f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- answer normalization ----------------------\n",
    "\n",
    "def normalize_answer_value(raw_value: str) -> str:\n",
    "    \"\"\"Normalize answer_value according to WattBot conventions.\"\"\"\n",
    "    if raw_value is None:\n",
    "        return \"is_blank\"\n",
    "\n",
    "    s = str(raw_value).strip()\n",
    "\n",
    "    if not s or s.lower() == \"none\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        return s\n",
    "\n",
    "    if s.lower() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # If there is whitespace, keep only the first token\n",
    "    if \" \" in s:\n",
    "        first, *_ = s.split()\n",
    "        s = first\n",
    "\n",
    "    # Remove commas\n",
    "    s = s.replace(\",\", \"\")\n",
    "\n",
    "    try:\n",
    "        val = float(s)\n",
    "        if val.is_integer():\n",
    "            return str(int(val))\n",
    "        return f\"{val:.10g}\"  # avoid scientific notation\n",
    "    except ValueError:\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb27e385",
   "metadata": {},
   "source": [
    "## Calling Claude 3 on Amazon Bedrock\n",
    "\n",
    "Next, we define a small helper that:\n",
    "\n",
    "- Formats a request for the Claude 3 Messages API on Bedrock.\n",
    "- Sends the request with `bedrock-runtime.invoke_model`.\n",
    "- Returns the generated text string.\n",
    "\n",
    "(The response also includes token usage; you can extend this function to track total\n",
    "input/output tokens for cost estimation if you’d like.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25c1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock_claude(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    model_id: str = bedrock_model_id,\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.3,\n",
    ") -> str:\n",
    "    \"\"\"Call an Anthropic Claude 3 model on Bedrock and return the text response.\"\"\"\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"system\": system_prompt,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": user_prompt}],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    request = json.dumps(body)\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR calling Bedrock model {model_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    text = model_response[\"content\"][0][\"text\"]\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc6b61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- explanation helpers ----------------------\n",
    "\n",
    "def build_explanation_prompt(question: str, answer: str, supporting_materials: str) -> str:\n",
    "    return (\n",
    "        \"You are explaining answers for an energy, water, and carbon footprint assistant.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Answer: {answer}\\n\\n\"\n",
    "        f\"Supporting materials:\\n{supporting_materials}\\n\\n\"\n",
    "        \"In 1–3 sentences, explain how the supporting materials justify the answer. \"\n",
    "        \"Be precise but concise.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def explanation_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an AI assistant that explains how evidence supports answers about \"\n",
    "        \"energy, water, and carbon footprint. Focus on clear, factual reasoning, \"\n",
    "        \"and refer directly to the cited documents when appropriate.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def bedrock_explanation_phase_for_question(\n",
    "    qid: str,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    supporting_materials: str,\n",
    "    model_id: str = bedrock_model_id,\n",
    ") -> str:\n",
    "    sys_prompt = explanation_system_prompt()\n",
    "    prompt = build_explanation_prompt(question, answer, supporting_materials)\n",
    "    raw_explanation = call_bedrock_claude(\n",
    "        system_prompt=sys_prompt,\n",
    "        user_prompt=prompt,\n",
    "        model_id=model_id,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    return raw_explanation.strip()\n",
    "\n",
    "\n",
    "# ---------------------- answer phase (JSON contract) ----------------------\n",
    "\n",
    "def bedrock_answer_phase_for_question(\n",
    "    qid: str,\n",
    "    question: str,\n",
    "    retrieved_chunks: List[Dict[str, Any]],\n",
    "    model_id: str = bedrock_model_id,\n",
    "):\n",
    "    \"\"\"Use Claude 3 on Bedrock to answer a single WattBot question given retrieved chunks.\"\"\"\n",
    "    context = format_context_for_prompt(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are WattBot, a question-answering assistant for energy, water, and carbon footprint.\\n\"\n",
    "        \"You must answer questions using ONLY the provided context from scientific papers.\\n\"\n",
    "        \"If the context does not contain enough information to answer with high confidence,\\n\"\n",
    "        \"you must mark the question as unanswerable.\\n\\n\"\n",
    "        \"You must respond with a single JSON object with the following keys:\\n\"\n",
    "        \"- answer: natural language answer, including numeric value and units if applicable.\\n\"\n",
    "        \"- answer_value: normalized numeric (0 for false, 1 for true), or categorical value with NO units or symbols;\\n\"\n",
    "        \"  use 'is_blank' if the question is unanswerable.\\n\"\n",
    "        \"- answer_unit: unit string (e.g., kWh, gCO2, %, is_blank).\\n\"\n",
    "        \"- ref_id: list of document IDs that support the answer.\\n\"\n",
    "        \"- is_blank: true if unanswerable, false otherwise.\\n\"\n",
    "        \"- supporting_materials: short quote or table/figure pointer from the context.\\n\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Use the context below to answer the question. \"\n",
    "        \"Return ONLY a JSON object, no extra commentary.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\"\n",
    "    )\n",
    "\n",
    "    raw_answer = call_bedrock_claude(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        model_id=model_id,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    parsed = {\n",
    "        \"answer\": \"\",\n",
    "        \"answer_value\": \"is_blank\",\n",
    "        \"answer_unit\": \"is_blank\",\n",
    "        \"ref_id\": [],\n",
    "        \"is_blank\": True,\n",
    "        \"supporting_materials\": \"is_blank\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        first_brace = raw_answer.find(\"{\")\n",
    "        last_brace = raw_answer.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            json_str = raw_answer[first_brace : last_brace + 1]\n",
    "        else:\n",
    "            json_str = raw_answer\n",
    "\n",
    "        candidate = json.loads(json_str)\n",
    "\n",
    "        parsed[\"answer\"] = candidate.get(\"answer\", \"\").strip()\n",
    "        parsed[\"answer_value\"] = normalize_answer_value(candidate.get(\"answer_value\", \"is_blank\"))\n",
    "        parsed[\"answer_unit\"] = str(candidate.get(\"answer_unit\", \"is_blank\")).strip() or \"is_blank\"\n",
    "\n",
    "        ref_id = candidate.get(\"ref_id\", [])\n",
    "        if isinstance(ref_id, str):\n",
    "            ref_ids = [ref_id]\n",
    "        elif isinstance(ref_id, list):\n",
    "            ref_ids = [str(x).strip() for x in ref_id if x]\n",
    "        else:\n",
    "            ref_ids = []\n",
    "        parsed[\"ref_id\"] = ref_ids\n",
    "\n",
    "        is_blank_flag = candidate.get(\"is_blank\", False)\n",
    "        parsed[\"is_blank\"] = bool(is_blank_flag)\n",
    "\n",
    "        supp = candidate.get(\"supporting_materials\", \"is_blank\")\n",
    "        parsed[\"supporting_materials\"] = str(supp).strip() or \"is_blank\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parse error for question {qid}; defaulting to is_blank. Error: {e}\")\n",
    "\n",
    "    return (\n",
    "        parsed[\"answer\"],\n",
    "        parsed[\"answer_value\"],\n",
    "        parsed[\"is_blank\"],\n",
    "        parsed[\"ref_id\"],\n",
    "        parsed[\"supporting_materials\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d940fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_qa_bedrock(\n",
    "    row: pd.Series,\n",
    "    embedder: SentenceTransformer,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    docid_to_url: Dict[str, str],\n",
    "    top_k: int = 8,\n",
    "    retrieval_threshold: float = 0.25,\n",
    "    model_id: str = bedrock_model_id,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Full RAG + Bedrock pipeline for a single question.\"\"\"\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "\n",
    "    retrieved, q_emb = retrieve_context_for_question(\n",
    "        question=question,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n",
    "\n",
    "    (\n",
    "        answer,\n",
    "        answer_value,\n",
    "        is_blank_llm,\n",
    "        ref_ids,\n",
    "        supporting_materials,\n",
    "    ) = bedrock_answer_phase_for_question(\n",
    "        qid=qid,\n",
    "        question=question,\n",
    "        retrieved_chunks=retrieved,\n",
    "        model_id=model_id,\n",
    "    )\n",
    "\n",
    "    is_blank = bool(is_blank_llm) or (top_score < retrieval_threshold)\n",
    "\n",
    "    if is_blank:\n",
    "        answer = \"Unable to answer with confidence based on the provided documents.\"\n",
    "        answer_value = \"is_blank\"\n",
    "        answer_unit = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "        supporting_materials = \"is_blank\"\n",
    "        explanation = \"\"\n",
    "    else:\n",
    "        answer_value = normalize_answer_value(answer_value)\n",
    "        answer_unit = \"is_blank\"\n",
    "\n",
    "        if isinstance(ref_ids, list) and ref_ids:\n",
    "            ref_id_str = \";\".join(ref_ids)\n",
    "            urls = []\n",
    "            for rid in ref_ids:\n",
    "                url = docid_to_url.get(str(rid), \"\")\n",
    "                if url:\n",
    "                    urls.append(url)\n",
    "            ref_url_str = \";\".join(urls) if urls else \"is_blank\"\n",
    "        else:\n",
    "            ref_id_str = \"is_blank\"\n",
    "            ref_url_str = \"is_blank\"\n",
    "\n",
    "        explanation = bedrock_explanation_phase_for_question(\n",
    "            qid=qid,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            supporting_materials=supporting_materials,\n",
    "            model_id=model_id,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_value\": answer_value,\n",
    "        \"answer_unit\": answer_unit,\n",
    "        \"ref_id\": ref_id_str,\n",
    "        \"ref_url\": ref_url_str,\n",
    "        \"supporting_materials\": supporting_materials,\n",
    "        \"explanation\": explanation,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513d271",
   "metadata": {},
   "source": [
    "## Run the WattBot evaluation with Bedrock\n",
    "\n",
    "Now we can loop over all questions in `train_QA.csv`, run retrieval + Bedrock\n",
    "generation, and write a `wattbot_solutions_bedrock.csv` file.\n",
    "\n",
    "This mirrors the logic from Episode 02 – the only difference is that the answer\n",
    "and explanation phases call a hosted Claude 3 model instead of a local Qwen model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28e3a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################################################################\n",
      "QUESTION: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "ANSWER: The ML.ENERGY Benchmark is the name of the benchmark suite presented in the paper for measuring inference energy consumption.\n",
      "ref_ids: chung2025\n",
      "EXPLANATION: The supporting materials directly state that the ML.ENERGY Benchmark is the name of the benchmark suite presented in the paper for measuring inference energy consumption. The paper introduces this new benchmark suite specifically designed to measure the energy consumption of modern generative AI models during inference.\n",
      "########################################################################################################\n",
      "QUESTION: What were the net CO2e emissions from training the GShard-600B model?\n",
      "ANSWER: The net CO2e emissions from training the GShard-600B model were 4.3 metric tons.\n",
      "ref_ids: patterson2021\n",
      "EXPLANATION: The supporting materials directly state that the net CO2e emissions from training the GShard-600B model were 4.3 metric tons, as shown in Table 4 of the cited document. This information provides clear, factual evidence to support the answer provided.\n",
      "########################################################################################################\n",
      "QUESTION: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "########################################################################################################\n",
      "QUESTION: What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "########################################################################################################\n",
      "QUESTION: True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "ANSWER: True. According to the context, hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "ref_ids: wu2021b;patterson2021\n",
      "EXPLANATION: The supporting materials directly state that hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers, as measured by power usage effectiveness (PUE). This statistic is provided in Figure 1, which shows the stark difference in PUE between traditional and highly optimized hyperscale data centers. The supporting evidence clearly justifies the claim that the statement is true.\n",
      "########################################################################################################\n",
      "QUESTION: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "ANSWER: For every medium-length GPT-3 completion (prompt = 800 words; response 150 - 300 words), the model 'drinks' roughly 10 - 50 bottles of 500 mL water.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials directly state that for every medium-length GPT-3 completion (prompt = 800 words; response 150 - 300 words), the model needs to \"drink\" (consume) roughly 10 - 50 bottles of 500 mL water. This provides the specific range of water consumption that justifies the answer given.\n",
      "########################################################################################################\n",
      "QUESTION: From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?\n",
      "ANSWER: The difference between the percentage of CVPR papers that target accuracy (75%) and the percentage of CVPR papers that target efficiency (20%) is 55 percentage points.\n",
      "ref_ids: schwartz2019\n",
      "EXPLANATION: The supporting materials, specifically Figure 2 from the provided document, directly show that 75% of CVPR papers target accuracy, while only 20% target efficiency. Therefore, the difference between these two percentages is 55 percentage points, which is the answer stated in the original response.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.\n",
      "ANSWER: False. The AI Act does not make energy consumption data from providers publicly available to NGOs, analysts, and the general public. The context indicates that while the AI Act requires reporting of energy consumption, this information is restricted to authorities and is not accessible to the general public due to confidentiality clauses.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials clearly state that the AI Act does not mandate the disclosure of energy consumption data to the general public. While the Act requires reporting of energy consumption, this information is restricted to authorities and is not accessible to NGOs, analysts, or the general public due to confidentiality clauses in the legislation. This directly contradicts the claim that the AI Act makes this data publicly available, justifying the conclusion that the statement is false.\n",
      "########################################################################################################\n",
      "QUESTION: What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?\n",
      "ANSWER: According to the context, for a projected GPU capacity of 100GB, the maximum batch size for fine-tuning a Mixtral model is projected to be 28 samples.\n",
      "ref_ids: xia2024\n",
      "EXPLANATION: The supporting materials directly state that for a projected GPU capacity of 100GB, the maximum batch size for fine-tuning a Mixtral model is projected to be 28 samples. This provides a clear and specific answer supported by the given context.\n",
      "########################################################################################################\n",
      "QUESTION: What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?\n",
      "ANSWER: The context indicates that for the LLaMA-7B model, there was approximately a 2x increase in inference throughput (words/tokens/responses per second) when using NVIDIA A100 GPUs compared to V100 GPUs. For the LLaMA-13B model, the increase was around 1.25x.\n",
      "ref_ids: samsi2024\n",
      "EXPLANATION: The supporting materials directly state that for the LLaMA-7B model, there was approximately a 2x increase in inference throughput (words/tokens/responses per second) when using NVIDIA A100 GPUs compared to V100 GPUs. For the larger LLaMA-13B model, the increase was around 1.25x. This evidence from the provided context clearly justifies the claim made in the answer.\n",
      "########################################################################################################\n",
      "QUESTION: What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?\n",
      "ANSWER: The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 16.904 million liters.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting material directly states that the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 16.904 million liters, as shown in Table 1 of the provided context. This quantitative data from the source document directly justifies the answer provided.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.\n",
      "ANSWER: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials clearly state that the authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI, but across all AI systems. This is because the carbon footprint of AI models is often unrelated to their classification as high or low risk, so considering environmental impacts across the entire AI landscape is important, regardless of risk level.\n",
      "########################################################################################################\n",
      "QUESTION: As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?\n",
      "ANSWER: As of 2023, the water use effectiveness (WUE) for AWS data centers was 0.18 liters per kilowatt-hour (L/kWh), a 5% improvement from 2022 and a 28% improvement since 2021.\n",
      "ref_ids: amazon2023\n",
      "EXPLANATION: The supporting materials directly state that as of 2023, the water use effectiveness (WUE) for AWS data centers was 0.18 liters per kilowatt-hour (L/kWh). This represents a 5% improvement from the 2022 WUE of 0.19 L/kWh, and a 28% improvement since 2021. The supporting materials provide the specific WUE value for 2023 as well as the year-over-year and multi-year improvements, clearly justifying the answer.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\n",
      "ANSWER: False. The context does not indicate that local inference was emphasized as a sustainability measure to reduce network overhead and carbon footprint when deploying large language models. The context focuses on the use of local inference optimization, energy-efficient model selection, and a comprehensive evaluation methodology to reduce energy consumption and carbon emissions of LLM deployment, without explicitly mentioning network overhead reduction as a motivation.\n",
      "ref_ids: khan2025;zschache2025;fernandez2025;luccioni2024\n",
      "EXPLANATION: The supporting materials do not indicate that reducing network overhead was a motivation for emphasizing local inference as a sustainability measure. The context focuses on using local inference optimization, energy-efficient model selection, and a comprehensive evaluation methodology to directly reduce the energy consumption and carbon emissions of LLM deployment, without citing network overhead reduction as a reason for the local inference approach.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\n",
      "ANSWER: True. Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\n",
      "ref_ids: strubell2019;cottier2024\n",
      "EXPLANATION: The supporting materials indicate that the runtime of training jobs can vary significantly, from as little as 3 minutes to as long as 9 days, with an average of 52 hours. This shows that tracking the actual runtime of a training job is important for accurately estimating the compute cost, as the runtime can have a large impact on the final cost, especially in GPU-based or cloud environments where costs are often tied to usage time. Without knowing the actual runtime, the authors had to use a median value of 33 days, which may not accurately reflect the true cost.\n",
      "########################################################################################################\n",
      "QUESTION: For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?\n",
      "ANSWER: The LLaMA-65B model experienced a maximum performance improvement (latency reduction) of 13.2% by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study.\n",
      "ref_ids: chen2024\n",
      "EXPLANATION: The 2025 Chen et al. study directly states that the LLaMA-65B model achieved a maximum performance improvement (latency reduction) of 13.2% by enabling the automated resource utilization overlapping feature, as shown in Figure 14 of the study. The supporting materials provide clear and specific evidence to justify the stated answer.\n",
      "########################################################################################################\n",
      "QUESTION: How much does an elephant weigh?\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "########################################################################################################\n",
      "QUESTION: Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?\n",
      "ANSWER: Based on the information provided in the context, the large NLP DNN with the highest energy consumption is GPT-3. The context states that GPT-3 has an estimated energy consumption of 1287 MWh and carbon emissions of 552 tCO2e during training, which are the highest among the five models mentioned (T5, Meena, GShard, Switch Transformer, and GPT-3).\n",
      "ref_ids: patterson2021\n",
      "EXPLANATION: The supporting materials directly state that GPT-3 has the highest estimated energy consumption of 1287 MWh among the five large NLP DNNs mentioned. This evidence clearly indicates that GPT-3 has the highest energy consumption compared to the other models listed, which is the key information needed to answer the question.\n",
      "########################################################################################################\n",
      "QUESTION: How many days of CO₂ emissions from an average American life are equivalent to training BERT base?\n",
      "ANSWER: According to the provided context, training BERT base on 8 V100 GPUs for 36 hours would emit around 7,000 to 26,000 grams of CO2, depending on the region. The average American life emits around 36,156 lbs of CO2 per year. Therefore, the emissions from training BERT base are equivalent to around 0.5 to 2 days of CO2 emissions from an average American life.\n",
      "ref_ids: dodge2022;strubell2019\n",
      "EXPLANATION: The supporting materials provide the key information needed to calculate the number of days of CO2 emissions from an average American life that are equivalent to training BERT base. First, the emissions from training BERT base are given as 7,000 to 26,000 grams of CO2. Second, the average annual CO2 emissions for an American life is stated as 36,156 lbs. By converting the BERT emissions to lbs and dividing by the annual American emissions, we can determine that the BERT training is equivalent to 0.5 to 2 days of a typical American's CO2 output.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\n",
      "ANSWER: False. The context indicates that the Evolved Transformer architecture outperforms the Transformer architecture on the WMT'24 EN-DE BLUE task, even as the model sizes grow. The Evolved Transformer has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure compared to the vanilla Transformer (Big) model.\n",
      "ref_ids: patterson2021\n",
      "EXPLANATION: The supporting materials, specifically Figure 4 in the provided context, show that the Evolved Transformer architecture outperforms the Transformer architecture on the WMT'24 EN-DE BLUE task. The Evolved Transformer has 37% fewer parameters and converges to the same accuracy with 25% less energy expenditure compared to the vanilla Transformer (Big) model. This evidence directly contradicts the claim in the question, which states that the Transformer architecture eventually outperforms the Evolved Transformers architecture as the model sizes grow.\n",
      "########################################################################################################\n",
      "QUESTION: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "########################################################################################################\n",
      "QUESTION: True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.\n",
      "ANSWER: True, eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.\n",
      "ref_ids: erben2023\n",
      "EXPLANATION: The supporting materials indicate that the cost per 1 million samples is significantly lower for eight T4 spot instances ($1.77) compared to a DGX-2 node ($4.24). This suggests that using the eight T4 spot instances would be more cost-efficient for distributed training, even when accounting for potential additional egress costs. The lower per-sample cost of the T4 spot instances makes them a more cost-effective option than the DGX-2 node for this use case.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.\n",
      "ANSWER: False. The 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI.\n",
      "ref_ids: luccioni2025b\n",
      "EXPLANATION: The supporting material directly states that the 2023 US Executive Order regarding AI did not mention the greenhouse gas emissions or energy usage of AI. Since the order did not address these environmental impacts of AI, the claim that it did is false.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.\n",
      "ANSWER: False. The 2023 Energy Efficiency Act in Germany requires data centers to run on 50% renewable energy by 2027, not 100% renewable energy.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials clearly state that the 2023 Energy Efficiency Act in Germany requires data centers to run on 50% renewable energy by 2027, not 100% renewable energy. The act sets a target of 100% renewable energy for data centers by January 1, 2027, but this is not the same as requiring 100% renewable energy starting on that date.\n",
      "########################################################################################################\n",
      "QUESTION: Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?\n",
      "ANSWER: Out of a sample of 60 papers from top AI conferences, 6 papers from ACL targeted both accuracy and efficiency.\n",
      "ref_ids: schwartz2019\n",
      "EXPLANATION: The supporting material, specifically Figure 2, indicates that 10% of the papers from the ACL conference targeted both accuracy and efficiency. Since the question asks about a sample of 60 papers, and 10% of that is 6 papers, the answer that 6 papers from ACL targeted both accuracy and efficiency is justified by the information provided in the supporting materials.\n",
      "########################################################################################################\n",
      "QUESTION: According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?\n",
      "ANSWER: According to recent estimates, inference can account for up to 90% of a model's total lifecycle energy use.\n",
      "ref_ids: jegham2025;patterson2021\n",
      "EXPLANATION: The supporting materials directly state that recent estimates indicate inference can account for up to 90% of a model's total lifecycle energy use [14, 15]. This provides clear evidence to support the claim that inference can account for a significant majority, up to 90%, of a model's total lifecycle energy consumption.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.\n",
      "ANSWER: False. The AI Act requires providers to report energy consumption during the development phase of general-purpose AI models, but does not explicitly require reporting of energy consumption during the inference phase.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting material from [ebert2024] clearly states that the AI Act only requires providers to report the energy consumption during the development phase of general-purpose AI models, but does not mandate the reporting of energy consumption during the inference phase. This directly contradicts the claim in the question that the AI Act requires reporting of both training and inference energy consumption, making the statement false.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.\n",
      "ANSWER: False. The AI Act currently does not require providers to report energy use during the inference phase of AI models.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials indicate that the AI Act currently only requires providers to document the computational resources used during the development, training, testing, and validation stages of AI models, but does not mandate reporting on the energy consumption during the inference phase. Since the Act does not explicitly include the inference phase in its requirements, the statement that the AI Act requires reporting on energy use during inference is false.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: New AI data centers often rely on air cooling due to high server power densities.\n",
      "ANSWER: True. New AI data centers often rely on air cooling due to high server power densities.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials directly state that new AI data centers often rely on air cooling due to high server power densities. This indicates that the claim in the question is true - new AI data centers do indeed frequently use air cooling systems to manage the high power requirements of their servers.\n",
      "########################################################################################################\n",
      "QUESTION: By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?\n",
      "ANSWER: Platform-level caching improved the power efficiency of the inference workload for the cross-lingual Transformer language model by a factor of 6.7x.\n",
      "ref_ids: wu2021a\n",
      "EXPLANATION: The supporting materials from Wu et al. (2021) directly state that platform-level caching improved the power efficiency of the inference workload for the cross-lingual Transformer language model by a factor of 6.7x. This improvement was achieved by pre-computing and caching frequently accessed embeddings, which reduced the computational load during the language translation tasks.\n",
      "########################################################################################################\n",
      "QUESTION: What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?\n",
      "ANSWER: The estimated CO2 emissions from training a BERT base model for 79 hours using 64 V100 GPUs is approximately 1,438 pounds.\n",
      "ref_ids: strubell2019\n",
      "EXPLANATION: The Strubell et al. (2019) paper provides a detailed analysis of the energy consumption and carbon emissions associated with training various deep learning models, including the BERT base model. Specifically, Table 3 in the paper shows that training a BERT base model for 79 hours using 64 V100 GPUs is estimated to result in 1,438 pounds of CO2 emissions. This direct citation from the supporting material justifies the answer provided.\n",
      "########################################################################################################\n",
      "QUESTION: According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?\n",
      "ANSWER: According to the provided context, ML inference reportedly accounts for 80-90% of total compute demand.\n",
      "ref_ids: patterson2021;luccioni2024;fernandez2025\n",
      "EXPLANATION: The supporting materials directly cite estimates from NVIDIA and Amazon Web Services that ML inference accounts for 80-90% of total compute demand. These industry sources provide clear and specific data points that support the claim that ML inference makes up the majority of total compute demand, in the range of 80-90%.\n",
      "########################################################################################################\n",
      "QUESTION: How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?\n",
      "ANSWER: Training a 6.1B-parameter language model is equivalent to about 12 years and 2 months of electricity consumption by the average U.S. household.\n",
      "ref_ids: morrison2025\n",
      "EXPLANATION: The supporting materials from [morrison2025] directly state that training a 13B-parameter language model consumed the equivalent of 12 years and 2 months of electricity usage by the average U.S. household. Since the question asks about a 6.1B-parameter model, which is less than half the size, the answer scales this value down proportionally to arrive at 12 years and 2 months of U.S. household electricity consumption.\n",
      "########################################################################################################\n",
      "QUESTION: True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.\n",
      "ANSWER: True, egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.\n",
      "ref_ids: erben2023\n",
      "EXPLANATION: The supporting materials show that for geo-distributed NLP experiments, the external egress cost can account for over 90% of the total cost per VM. Specifically, the document states that for Google Cloud, the egress cost of $4.329/h is more than 90% of the total cost per VM of $4.804/h. Even on Azure, which has lower egress rates, the egress cost of $1.882/h is still a significant portion of the total $2.101/h cost per VM. This evidence supports the claim that egress costs can make up over 90% of the total cost in these types of geo-distributed NLP experiments.\n",
      "########################################################################################################\n",
      "QUESTION: Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "########################################################################################################\n",
      "QUESTION: What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?\n",
      "ANSWER: The term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge' is 'water consumption'.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials directly define water consumption as \"water withdrawal minus water discharge\", which aligns with the description in the question of the amount of water \"evaporated, transpired, or incorporated into products\". The definition provided in the supporting materials clearly justifies the answer that the term for this quantity is \"water consumption\".\n",
      "########################################################################################################\n",
      "QUESTION: What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?\n",
      "ANSWER: The observed range of inference energy per second for LLaMA-65B across GPU shard configurations was between 300 Watts to 1 Kilowatt.\n",
      "ref_ids: samsi2024\n",
      "EXPLANATION: The supporting materials directly state that the observed range of inference energy per second for LLaMA-65B across different GPU shard configurations was between 300 Watts and 1 Kilowatt. This range is specified based on the lower end of 8 GPUs up to the higher end of 32 GPUs, providing the observed range for this model's energy consumption during inference.\n",
      "########################################################################################################\n",
      "QUESTION: When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?\n",
      "ANSWER: The 72B version of the Qwen model consumed approximately 2.6 times more energy than the 7B version for zero-shot classification.\n",
      "ref_ids: chung2025\n",
      "EXPLANATION: The supporting materials directly address the question by citing Figure 3 from the Chung et al. paper, which shows that the 72B version of the Qwen model consumed approximately 2.6 times more energy per generation than the 7B version when performing zero-shot classification on the NVIDIA H100 GPU. This quantitative evidence from the cited source clearly justifies the answer provided.\n",
      "########################################################################################################\n",
      "QUESTION: By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?\n",
      "ANSWER: Qwen's carbon emissions fell by 45% when applying quantization and local inference for sentiment classification.\n",
      "ref_ids: khan2025\n",
      "EXPLANATION: The supporting materials, specifically Table III in the referenced document, show that applying quantization and local inference techniques for sentiment classification resulted in up to a 45% reduction in carbon emissions across the models tested. This directly supports the claim that Qwen's carbon emissions fell by 45% when using these optimization techniques.\n",
      "########################################################################################################\n",
      "QUESTION: How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?\n",
      "ANSWER: The latest iteration of the ML.ENERGY Benchmark included 40 widely used model architectures across 6 different tasks.\n",
      "ref_ids: chung2025\n",
      "EXPLANATION: The supporting materials, specifically Tables 2 and 3 in the ML.ENERGY Benchmark paper, directly state that the latest iteration of the benchmark included 40 widely used model architectures across 6 different tasks. This provides clear evidence to support the claim in the answer.\n",
      "########################################################################################################\n",
      "QUESTION: In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "Wrote predictions to outputs/wattbot_solutions_bedrock.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The ML.ENERGY Benchmark is the name of the ben...</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>chung2025</td>\n",
       "      <td>https://arxiv.org/pdf/2505.06371</td>\n",
       "      <td>The ML.ENERGY Benchmark is the first inference...</td>\n",
       "      <td>The supporting materials directly state that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>The net CO2e emissions from training the GShar...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>patterson2021</td>\n",
       "      <td>https://arxiv.org/pdf/2104.10350</td>\n",
       "      <td>GShard-600B's emissions (Table 4) are 4.3 tCO2...</td>\n",
       "      <td>The supporting materials directly state that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>True. According to the context, hyperscale dat...</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>wu2021b;patterson2021</td>\n",
       "      <td>https://arxiv.org/pdf/2108.06738;https://arxiv...</td>\n",
       "      <td>Furthermore, between traditional and highly op...</td>\n",
       "      <td>The supporting materials directly state that h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  q003  What is the name of the benchmark suite presen...   \n",
       "1  q009  What were the net CO2e emissions from training...   \n",
       "2  q054  What is the model size in gigabytes (GB) for t...   \n",
       "3  q062  What was the total electricity consumption of ...   \n",
       "4  q075  True or False: Hyperscale data centers in 2020...   \n",
       "\n",
       "                                              answer answer_value answer_unit  \\\n",
       "0  The ML.ENERGY Benchmark is the name of the ben...            1    is_blank   \n",
       "1  The net CO2e emissions from training the GShar...          4.3    is_blank   \n",
       "2  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "3  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "4  True. According to the context, hyperscale dat...            1    is_blank   \n",
       "\n",
       "                  ref_id                                            ref_url  \\\n",
       "0              chung2025                   https://arxiv.org/pdf/2505.06371   \n",
       "1          patterson2021                   https://arxiv.org/pdf/2104.10350   \n",
       "2               is_blank                                           is_blank   \n",
       "3               is_blank                                           is_blank   \n",
       "4  wu2021b;patterson2021  https://arxiv.org/pdf/2108.06738;https://arxiv...   \n",
       "\n",
       "                                supporting_materials  \\\n",
       "0  The ML.ENERGY Benchmark is the first inference...   \n",
       "1  GShard-600B's emissions (Table 4) are 4.3 tCO2...   \n",
       "2                                           is_blank   \n",
       "3                                           is_blank   \n",
       "4  Furthermore, between traditional and highly op...   \n",
       "\n",
       "                                         explanation  \n",
       "0  The supporting materials directly state that t...  \n",
       "1  The supporting materials directly state that t...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4  The supporting materials directly state that h...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# For quick smoke tests, you can slice train_df (e.g., train_df.head(5))\n",
    "for _, row in train_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    print(f\"########################################################################################################\\nQUESTION: {question}\")\n",
    "\n",
    "    out = run_single_qa_bedrock(\n",
    "        row=row,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        docid_to_url=docid_to_url,\n",
    "        top_k=8,\n",
    "        retrieval_threshold=0.25,\n",
    "        model_id=bedrock_model_id,\n",
    "    )\n",
    "    \n",
    "    answer = out[\"answer\"]\n",
    "    ref_ids = out[\"ref_id\"]\n",
    "    explanation = out[\"explanation\"]\n",
    "    \n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    print(f\"ref_ids: {ref_ids}\")\n",
    "    print(f\"EXPLANATION: {explanation}\")\n",
    "    \n",
    "    results.append(out)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"wattbot_solutions_bedrock.csv\")\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Wrote predictions to {output_path}\")\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "172697da-013c-40dd-9b9b-3b06fba000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"True\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"False\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",   # can also pass \"is_blank\" or None\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,    # can pass \"is_blank\", or leave None to auto\n",
    "    n_examples=10,          # how many incorrect examples to print\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "\n",
    "    NA logic:\n",
    "    - If an explicit NA column is found/used (e.g. is_NA), we use it via _to_bool_flag.\n",
    "    - If you pass gt_is_na_col=\"is_blank\" or pred_is_na_col=\"is_blank\",\n",
    "      we *derive* NA from answer_value == \"is_blank\" instead of expecting a real column.\n",
    "    - If no NA column is available at all, we derive from answer_value == \"is_blank\".\n",
    "\n",
    "    Also prints up to `n_examples` rows where the model is not perfect\n",
    "    (answer_score < 1, ref_id_score < 1, or is_NA_score < 1).\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "\n",
    "    # ----- ground truth NA flags -----\n",
    "    if gt_is_na_col is not None and gt_is_na_col in merged.columns:\n",
    "        # Use explicit column (e.g. \"is_NA\")\n",
    "        gt_is_na_series = merged[gt_is_na_col].map(_to_bool_flag)\n",
    "    elif gt_is_na_col is not None and gt_is_na_col.lower() == \"is_blank\":\n",
    "        # Special meaning: derive NA from answer_value_gt == \"is_blank\"\n",
    "        gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "    else:\n",
    "        # Fallback: if we have is_NA or is_blank col, use it; else derive\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "\n",
    "    # ----- prediction NA flags -----\n",
    "    if pred_is_na_col is not None and pred_is_na_col in merged.columns:\n",
    "        pred_is_na_series = merged[pred_is_na_col].map(_to_bool_flag)\n",
    "    elif pred_is_na_col is not None and pred_is_na_col.lower() == \"is_blank\":\n",
    "        # Same convention: derive from answer_value_pred\n",
    "        pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "    else:\n",
    "        # Auto-detect or derive if no NA column in preds\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "\n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = bool(gt_is_na_series.iloc[idx])\n",
    "        pred_is_na = bool(pred_is_na_series.iloc[idx])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    # ----- Show some incorrect examples -----\n",
    "    incorrect = merged[\n",
    "        (merged[\"answer_score\"] < 1.0)\n",
    "        | (merged[\"ref_id_score\"] < 1.0)\n",
    "        | (merged[\"is_NA_score\"] < 1.0)\n",
    "    ]\n",
    "    \n",
    "    if not incorrect.empty and n_examples > 0:\n",
    "        print(\"\\nExamples of incorrect / partially correct responses \"\n",
    "              f\"(up to {n_examples} rows):\\n\")\n",
    "        # Grab up to n_examples \"worst\" rows by wattbot_score\n",
    "        for _, row in incorrect.sort_values(\"wattbot_score\").head(n_examples).iterrows():\n",
    "            q = row[\"question_gt\"] if \"question_gt\" in row.index else None\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"id: {row[id_col]}\")\n",
    "            if q is not None:\n",
    "                print(f\"Question: {q}\")\n",
    "            print(f\"GT answer_value:   {row[f'{gt_answer_col}_gt']}\")\n",
    "            print(f\"Pred answer_value: {row[f'{pred_answer_col}_pred']}\")\n",
    "            print(f\"GT ref_id:         {row[f'{gt_ref_col}_gt']}\")\n",
    "            print(f\"Pred ref_id:       {row[f'{pred_ref_col}_pred']}\")\n",
    "            print(f\"answer_score: {row['answer_score']:.3f}, \"\n",
    "                  f\"ref_id_score: {row['ref_id_score']:.3f}, \"\n",
    "                  f\"is_NA_score: {row['is_NA_score']:.3f}, \"\n",
    "                  f\"wattbot_score: {row['wattbot_score']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c3c7d5d-556c-4974-8f3c-d15ff9912eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows compared: 41\n",
      "Mean answer_value score: 0.5366\n",
      "Mean ref_id score:       0.0732\n",
      "Mean is_NA score:        0.9024\n",
      "Overall WattBot score:   0.5037\n",
      "\n",
      "Examples of incorrect / partially correct responses (up to 10 rows):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "id: q054\n",
      "Question: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "GT answer_value:   64.7\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['chen2024']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q202\n",
      "Question: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "GT answer_value:   Financial Sentiment Analysis\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['khan2025']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q280\n",
      "Question: Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.\n",
      "GT answer_value:   13\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['shen2024']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q316\n",
      "Question: In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?\n",
      "GT answer_value:   2510000\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['han2024']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q146\n",
      "Question: True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\n",
      "GT answer_value:   1\n",
      "Pred answer_value: 0\n",
      "GT ref_id:         ['khan2025']\n",
      "Pred ref_id:       khan2025;zschache2025;fernandez2025;luccioni2024\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q166\n",
      "Question: Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?\n",
      "GT answer_value:   GPT-3\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['patterson2021']\n",
      "Pred ref_id:       patterson2021\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q078\n",
      "Question: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "GT answer_value:   [0.02,0.1]\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       li2025b\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q003\n",
      "Question: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "GT answer_value:   ML.ENERGY Benchmark\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['chung2025']\n",
      "Pred ref_id:       chung2025\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q211\n",
      "Question: True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.\n",
      "GT answer_value:   1\n",
      "Pred answer_value: 0\n",
      "GT ref_id:         ['ebert2024']\n",
      "Pred ref_id:       ebert2024\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q246\n",
      "Question: True or False: New AI data centers often rely on air cooling due to high server power densities.\n",
      "GT answer_value:   0\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       li2025b\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=output_dir + \"/wattbot_solutions_bedrock.csv\",\n",
    "    gt_is_na_col=\"is_NA\",   # or \"is_blank\" / None depending on how you mark NAs\n",
    "    n_examples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a52c8b",
   "metadata": {},
   "source": [
    "## Wrap‑up: comparing Bedrock to GPU‑based runs\n",
    "\n",
    "At this point you should have three versions of the WattBot evaluation:\n",
    "\n",
    "1. **Episode 01 – Notebook GPU instance** using a locally loaded open‑source model.  \n",
    "2. **Episode 02 – SageMaker Processing job** running the same model in batch with on-demand compute. \n",
    "3. **Episode 03 – Bedrock** using a hosted Claude 3 model with per‑token billing.\n",
    "\n",
    "When deciding between these options in practice:\n",
    "\n",
    "- Use **Bedrock or other hosted APIs** when:\n",
    "  - You want to try the latest frontier models quickly.  \n",
    "  - You only need to run a modest number of questions, or you are still prototyping.  \n",
    "  - You prefer a simple, token‑based cost model and don’t want to manage GPU capacity.\n",
    "\n",
    "- Use **self‑hosted models on GPU instances** when:\n",
    "  - You expect to run large batches repeatedly (e.g., many thousands of questions).  \n",
    "  - You want tight control over which architectures/checkpoints you run or fine‑tune.  \n",
    "  - You already have institutional access to cost‑effective on‑prem or cloud GPUs.\n",
    "\n",
    "The core **RAG evaluation logic stays identical** across all three episodes, which is the main takeaway:\n",
    "once you have a clean retrieval + normalization pipeline (like WattBot’s), swapping out the generator\n",
    "is mostly a matter of re‑implementing `answer_phase_for_question` and `explanation_phase_for_question`\n",
    "for each compute option you care about.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
