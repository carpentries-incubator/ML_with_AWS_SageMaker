{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "version": "3.x"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Episode 03 \u2013 Evaluating WattBot RAG with Amazon Bedrock\n", "\n", "In this episode, we re-run the WattBot evaluation pipeline using a **hosted LLM on Amazon Bedrock** instead of:\n", "\n", "- a powerful notebook instance (Episode 01), or  \n", "- a SageMaker Processing job (Episode 02).\n", "\n", "The **retrieval and evaluation logic stays the same**. The only major change is how we generate answers:\n", "\n", "- Before: we loaded an open\u2011source model (Qwen) on our own GPU instance.\n", "- Now: we send prompts to a managed model on Amazon Bedrock (for example, Claude 3 Haiku or Sonnet) and pay **per token**.\n", "\n", "This gives you a template for plugging WattBot (or other RAG systems) into hosted frontier models.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Why use a hosted LLM for WattBot?\n", "\n", "Amazon Bedrock exposes popular, high\u2011capacity models from providers such as **Anthropic (Claude)**, **Meta (Llama)**, **Mistral**, **Cohere**, and **Amazon Titan**.  \n", "From a WattBot perspective, the pattern is similar to using other hosted APIs (including OpenAI APIs): you send a prompt and receive a completion, and you are **billed per token**, not per GPU\u2011hour.\n", "\n", "There are real trade\u2011offs here:\n", "\n", "- **Pros of hosted LLMs (Bedrock / OpenAI\u2011style APIs)**  \n", "  - You can use state\u2011of\u2011the\u2011art models without provisioning or patching GPU instances.  \n", "  - Scaling up/down is handled for you by the provider.  \n", "  - For **small or one\u2011off evaluations** (like a single WattBot run over a modest question set), token\u2011based pricing is often cheaper and much simpler to budget.\n", "\n", "- **Pros of running your own model on GPU instances (Episodes 01\u201302)**  \n", "  - You have full control over which model you run (including custom fine\u2011tunes).  \n", "  - If you run **many large batch jobs** or keep GPUs busy most of the time, paying by instance\u2011hour can be comparable or cheaper than token\u2011based APIs.  \n", "  - There are no provider\u2011level rate limits beyond what your infrastructure can handle.\n", "\n", "In practice, the choice depends on:\n", "\n", "- How many questions you need to answer.  \n", "- How large the model is and how many tokens you expect per question.  \n", "- How often you will repeat this evaluation.  \n", "- Whether your team is comfortable managing GPU infrastructure.\n", "\n", "In this notebook we\u2019ll keep the **RAG evaluation strategy identical** to earlier episodes and simply swap in a Bedrock model for generation.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setup: libraries, configuration, and Bedrock client\n", "\n", "This notebook assumes you:\n", "\n", "- Already ran Episodes 01\u201302 and uploaded the following artifacts to S3:\n", "  - `wattbot_chunks.jsonl` (RAG chunks)\n", "  - `embeddings.npy` (chunk embeddings)\n", "  - `train_QA.csv` (WattBot training questions)\n", "  - `metadata.csv` (document\u2011level metadata)\n", "- Have **Amazon Bedrock** enabled in your account and an Anthropic Claude 3 model available.\n", "\n", "You can adjust the S3 keys and model ID below to match your environment.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import os\n", "import json\n", "from typing import Dict, Any, List\n", "\n", "import boto3\n", "import pandas as pd\n", "import numpy as np\n", "\n", "from sentence_transformers import SentenceTransformer\n", "from botocore.exceptions import ClientError\n", "\n", "# ---- AWS configuration ----\n", "\n", "region = \"us-east-1\"  # Update if needed\n", "\n", "# Claude 3 Haiku is a good starting point for batch evaluation.\n", "# Swap for Sonnet/Opus if you have access and want higher quality.\n", "bedrock_model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n", "\n", "# S3 bucket + keys where Episode 02 wrote the artifacts.\n", "# TODO: Update these keys to match your pipeline.\n", "bucket_name = \"chris-rag\"  # <-- change to your bucket\n", "chunks_key = \"wattbot/chunks/wattbot_chunks.jsonl\"\n", "embeddings_key = \"wattbot/embeddings/embeddings.npy\"\n", "train_key = \"wattbot/train/train_QA.csv\"\n", "metadata_key = \"wattbot/metadata/metadata.csv\"\n", "\n", "# Local working directory for downloaded artifacts\n", "local_data_dir = \"data\"\n", "os.makedirs(local_data_dir, exist_ok=True)\n", "\n", "# AWS clients\n", "s3 = boto3.client(\"s3\", region_name=region)\n", "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["def download_from_s3(key: str, local_name: str) -> str:\n", "    \"\"\"Download a file from S3 to local_data_dir and return the local path.\"\"\"\n", "    local_path = os.path.join(local_data_dir, local_name)\n", "    print(f\"Downloading s3://{bucket_name}/{key} -> {local_path}\")\n", "    s3.download_file(bucket_name, key, local_path)\n", "    return local_path\n", "\n", "\n", "chunks_path = download_from_s3(chunks_key, \"wattbot_chunks.jsonl\")\n", "emb_path = download_from_s3(embeddings_key, \"embeddings.npy\")\n", "train_qa_path = download_from_s3(train_key, \"train_QA.csv\")\n", "metadata_path = download_from_s3(metadata_key, \"metadata.csv\")\n", "\n", "# Load artifacts\n", "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n", "    chunked_docs = [json.loads(line) for line in f]\n", "\n", "chunk_embeddings = np.load(emb_path)\n", "train_df = pd.read_csv(train_qa_path)\n", "\n", "# Robust metadata load: handle possible non-UTF-8 characters\n", "try:\n", "    metadata_df = pd.read_csv(metadata_path)\n", "except UnicodeDecodeError:\n", "    metadata_df = pd.read_csv(metadata_path, encoding=\"latin1\")\n", "\n", "print(f\"Chunks: {len(chunked_docs)}\")\n", "print(f\"Train QAs: {len(train_df)}\")\n", "print(\"Embeddings shape:\", chunk_embeddings.shape)\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# Build doc_id -> url mapping from metadata\n", "docid_to_url: Dict[str, str] = {}\n", "for _, row in metadata_df.iterrows():\n", "    doc_id = str(row.get(\"id\", \"\")).strip()\n", "    url = row.get(\"url\", \"\")\n", "    if doc_id and isinstance(url, str) and url.strip():\n", "        docid_to_url[doc_id] = url.strip()\n", "\n", "print(f\"Metadata doc URLs: {len(docid_to_url)} entries\")\n", "\n", "# Load the same embedding model we used earlier\n", "embedding_model_id = \"thenlper/gte-large\"\n", "embedder = SentenceTransformer(embedding_model_id)\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# ---------------------- similarity + retrieval ----------------------\n", "\n", "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n", "    \"\"\"Cosine similarity between two sets of vectors.\"\"\"\n", "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n", "    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\n", "    return np.matmul(a_norm, b_norm.T)\n", "\n", "\n", "def retrieve_top_k(\n", "    query_embedding: np.ndarray,\n", "    chunk_embeddings: np.ndarray,\n", "    chunked_docs: List[Dict[str, Any]],\n", "    k: int = 8,\n", ") -> List[Dict[str, Any]]:\n", "    \"\"\"Return the top\u2013k chunks for a single query embedding.\"\"\"\n", "    query = query_embedding.reshape(1, -1)\n", "    sims = cosine_similarity_matrix(query, chunk_embeddings)[0]\n", "\n", "    top_idx = np.argsort(-sims)[:k]\n", "\n", "    results = []\n", "    for idx in top_idx:\n", "        ch = chunked_docs[idx]\n", "        results.append(\n", "            {\n", "                \"score\": float(sims[idx]),\n", "                \"text\": ch[\"text\"],\n", "                \"doc_id\": ch.get(\"doc_id\", \"\"),\n", "                \"title\": ch.get(\"title\", \"\"),\n", "                \"url\": ch.get(\"url\", \"\"),\n", "                \"page_num\": ch.get(\"page_num\", None),\n", "                \"page_label\": ch.get(\"page_label\", None),\n", "            }\n", "        )\n", "    return results\n", "\n", "\n", "def format_context_for_prompt(retrieved_chunks: List[Dict[str, Any]]) -> str:\n", "    \"\"\"Turn retrieved chunk dicts into a compact context string for the LLM.\"\"\"\n", "    lines = []\n", "    for i, ch in enumerate(retrieved_chunks, start=1):\n", "        label = ch.get(\"doc_id\", f\"chunk_{i}\")\n", "        page = ch.get(\"page_label\", ch.get(\"page_num\", \"\"))\n", "        header = f\"[{label}, page {page}]\".strip()\n", "        txt = ch[\"text\"].replace(\"\\n\", \" \")\n", "        lines.append(f\"{header}: {txt}\")\n", "    return \"\\n\".join(lines)\n", "\n", "\n", "def retrieve_context_for_question(\n", "    question: str,\n", "    embedder: SentenceTransformer,\n", "    chunk_embeddings: np.ndarray,\n", "    chunked_docs: List[Dict[str, Any]],\n", "    top_k: int = 8,\n", "):\n", "    q_emb = embedder.encode([question], convert_to_numpy=True, normalize_embeddings=True)[0]\n", "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n", "    return retrieved, q_emb\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# ---------------------- answer normalization ----------------------\n", "\n", "def normalize_answer_value(raw_value: str) -> str:\n", "    \"\"\"Normalize answer_value according to WattBot conventions.\"\"\"\n", "    if raw_value is None:\n", "        return \"is_blank\"\n", "\n", "    s = str(raw_value).strip()\n", "\n", "    if not s or s.lower() == \"none\":\n", "        return \"is_blank\"\n", "\n", "    if s.startswith(\"[\") and s.endswith(\"]\"):\n", "        return s\n", "\n", "    if s.lower() == \"is_blank\":\n", "        return \"is_blank\"\n", "\n", "    # If there is whitespace, keep only the first token\n", "    if \" \" in s:\n", "        first, *_ = s.split()\n", "        s = first\n", "\n", "    # Remove commas\n", "    s = s.replace(\",\", \"\")\n", "\n", "    try:\n", "        val = float(s)\n", "        if val.is_integer():\n", "            return str(int(val))\n", "        return f\"{val:.10g}\"  # avoid scientific notation\n", "    except ValueError:\n", "        return s\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## Calling Claude 3 on Amazon Bedrock\n", "\n", "Next, we define a small helper that:\n", "\n", "- Formats a request for the Claude 3 Messages API on Bedrock.\n", "- Sends the request with `bedrock-runtime.invoke_model`.\n", "- Returns the generated text string.\n", "\n", "(The response also includes token usage; you can extend this function to track total\n", "input/output tokens for cost estimation if you\u2019d like.)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def call_bedrock_claude(\n", "    system_prompt: str,\n", "    user_prompt: str,\n", "    model_id: str = bedrock_model_id,\n", "    max_tokens: int = 512,\n", "    temperature: float = 0.3,\n", ") -> str:\n", "    \"\"\"Call an Anthropic Claude 3 model on Bedrock and return the text response.\"\"\"\n", "    body = {\n", "        \"anthropic_version\": \"bedrock-2023-05-31\",\n", "        \"max_tokens\": max_tokens,\n", "        \"temperature\": temperature,\n", "        \"system\": system_prompt,\n", "        \"messages\": [\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": [{\"type\": \"text\", \"text\": user_prompt}],\n", "            }\n", "        ],\n", "    }\n", "\n", "    request = json.dumps(body)\n", "    try:\n", "        response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n", "    except ClientError as e:\n", "        print(f\"ERROR calling Bedrock model {model_id}: {e}\")\n", "        raise\n", "\n", "    model_response = json.loads(response[\"body\"].read())\n", "    text = model_response[\"content\"][0][\"text\"]\n", "    return text.strip()\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# ---------------------- explanation helpers ----------------------\n", "\n", "def build_explanation_prompt(question: str, answer: str, supporting_materials: str) -> str:\n", "    return (\n", "        \"You are explaining answers for an energy, water, and carbon footprint assistant.\\n\\n\"\n", "        f\"Question: {question}\\n\\n\"\n", "        f\"Answer: {answer}\\n\\n\"\n", "        f\"Supporting materials:\\n{supporting_materials}\\n\\n\"\n", "        \"In 1\u20133 sentences, explain how the supporting materials justify the answer. \"\n", "        \"Be precise but concise.\"\n", "    )\n", "\n", "\n", "def explanation_system_prompt() -> str:\n", "    return (\n", "        \"You are an AI assistant that explains how evidence supports answers about \"\n", "        \"energy, water, and carbon footprint. Focus on clear, factual reasoning, \"\n", "        \"and refer directly to the cited documents when appropriate.\"\n", "    )\n", "\n", "\n", "def bedrock_explanation_phase_for_question(\n", "    qid: str,\n", "    question: str,\n", "    answer: str,\n", "    supporting_materials: str,\n", "    model_id: str = bedrock_model_id,\n", ") -> str:\n", "    sys_prompt = explanation_system_prompt()\n", "    prompt = build_explanation_prompt(question, answer, supporting_materials)\n", "    raw_explanation = call_bedrock_claude(\n", "        system_prompt=sys_prompt,\n", "        user_prompt=prompt,\n", "        model_id=model_id,\n", "        max_tokens=256,\n", "    )\n", "    return raw_explanation.strip()\n", "\n", "\n", "# ---------------------- answer phase (JSON contract) ----------------------\n", "\n", "def bedrock_answer_phase_for_question(\n", "    qid: str,\n", "    question: str,\n", "    retrieved_chunks: List[Dict[str, Any]],\n", "    model_id: str = bedrock_model_id,\n", "):\n", "    \"\"\"Use Claude 3 on Bedrock to answer a single WattBot question given retrieved chunks.\"\"\"\n", "    context = format_context_for_prompt(retrieved_chunks)\n", "\n", "    system_prompt = (\n", "        \"You are WattBot, a question-answering assistant for energy, water, and carbon footprint.\\n\"\n", "        \"You must answer questions using ONLY the provided context from scientific papers.\\n\"\n", "        \"If the context does not contain enough information to answer with high confidence,\\n\"\n", "        \"you must mark the question as unanswerable.\\n\\n\"\n", "        \"You must respond with a single JSON object with the following keys:\\n\"\n", "        \"- answer: natural language answer, including numeric value and units if applicable.\\n\"\n", "        \"- answer_value: normalized numeric or categorical value with NO units or symbols;\\n\"\n", "        \"  use 'is_blank' if the question is unanswerable.\\n\"\n", "        \"- answer_unit: unit string (e.g., kWh, gCO2, %, is_blank).\\n\"\n", "        \"- ref_id: list of document IDs that support the answer.\\n\"\n", "        \"- is_blank: true if unanswerable, false otherwise.\\n\"\n", "        \"- supporting_materials: short quote or table/figure pointer from the context.\\n\"\n", "    )\n", "\n", "    user_prompt = (\n", "        \"Use the context below to answer the question. \"\n", "        \"Return ONLY a JSON object, no extra commentary.\\n\\n\"\n", "        f\"Question: {question}\\n\\n\"\n", "        f\"Context:\\n{context}\\n\"\n", "    )\n", "\n", "    raw_answer = call_bedrock_claude(\n", "        system_prompt=system_prompt,\n", "        user_prompt=user_prompt,\n", "        model_id=model_id,\n", "        max_tokens=512,\n", "    )\n", "\n", "    parsed = {\n", "        \"answer\": \"\",\n", "        \"answer_value\": \"is_blank\",\n", "        \"answer_unit\": \"is_blank\",\n", "        \"ref_id\": [],\n", "        \"is_blank\": True,\n", "        \"supporting_materials\": \"is_blank\",\n", "    }\n", "\n", "    try:\n", "        first_brace = raw_answer.find(\"{\")\n", "        last_brace = raw_answer.rfind(\"}\")\n", "        if first_brace != -1 and last_brace != -1:\n", "            json_str = raw_answer[first_brace : last_brace + 1]\n", "        else:\n", "            json_str = raw_answer\n", "\n", "        candidate = json.loads(json_str)\n", "\n", "        parsed[\"answer\"] = candidate.get(\"answer\", \"\").strip()\n", "        parsed[\"answer_value\"] = normalize_answer_value(candidate.get(\"answer_value\", \"is_blank\"))\n", "        parsed[\"answer_unit\"] = str(candidate.get(\"answer_unit\", \"is_blank\")).strip() or \"is_blank\"\n", "\n", "        ref_id = candidate.get(\"ref_id\", [])\n", "        if isinstance(ref_id, str):\n", "            ref_ids = [ref_id]\n", "        elif isinstance(ref_id, list):\n", "            ref_ids = [str(x).strip() for x in ref_id if x]\n", "        else:\n", "            ref_ids = []\n", "        parsed[\"ref_id\"] = ref_ids\n", "\n", "        is_blank_flag = candidate.get(\"is_blank\", False)\n", "        parsed[\"is_blank\"] = bool(is_blank_flag)\n", "\n", "        supp = candidate.get(\"supporting_materials\", \"is_blank\")\n", "        parsed[\"supporting_materials\"] = str(supp).strip() or \"is_blank\"\n", "\n", "    except Exception as e:\n", "        print(f\"JSON parse error for question {qid}; defaulting to is_blank. Error: {e}\")\n", "\n", "    return (\n", "        parsed[\"answer\"],\n", "        parsed[\"answer_value\"],\n", "        parsed[\"is_blank\"],\n", "        parsed[\"ref_id\"],\n", "        parsed[\"supporting_materials\"],\n", "    )\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["def run_single_qa_bedrock(\n", "    row: pd.Series,\n", "    embedder: SentenceTransformer,\n", "    chunk_embeddings: np.ndarray,\n", "    chunked_docs: List[Dict[str, Any]],\n", "    docid_to_url: Dict[str, str],\n", "    top_k: int = 8,\n", "    retrieval_threshold: float = 0.25,\n", "    model_id: str = bedrock_model_id,\n", ") -> Dict[str, Any]:\n", "    \"\"\"Full RAG + Bedrock pipeline for a single question.\"\"\"\n", "    qid = row[\"id\"]\n", "    question = row[\"question\"]\n", "\n", "    retrieved, q_emb = retrieve_context_for_question(\n", "        question=question,\n", "        embedder=embedder,\n", "        chunk_embeddings=chunk_embeddings,\n", "        chunked_docs=chunked_docs,\n", "        top_k=top_k,\n", "    )\n", "\n", "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n", "\n", "    (\n", "        answer,\n", "        answer_value,\n", "        is_blank_llm,\n", "        ref_ids,\n", "        supporting_materials,\n", "    ) = bedrock_answer_phase_for_question(\n", "        qid=qid,\n", "        question=question,\n", "        retrieved_chunks=retrieved,\n", "        model_id=model_id,\n", "    )\n", "\n", "    is_blank = bool(is_blank_llm) or (top_score < retrieval_threshold)\n", "\n", "    if is_blank:\n", "        answer = \"Unable to answer with confidence based on the provided documents.\"\n", "        answer_value = \"is_blank\"\n", "        answer_unit = \"is_blank\"\n", "        ref_ids = []\n", "        ref_id_str = \"is_blank\"\n", "        ref_url_str = \"is_blank\"\n", "        supporting_materials = \"is_blank\"\n", "        explanation = \"\"\n", "    else:\n", "        answer_value = normalize_answer_value(answer_value)\n", "        answer_unit = \"is_blank\"\n", "\n", "        if isinstance(ref_ids, list) and ref_ids:\n", "            ref_id_str = \";\".join(ref_ids)\n", "            urls = []\n", "            for rid in ref_ids:\n", "                url = docid_to_url.get(str(rid), \"\")\n", "                if url:\n", "                    urls.append(url)\n", "            ref_url_str = \";\".join(urls) if urls else \"is_blank\"\n", "        else:\n", "            ref_id_str = \"is_blank\"\n", "            ref_url_str = \"is_blank\"\n", "\n", "        explanation = bedrock_explanation_phase_for_question(\n", "            qid=qid,\n", "            question=question,\n", "            answer=answer,\n", "            supporting_materials=supporting_materials,\n", "            model_id=model_id,\n", "        )\n", "\n", "    return {\n", "        \"id\": qid,\n", "        \"question\": question,\n", "        \"answer\": answer,\n", "        \"answer_value\": answer_value,\n", "        \"answer_unit\": answer_unit,\n", "        \"ref_id\": ref_id_str,\n", "        \"ref_url\": ref_url_str,\n", "        \"supporting_materials\": supporting_materials,\n", "        \"explanation\": explanation,\n", "    }\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run the WattBot evaluation with Bedrock\n", "\n", "Now we can loop over all questions in `train_QA.csv`, run retrieval + Bedrock\n", "generation, and write a `wattbot_solutions_bedrock.csv` file.\n", "\n", "This mirrors the logic from Episode 02 \u2013 the only difference is that the answer\n", "and explanation phases call a hosted Claude 3 model instead of a local Qwen model.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["results = []\n", "\n", "# For quick smoke tests, you can slice train_df (e.g., train_df.head(5))\n", "for _, row in train_df.iterrows():\n", "    out = run_single_qa_bedrock(\n", "        row=row,\n", "        embedder=embedder,\n", "        chunk_embeddings=chunk_embeddings,\n", "        chunked_docs=chunked_docs,\n", "        docid_to_url=docid_to_url,\n", "        top_k=8,\n", "        retrieval_threshold=0.25,\n", "        model_id=bedrock_model_id,\n", "    )\n", "    results.append(out)\n", "\n", "results_df = pd.DataFrame(results)\n", "\n", "output_dir = \"outputs\"\n", "os.makedirs(output_dir, exist_ok=True)\n", "output_path = os.path.join(output_dir, \"wattbot_solutions_bedrock.csv\")\n", "\n", "results_df.to_csv(output_path, index=False)\n", "print(f\"Wrote predictions to {output_path}\")\n", "\n", "results_df.head()\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## Wrap\u2011up: comparing Bedrock to GPU\u2011based runs\n", "\n", "At this point you should have three versions of the WattBot evaluation:\n", "\n", "1. **Episode 01 \u2013 Notebook GPU instance** using a locally loaded open\u2011source model.  \n", "2. **Episode 02 \u2013 SageMaker Processing job** running the same model in batch.  \n", "3. **Episode 03 \u2013 Bedrock** using a hosted Claude 3 model with per\u2011token billing.\n", "\n", "When deciding between these options in practice:\n", "\n", "- Use **Bedrock or other hosted APIs** when:\n", "  - You want to try the latest frontier models quickly.  \n", "  - You only need to run a modest number of questions, or you are still prototyping.  \n", "  - You prefer a simple, token\u2011based cost model and don\u2019t want to manage GPU capacity.\n", "\n", "- Use **self\u2011hosted models on GPU instances** when:\n", "  - You expect to run large batches repeatedly (e.g., many thousands of questions).  \n", "  - You want tight control over which architectures/checkpoints you run or fine\u2011tune.  \n", "  - You already have institutional access to cost\u2011effective on\u2011prem or cloud GPUs.\n", "\n", "The core **RAG evaluation logic stays identical** across all three episodes, which is the main takeaway:\n", "once you have a clean retrieval + normalization pipeline (like WattBot\u2019s), swapping out the generator\n", "is mostly a matter of re\u2011implementing `answer_phase_for_question` and `explanation_phase_for_question`\n", "for each compute option you care about.\n"]}]}