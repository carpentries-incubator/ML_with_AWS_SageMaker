{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef53255",
   "metadata": {},
   "source": [
    "# Episode 03 – Evaluating WattBot RAG with Amazon Bedrock\n",
    "\n",
    "In the previous episodes you built a basic RAG pipeline for WattBot using a local GPU instance\n",
    "and then an offline SageMaker Processing job. Both approaches gave you full control over the\n",
    "models, but you were responsible for provisioning compute and keeping model versions up to date.\n",
    "\n",
    "In this episode we move the core model work — **both text embeddings and answer generation** —\n",
    "onto **Amazon Bedrock**. We'll use:\n",
    "\n",
    "- an **Amazon Titan Text Embeddings V2** model to turn WattBot chunks into vectors, and  \n",
    "- an **Anthropic Claude** model hosted on Bedrock to generate answers and explanations.\n",
    "\n",
    "The retrieval, evaluation, and WattBot scoring logic are exactly the same as before; we're just\n",
    "swapping out the underlying models and where they run. This lets you experiment with hosted,\n",
    "state‑of‑the‑art models without having to manage GPUs or container images yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d7f19",
   "metadata": {},
   "source": [
    "## Why Bedrock for WattBot?\n",
    "\n",
    "For the GPU instance and Processing Job episodes, you were responsible for picking a model,\n",
    "managing versions, and making sure your instance had enough VRAM. That’s fine for experiments,\n",
    "but it can get painful once multiple teams or challenges want to reuse the same pipeline.\n",
    "\n",
    "Running your **embedding + generation** steps on Amazon Bedrock gives you a few nice properties:\n",
    "\n",
    "- **Managed, up‑to‑date models.** You can use high‑quality models from Anthropic, Amazon, and\n",
    "  others without worrying about container images or CUDA versions.\n",
    "- **Pay for what you use (in tokens).** Instead of paying for a GPU instance that might sit\n",
    "  idle, you pay per token (input + output) when you call the model. For some workloads this\n",
    "  is cheaper; for large offline batches with smaller models, a dedicated GPU can still win.\n",
    "- **Easier sharing and governance.** It’s easier to standardize on a small set of Bedrock\n",
    "  models across courses, hackathons, or labs than to manage many separate GPU instances.\n",
    "\n",
    "In this notebook, we’ll keep the same WattBot training questions and scoring helper you used\n",
    "before, and we’ll simply move both the **embedding** and **answer/explanation** steps onto\n",
    "Bedrock-hosted models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c83d3",
   "metadata": {},
   "source": [
    "## Setup: what you should already have\n",
    "\n",
    "This notebook assumes you have already run the earlier WattBot episodes so that:\n",
    "\n",
    "- the WattBot corpus has been chunked into `wattbot_chunks.jsonl`\n",
    "- the WattBot training questions `train_QA.csv` and `metadata.csv` live under a `data/` folder\n",
    "- (optionally) you have a local embedding file from earlier experiments, e.g. `embeddings.npy`\n",
    "\n",
    "In this episode we’ll recompute embeddings **using an Amazon Titan Text Embeddings V2 model\n",
    "on Bedrock**, and we’ll save those vectors out as `embeddings_bedrock.npy`. That keeps this\n",
    "notebook self‑contained while still letting you compare against the earlier GPU / Processing\n",
    "Job runs if you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46947720-f2fd-4eab-854c-d602cefef675",
   "metadata": {},
   "source": [
    "### Models used in this episode\n",
    "\n",
    "We’ll work with **Amazon Bedrock–hosted foundation models** for both embedding and generation:\n",
    "\n",
    "- **Amazon Titan Text Embeddings V2** (`amazon.titan-embed-text-v2:0`)\n",
    "\n",
    "  - General‑purpose text‑embedding model for semantic search, retrieval, clustering, and classification.\n",
    "  - Supports configurable embedding dimensions (for example 256–8,192) and has presets tuned for retrieval or binary indexing.\n",
    "  - AWS does not publish the exact number of parameters for Titan models; you can treat it as a modern transformer specialized for embeddings rather than free‑form text generation. \n",
    "\n",
    "- **Anthropic Claude 3 Haiku** (`anthropic.claude-3-haiku-20240307-v1:0` via Bedrock)\n",
    "\n",
    "  - A fast, mid‑sized Claude model that balances cost and quality for workloads like RAG, chat, and lightweight analysis.\n",
    "  - Particularly useful when you want many calls (e.g., one per question) and care about low latency and lower per‑token pricing compared to flagship models such as Claude Opus or Claude 3.5 Sonnet. \n",
    "  - Anthropic does not publish exact parameter counts for Claude models; Haiku sits in the “smallest / fastest” tier within the Claude 3 family.\n",
    "\n",
    "- **(Optional) Multimodal models for tables and figures**\n",
    "\n",
    "  - Bedrock also exposes **multimodal models** that can reason over images, charts, and document layouts (for example, Claude 3.5 Sonnet with vision, or Amazon Titan Multimodal Embeddings). These are a good fit if much of your evidence lives in **figures, tables, or scanned PDFs**.\n",
    "  - To use them from Bedrock you send **both text and image content** in a single request:\n",
    "      - Pre‑process PDFs by rendering pages (or cropping individual tables/figures) to images using a tool like `pdf2image` or a headless browser.\n",
    "      - Base64‑encode those images and include them as image parts alongside text in the model request.\n",
    "      - For multimodal embeddings, you call a Titan multimodal embedding model with an `inputImage` (and optionally `inputText`) payload to obtain a single vector that mixes visual and textual information.\n",
    "  - This notebook stays with **text‑only** embeddings + generation to keep the workflow simple, but the same RAG pattern extends naturally to multimodal models once you add an image‑extraction step to your preprocessing pipeline.\n",
    "\n",
    "For a full catalog of available models (including other Claude variants, Amazon models, and partner models), open the **Model catalog** in the Amazon Bedrock console. Each entry provides a model card with capabilities, typical use cases, and pricing details so learners can explore alternatives for their own RAG systems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fe3b41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# ---- AWS configuration ----\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Claude 3 Haiku is a good starting point for batch evaluation.\n",
    "# Swap for Sonnet/Opus if you have access and want higher quality.\n",
    "bedrock_model_id = \"deepseek.v3-v1:0\"\n",
    "\n",
    "# S3 bucket + keys where Episode 02 wrote the artifacts.\n",
    "# TODO: Update these keys to match your pipeline.\n",
    "bucket_name = \"chris-rag\"  # <-- change to your bucket\n",
    "chunks_key = \"wattbot_chunks.jsonl\"\n",
    "# embeddings_key = \"embeddings/embeddings.npy\"\n",
    "train_key = \"train_QA.csv\"\n",
    "metadata_key = \"metadata.csv\"\n",
    "\n",
    "# Local working directory for downloaded artifacts\n",
    "local_data_dir = \"bedrock\"\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "# AWS clients\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cbce0d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading s3://chris-rag/wattbot_chunks.jsonl -> bedrock/wattbot_chunks.jsonl\n",
      "Downloading s3://chris-rag/train_QA.csv -> bedrock/train_QA.csv\n",
      "Downloading s3://chris-rag/metadata.csv -> bedrock/metadata.csv\n",
      "Chunks: 2874\n",
      "Train QAs: 41\n"
     ]
    }
   ],
   "source": [
    "def download_from_s3(key: str, local_name: str) -> str:\n",
    "    \"\"\"Download a file from S3 to local_data_dir and return the local path.\"\"\"\n",
    "    local_path = os.path.join(local_data_dir, local_name)\n",
    "    print(f\"Downloading s3://{bucket_name}/{key} -> {local_path}\")\n",
    "    s3.download_file(bucket_name, key, local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "chunks_path = download_from_s3(chunks_key, \"wattbot_chunks.jsonl\")\n",
    "# emb_path = download_from_s3(embeddings_key, \"embeddings.npy\")\n",
    "train_qa_path = download_from_s3(train_key, \"train_QA.csv\")\n",
    "metadata_path = download_from_s3(metadata_key, \"metadata.csv\")\n",
    "\n",
    "# Load artifacts\n",
    "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunked_docs = [json.loads(line) for line in f]\n",
    "\n",
    "# chunk_embeddings = np.load(emb_path)\n",
    "train_df = pd.read_csv(train_qa_path)\n",
    "\n",
    "# Robust metadata load: handle possible non-UTF-8 characters\n",
    "try:\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "except UnicodeDecodeError:\n",
    "    metadata_df = pd.read_csv(metadata_path, encoding=\"latin1\")\n",
    "\n",
    "print(f\"Chunks: {len(chunked_docs)}\")\n",
    "print(f\"Train QAs: {len(train_df)}\")\n",
    "# print(\"Embeddings shape:\", chunk_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "380ee16b-1cc1-4c0e-8cc8-47677569c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_for_question_bedrock(\n",
    "    question: str,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve top-k chunks for a question using Bedrock embeddings.\n",
    "\n",
    "    We call the Bedrock embedding model (via `bedrock_embed_text`) to\n",
    "    embed the question, then compute cosine similarity against the\n",
    "    pre-computed `chunk_embeddings` array.\n",
    "    \"\"\"\n",
    "    # Embed the question with the same Bedrock model used for chunks\n",
    "    q_emb = bedrock_embed_text(question)\n",
    "\n",
    "    # Use the same cosine similarity + top-k helper as before\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    return retrieved, q_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "997ead87-85cd-4e2e-a39a-f3811f976fa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "95f183fc-c5b5-45f3-aeab-11620b94da43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docid_to_url has 32 entries.\n"
     ]
    }
   ],
   "source": [
    "# Build a mapping from doc_id -> URL so we can surface links in our outputs\n",
    "docid_to_url = {}\n",
    "for _, row in metadata_df.iterrows():\n",
    "    doc_id = str(row.get(\"id\", \"\")).strip()\n",
    "    url = row.get(\"url\", \"\")\n",
    "    if doc_id and isinstance(url, str) and url.strip():\n",
    "        docid_to_url[doc_id] = url.strip()\n",
    "\n",
    "print(f\"docid_to_url has {len(docid_to_url)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "882dbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing embeddings at data/embeddings_bedrock.npy. Skipping re-computation.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Bedrock embeddings for WattBot chunks\n",
    "# ----------------------------------------------------------------------------------\n",
    "embedding_model_id_bedrock = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "emb_save_path = data_dir / \"embeddings_bedrock.npy\"\n",
    "\n",
    "def bedrock_embed_text(text: str, model_id: str = embedding_model_id_bedrock):\n",
    "    \"\"\"Call a Bedrock embedding model for a single input string.\"\"\"\n",
    "    body = json.dumps({\"inputText\": text})\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body,\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    if embedding is None:\n",
    "        raise ValueError(f\"No 'embedding' found in response: {response_body}\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# If an embedding file already exists, skip recomputing and load it instead\n",
    "# -------------------------------------------------------------------------\n",
    "if emb_save_path.exists():\n",
    "    print(f\"Found existing embeddings at {emb_save_path}. Skipping re-computation.\")\n",
    "    chunk_embeddings = np.load(emb_save_path)\n",
    "else:\n",
    "    print(\"No existing embeddings found. Computing via Bedrock...\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    for idx, ch in enumerate(chunked_docs):\n",
    "        if (idx + 1) % 250 == 0:\n",
    "            print(f\"Embedding chunk {idx+1} / {len(chunked_docs)}\")\n",
    "        text = ch.get(\"text\", \"\")\n",
    "        emb = bedrock_embed_text(text)\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    chunk_embeddings = np.array(all_embeddings, dtype=\"float32\")\n",
    "\n",
    "    # Save embeddings for reuse\n",
    "    np.save(emb_save_path, chunk_embeddings)\n",
    "    print(f\"Saved embeddings to {emb_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "65aeb8c1-85bd-43bb-a248-3e6ad2c5c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Bedrock chunk embeddings to data/embeddings_bedrock.npy\n",
      "Embeddings shape: (2874, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings so we can reuse them later without re-calling Bedrock\n",
    "np.save(emb_save_path, chunk_embeddings)\n",
    "print(\"Saved Bedrock chunk embeddings to\", emb_save_path)\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "325f97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- similarity + retrieval ----------------------\n",
    "\n",
    "# ---------------------- similarity + retrieval ----------------------\n",
    "\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Cosine similarity between two sets of vectors.\n",
    "\n",
    "    This helper is intentionally defensive: it will accept Python lists,\n",
    "    list-of-lists, or NumPy arrays and cast everything to float32 arrays\n",
    "    before computing similarities.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=\"float32\")\n",
    "    b = np.asarray(b, dtype=\"float32\")\n",
    "\n",
    "    # Ensure 2D\n",
    "    if a.ndim == 1:\n",
    "        a = a.reshape(1, -1)\n",
    "    if b.ndim == 1:\n",
    "        b = b.reshape(1, -1)\n",
    "\n",
    "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return np.matmul(a_norm, b_norm.T)\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_top_k(\n",
    "    query_embedding: np.ndarray,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    k: int = 8,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return the top–k chunks for a single query embedding.\n",
    "\n",
    "    Accepts query/collection embeddings as either NumPy arrays or lists.\n",
    "    \"\"\"\n",
    "    # Defensive casting in case we accidentally pass in lists\n",
    "    query = np.asarray(query_embedding, dtype=\"float32\").reshape(1, -1)\n",
    "    chunks = np.asarray(chunk_embeddings, dtype=\"float32\")\n",
    "\n",
    "    sims = cosine_similarity_matrix(query, chunks)[0]\n",
    "\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        ch = chunked_docs[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"text\": ch[\"text\"],\n",
    "                \"doc_id\": ch.get(\"doc_id\", \"\"),\n",
    "                \"title\": ch.get(\"title\", \"\"),\n",
    "                \"url\": ch.get(\"url\", \"\"),\n",
    "                \"page_num\": ch.get(\"page_num\", None),\n",
    "                \"page_label\": ch.get(\"page_label\", None),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def format_context_for_prompt(retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Turn retrieved chunk dicts into a compact context string for the LLM.\"\"\"\n",
    "    lines = []\n",
    "    for i, ch in enumerate(retrieved_chunks, start=1):\n",
    "        label = ch.get(\"doc_id\", f\"chunk_{i}\")\n",
    "        page = ch.get(\"page_label\", ch.get(\"page_num\", \"\"))\n",
    "        header = f\"[{label}, page {page}]\".strip()\n",
    "        txt = ch[\"text\"].replace(\"\\n\", \" \")\n",
    "        lines.append(f\"{header}: {txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def retrieve_context_for_question(\n",
    "    question: str,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"Use Bedrock embeddings to retrieve the top-k chunks for a question.\"\"\"\n",
    "    # Embed question with Bedrock and make sure we end up with a 1D float32 vector\n",
    "    q_vec = bedrock_embed_text(question)\n",
    "    q_emb = np.asarray(q_vec, dtype=\"float32\")\n",
    "\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    return retrieved, q_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "46b86f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- answer normalization ----------------------\n",
    "\n",
    "def normalize_answer_value(raw_value: str) -> str:\n",
    "    \"\"\"Normalize answer_value according to WattBot conventions.\"\"\"\n",
    "    if raw_value is None:\n",
    "        return \"is_blank\"\n",
    "\n",
    "    s = str(raw_value).strip()\n",
    "\n",
    "    if not s or s.lower() == \"none\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        return s\n",
    "\n",
    "    if s.lower() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # If there is whitespace, keep only the first token\n",
    "    if \" \" in s:\n",
    "        first, *_ = s.split()\n",
    "        s = first\n",
    "\n",
    "    # Remove commas\n",
    "    s = s.replace(\",\", \"\")\n",
    "\n",
    "    try:\n",
    "        val = float(s)\n",
    "        if val.is_integer():\n",
    "            return str(int(val))\n",
    "        return f\"{val:.10g}\"  # avoid scientific notation\n",
    "    except ValueError:\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e25c1f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock_claude(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    model_id: str = bedrock_model_id,\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.3,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call a Bedrock chat model (Anthropic 4.x / Claude 3.5 / Llama 3.x, etc.)\n",
    "    that uses the OpenAI-style chat completions schema.\n",
    "    \"\"\"\n",
    "    # OpenAI-style chat body – this is what your error message is asking for\n",
    "    body = {\n",
    "        \"model\": model_id,  # some models allow omitting this, but it's safe to include\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    request = json.dumps(body)\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR calling Bedrock model {model_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # OpenAI-style response: choices[0].message.content\n",
    "    try:\n",
    "        text = model_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception:\n",
    "        # Fallback / debug\n",
    "        print(\"Unexpected model response:\", model_response)\n",
    "        raise\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dc6b61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- explanation helpers ----------------------\n",
    "\n",
    "def explanation_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an AI assistant that explains how evidence supports answers about \"\n",
    "        \"energy, water, and carbon footprint of AI models.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Write 1–3 sentences.\\n\"\n",
    "        \"- Directly explain how the cited supporting materials justify the answer.\\n\"\n",
    "        \"- Do NOT include any planning text, meta-reasoning, or tags like <reasoning>.\\n\"\n",
    "        \"- Do NOT start with phrases like 'We need to answer'—just give the explanation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def explanation_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an AI assistant that explains how evidence supports answers about \"\n",
    "        \"energy, water, and carbon footprint. Focus on clear, factual reasoning, \"\n",
    "        \"and refer directly to the cited documents when appropriate.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def bedrock_explanation_phase_for_question(\n",
    "    qid: str,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    supporting_materials: str,\n",
    "    model_id: str = bedrock_model_id,\n",
    ") -> str:\n",
    "    sys_prompt = explanation_system_prompt()\n",
    "    prompt = build_explanation_prompt(question, answer, supporting_materials)\n",
    "    raw_explanation = call_bedrock_claude(\n",
    "        system_prompt=sys_prompt,\n",
    "        user_prompt=prompt,\n",
    "        model_id=model_id,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    return raw_explanation.strip()\n",
    "\n",
    "\n",
    "# ---------------------- answer phase (JSON contract) ----------------------\n",
    "\n",
    "def bedrock_answer_phase_for_question(\n",
    "    qid: str,\n",
    "    question: str,\n",
    "    retrieved_chunks: List[Dict[str, Any]],\n",
    "    model_id: str = bedrock_model_id,\n",
    "):\n",
    "    \"\"\"Use Bedrock to answer a single WattBot question given retrieved chunks.\"\"\"\n",
    "    context = format_context_for_prompt(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are WattBot, a question-answering assistant for energy, water, and carbon footprint.\\n\"\n",
    "        \"You must answer questions using ONLY the provided context from scientific papers.\\n\"\n",
    "        \"If the context does not contain enough information to answer or infer,\\n\"\n",
    "        \"you must mark the question as unanswerable.\\n\\n\"\n",
    "        \"You must respond with a single JSON object with the following keys:\\n\"\n",
    "        \"- answer: natural language answer, including numeric value and units if applicable.\\n\"\n",
    "        \"- answer_value: normalized numeric (0 for false, 1 for true), or categorical value with NO units or symbols;\\n\"\n",
    "        \"  use 'is_blank' if the question is unanswerable.\\n\"\n",
    "        \"- answer_unit: unit string (e.g., kWh, gCO2, %, is_blank).\\n\"\n",
    "        \"- ref_id: list of document IDs that support the answer, e.g., ['ID1', 'ID2'].\\n\"\n",
    "        \"- is_blank: true if unanswerable, false otherwise.\\n\"\n",
    "        \"- supporting_materials: short quote or table/figure pointer from the context.\\n\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Use the context below to answer the question. \"\n",
    "        \"Return ONLY a JSON object, no extra commentary.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\"\n",
    "    )\n",
    "\n",
    "    raw_answer = call_bedrock_claude(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        model_id=model_id,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    parsed = {\n",
    "        \"answer\": \"\",\n",
    "        \"answer_value\": \"is_blank\",\n",
    "        \"answer_unit\": \"is_blank\",\n",
    "        \"ref_id\": [],\n",
    "        \"is_blank\": True,\n",
    "        \"supporting_materials\": \"is_blank\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        first_brace = raw_answer.find(\"{\")\n",
    "        last_brace = raw_answer.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            json_str = raw_answer[first_brace : last_brace + 1]\n",
    "        else:\n",
    "            json_str = raw_answer\n",
    "\n",
    "        candidate = json.loads(json_str)\n",
    "\n",
    "        parsed[\"answer\"] = candidate.get(\"answer\", \"\").strip()\n",
    "        parsed[\"answer_value\"] = normalize_answer_value(candidate.get(\"answer_value\", \"is_blank\"))\n",
    "        parsed[\"answer_unit\"] = str(candidate.get(\"answer_unit\", \"is_blank\")).strip() or \"is_blank\"\n",
    "\n",
    "        ref_id = candidate.get(\"ref_id\", [])\n",
    "        if isinstance(ref_id, str):\n",
    "            ref_ids = [ref_id]\n",
    "        elif isinstance(ref_id, list):\n",
    "            ref_ids = [str(x).strip() for x in ref_id if x]\n",
    "        else:\n",
    "            ref_ids = []\n",
    "        parsed[\"ref_id\"] = ref_ids\n",
    "\n",
    "        is_blank_flag = candidate.get(\"is_blank\", False)\n",
    "        parsed[\"is_blank\"] = bool(is_blank_flag)\n",
    "\n",
    "        supp = candidate.get(\"supporting_materials\", \"is_blank\")\n",
    "        parsed[\"supporting_materials\"] = str(supp).strip() or \"is_blank\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parse error for question {qid}; defaulting to is_blank. Error: {e}\")\n",
    "\n",
    "    return (\n",
    "        parsed[\"answer\"],\n",
    "        parsed[\"answer_value\"],\n",
    "        parsed[\"is_blank\"],\n",
    "        parsed[\"ref_id\"],\n",
    "        parsed[\"supporting_materials\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d940fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_qa_bedrock(\n",
    "    row,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs,\n",
    "    docid_to_url: dict,\n",
    "    top_k: int = 8,\n",
    "    retrieval_threshold: float = 0.25,\n",
    "    model_id: str = \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline for a single question using Bedrock for both retrieval-time\n",
    "    embeddings and generation.\n",
    "    \"\"\"\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "\n",
    "    # 1. Retrieve supporting chunks using Bedrock embeddings for the query\n",
    "    retrieved, q_emb = retrieve_context_for_question_bedrock(\n",
    "        question=question,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n",
    "\n",
    "    # 2. Call Bedrock Claude to produce answer JSON\n",
    "    (\n",
    "        answer,\n",
    "        answer_value,\n",
    "        is_blank_llm,\n",
    "        ref_ids,\n",
    "        supporting_materials,\n",
    "    ) = bedrock_answer_phase_for_question(\n",
    "        qid=qid,\n",
    "        question=question,\n",
    "        retrieved_chunks=retrieved,\n",
    "        model_id=model_id,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. DECISION: retrieval_threshold OR model blank?\n",
    "    # --------------------------------------------------------\n",
    "    # NOTE: we only tell the user when it *actually* gets blanked.\n",
    "    if is_blank_llm:\n",
    "        print(f\"[diag][{qid}] → Model returned is_blank (LLM could not answer).\")\n",
    "    elif top_score < retrieval_threshold:\n",
    "        print(\n",
    "            f\"[diag][{qid}] → Retrieval blocked: top cosine={top_score:.3f} \"\n",
    "            f\"< threshold={retrieval_threshold:.3f}\"\n",
    "        )\n",
    "    is_blank = bool(is_blank_llm) or (top_score < retrieval_threshold)\n",
    "\n",
    "    if is_blank:\n",
    "        answer = \"Unable to answer with confidence based on the provided documents.\"\n",
    "        answer_value = \"is_blank\"\n",
    "        answer_unit = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "        supporting_materials = \"is_blank\"\n",
    "        explanation = \"\"\n",
    "    else:\n",
    "        answer_value = normalize_answer_value(answer_value)\n",
    "        answer_unit = \"is_blank\"\n",
    "\n",
    "        if isinstance(ref_ids, list) and ref_ids:\n",
    "            ref_id_str = \";\".join(ref_ids)\n",
    "            urls = []\n",
    "            for rid in ref_ids:\n",
    "                url = docid_to_url.get(str(rid), \"\")\n",
    "                if url:\n",
    "                    urls.append(url)\n",
    "            ref_url_str = \";\".join(urls) if urls else \"is_blank\"\n",
    "        else:\n",
    "            ref_id_str = \"is_blank\"\n",
    "            ref_url_str = \"is_blank\"\n",
    "\n",
    "        explanation = bedrock_explanation_phase_for_question(\n",
    "            qid=qid,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            supporting_materials=supporting_materials,\n",
    "            model_id=model_id,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_value\": answer_value,\n",
    "        \"answer_unit\": answer_unit,\n",
    "        \"ref_id\": ref_id_str,\n",
    "        \"ref_url\": ref_url_str,\n",
    "        \"supporting_materials\": supporting_materials,\n",
    "        \"explanation\": explanation,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513d271",
   "metadata": {},
   "source": [
    "## Run the WattBot evaluation with Bedrock\n",
    "\n",
    "Now we can loop over all questions in `train_QA.csv`, run retrieval + Bedrock\n",
    "generation, and write a `wattbot_solutions_bedrock.csv` file.\n",
    "\n",
    "This mirrors the logic from Episode 02 – the only difference is that the answer\n",
    "and explanation phases call a hosted Claude 3 model instead of a local Qwen model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "28e3a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################################\n",
      "QUESTION: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "ANSWER: The benchmark suite is called the ML.ENERGY Benchmark.\n",
      "ref_ids: chung2025\n",
      "EXPLANATION: The supporting materials explicitly name the benchmark as the ML.ENERGY Benchmark and describe it as the first inference energy benchmark for generative AI models, directly confirming the answer.\n",
      "################################################################################################\n",
      "QUESTION: What were the net CO2e emissions from training the GShard-600B model?\n",
      "ANSWER: The net CO2e emissions from training the GShard-600B model were 4.3 metric tons.\n",
      "ref_ids: patterson2021\n",
      "EXPLANATION: The supporting materials state that training GShard-600B used 24 MWh of energy and resulted in 4.3 metric tons of net CO2e emissions, directly matching the answer provided.\n",
      "################################################################################################\n",
      "QUESTION: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "[diag][q054] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?\n",
      "[diag][q062] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "ANSWER: True: Hyperscale data centers achieved more than 40% higher efficiency compared to traditional data centers in 2020, as measured by Power Usage Effectiveness (PUE).\n",
      "ref_ids: wu2021b;wu2021a\n",
      "EXPLANATION: The supporting materials directly state that hyperscale data centers achieved more than 40% higher efficiency than traditional data centers, as measured by PUE. This evidence confirms the answer is true.\n",
      "################################################################################################\n",
      "QUESTION: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "ANSWER: GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10-50 medium-length responses, depending on when and where it is deployed.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials directly state that GPT-3 consumes a 500ml bottle of water for roughly 10–50 medium-length responses, which matches the answer exactly. This range accounts for variations based on deployment time and location.\n",
      "################################################################################################\n",
      "QUESTION: From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?\n",
      "ANSWER: The difference is 55 percentage points (75% target accuracy, 20% target efficiency).\n",
      "ref_ids: schwartz2019\n",
      "EXPLANATION: The supporting materials directly state that 75% of CVPR papers target accuracy and 20% target efficiency, so the difference is calculated as 75% minus 20%, which equals 55 percentage points.\n",
      "################################################################################################\n",
      "QUESTION: True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.\n",
      "ANSWER: False. The AI Act does not make energy consumption data publicly available to NGOs, analysts, and the general public; it restricts access to authorities and downstream providers due to confidentiality clauses.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials specify that the AI Act restricts energy consumption data to authorities and downstream providers under confidentiality clauses, not making it available to NGOs, analysts, or the public. This directly justifies the answer by highlighting the legal limitations on disclosure.\n",
      "################################################################################################\n",
      "QUESTION: What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?\n",
      "ANSWER: The projected maximum batch size for fine-tuning a Mixtral model with a projected GPU capacity of 100GB is 28 samples.\n",
      "ref_ids: xia2024\n",
      "EXPLANATION: The supporting materials directly state that for a 100GB GPU capacity, the maximum batch size for fine-tuning Mixtral is projected to be 28 samples. This matches the given GPU capacity and answer, providing clear evidence for the stated batch size.\n",
      "################################################################################################\n",
      "QUESTION: What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?\n",
      "ANSWER: The approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs is 2 times.\n",
      "ref_ids: samsi2024\n",
      "EXPLANATION: The supporting materials directly state that the LLaMA-7B model experienced a 2 times increase in inference throughput on A100 GPUs compared to V100 GPUs, measured across multiple metrics including words, tokens, and responses per second. This confirms the stated speedup of 2 times for the 7B model size.\n",
      "################################################################################################\n",
      "QUESTION: What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?\n",
      "ANSWER: The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 5.4 million liters, including 0.7 million liters of on-site water consumption and 4.7 million liters of off-site water consumption.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials explicitly state that training GPT-3 in Microsoft’s U.S. data centers consumes a total of 5.4 million liters of water, which directly matches the answer. This total includes 700,000 liters (0.7 million liters) of on-site water consumption, with the remainder being off-site.\n",
      "################################################################################################\n",
      "QUESTION: True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.\n",
      "ANSWER: True. The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems, because the carbon footprint of AI models is often unrelated to their risk classification.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials directly state that sustainability impact assessments (SIAs) should apply to all AI systems, not just high-risk ones, because the carbon footprint of AI models is often unrelated to their risk classification. This aligns exactly with the answer, confirming the authors' proposal.\n",
      "################################################################################################\n",
      "QUESTION: As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?\n",
      "ANSWER: The water use effectiveness (WUE) for AWS data centers in 2023 was 0.18 L/kWh.\n",
      "ref_ids: amazon2023\n",
      "EXPLANATION: The supporting materials directly state that AWS data centers achieved a water use effectiveness (WUE) of 0.18 L/kWh in 2023, which matches the answer provided. This figure is also contextualized with improvements from previous years, reinforcing its accuracy for the specified year.\n",
      "################################################################################################\n",
      "QUESTION: True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\n",
      "ANSWER: True. Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models, as it minimizes data transmission between clients and remote servers.\n",
      "ref_ids: khan2025\n",
      "EXPLANATION: The supporting materials directly state that minimizing data transmission through local inference reduces both network overhead and carbon footprint. This aligns exactly with the reasoning in the answer, confirming its validity.\n",
      "################################################################################################\n",
      "QUESTION: True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\n",
      "ANSWER: True. Tracking the runtime of a training job is important for estimating compute cost, as it is a key factor in calculating chip-hours and energy consumption, which are used to determine both hardware amortization and energy costs.\n",
      "ref_ids: cottier2024;strubell2019\n",
      "EXPLANATION: The supporting materials justify the answer by directly linking runtime (measured in chip-hours) to compute cost. The citation from cottier2024 defines total cost as a function of training chip-hours, while strubell2019 shows that cloud compute and electricity costs are estimated based on such metrics. This confirms that runtime is a key input for cost calculation.\n",
      "################################################################################################\n",
      "QUESTION: For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?\n",
      "ANSWER: The maximum performance improvement (latency reduction) achieved by enabling automated resource utilization overlapping for the LLaMA-65B model was 13.2%.\n",
      "ref_ids: chen2024\n",
      "EXPLANATION: The supporting materials directly state that the LLaMA-65B model achieved up to a 13.2% performance improvement, as shown in Figure 14, through automated resource utilization overlapping. This figure and description provide clear evidence of the latency reduction claimed in the answer.\n",
      "################################################################################################\n",
      "QUESTION: How much does an elephant weigh?\n",
      "[diag][q164] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?\n",
      "ANSWER: GPT-3 has the highest energy consumption at 1287 MWh.\n",
      "ref_ids: patterson2021\n",
      "EXPLANATION: The supporting materials directly state that GPT-3's energy consumption is 1287 MWh, which is the highest value provided among the listed models, confirming it as the most energy-intensive.\n",
      "################################################################################################\n",
      "QUESTION: How many days of CO₂ emissions from an average American life are equivalent to training BERT base?\n",
      "[diag][q170] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\n",
      "[diag][q200] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "[diag][q202] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.\n",
      "ANSWER: True, for CV tasks, eight T4 spot instances can be more cost-efficient than a DGX-2 node, being 58% cheaper while 37% slower, but for NLP tasks, the DGX-2 is more cost-efficient.\n",
      "ref_ids: erben2023\n",
      "EXPLANATION: The supporting materials directly state that for CV tasks, eight T4 spot instances are 58% cheaper than a DGX-2 node, confirming greater cost-efficiency despite being 37% slower. This justifies the answer's claim for CV, while the absence of similar T4 advantages for NLP in the provided text implies DGX-2 remains more cost-efficient there.\n",
      "################################################################################################\n",
      "QUESTION: True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.\n",
      "ANSWER: False. The 2023 US Executive Order regarding AI did not mention AI's greenhouse gas emissions nor energy usage.\n",
      "ref_ids: luccioni2025b\n",
      "EXPLANATION: The supporting materials directly reference the Executive Order, confirming that it did not include any mention of AI's greenhouse gas emissions or energy usage, which aligns with the answer provided.\n",
      "################################################################################################\n",
      "QUESTION: True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.\n",
      "ANSWER: True. Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting material directly quotes the Act's requirement for data centers to operate on 100% renewable energy by January 1, 2027, confirming the answer as true.\n",
      "################################################################################################\n",
      "QUESTION: Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?\n",
      "ANSWER: 6 papers\n",
      "ref_ids: schwartz2019\n",
      "EXPLANATION: The supporting materials indicate that 10% of ACL papers in the sample targeted both accuracy and efficiency. Given that ACL papers are part of the 60-paper sample, this percentage directly supports the answer of 6 papers.\n",
      "################################################################################################\n",
      "QUESTION: According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?\n",
      "ANSWER: According to recent estimates, inference can account for up to 90% of a model's total lifecycle energy use.\n",
      "ref_ids: jegham2025\n",
      "EXPLANATION: The supporting materials directly state that recent estimates attribute up to 90% of a model's total lifecycle energy use to inference, as cited in references [14, 15]. This aligns exactly with the answer provided.\n",
      "################################################################################################\n",
      "QUESTION: True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.\n",
      "ANSWER: False. The AI Act requires providers to report energy consumption for the development phase (including training) of general-purpose AI models, but it does not explicitly require reporting for the inference phase, creating a significant gap in coverage.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials specify that Annex XI of the AI Act mandates reporting of energy consumption only for the development phase, which includes training. This confirms that inference energy consumption is not explicitly required, justifying the answer \"False.\"\n",
      "################################################################################################\n",
      "QUESTION: True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.\n",
      "ANSWER: False. The AI Act currently requires energy consumption reporting only for the development phase of AI models, not the inference phase, leaving a significant gap in environmental accountability.\n",
      "ref_ids: ebert2024\n",
      "EXPLANATION: The supporting materials explicitly state that the AI Act's energy reporting requirement only applies to the development phase and excludes the inference phase, directly justifying the answer that such reporting is not currently required for inference.\n",
      "################################################################################################\n",
      "QUESTION: True or False: New AI data centers often rely on air cooling due to high server power densities.\n",
      "ANSWER: False. New AI data centers often rely on liquid cooling due to high server power densities.\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials directly state that new AI data centers often rely on liquid cooling, not air cooling, because of high server power densities. This confirms the answer by providing a clear, factual basis for the reasoning.\n",
      "################################################################################################\n",
      "QUESTION: By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?\n",
      "ANSWER: Platform-level caching improved power efficiency by 6.7 times for the cross-lingual Transformer language model.\n",
      "ref_ids: wu2021a\n",
      "EXPLANATION: The supporting materials directly state that platform-level caching improves power efficiency by 6.7× compared to a CPU server baseline, which aligns with the answer's claim of a 6.7 times improvement. This factor is explicitly provided in the cited text, confirming the magnitude of efficiency gain.\n",
      "################################################################################################\n",
      "QUESTION: What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?\n",
      "ANSWER: The estimated CO2 emissions from training a BERT base model for 79 hours using 64 V100 GPUs is 1,438 pounds of CO2e.\n",
      "ref_ids: strubell2019\n",
      "EXPLANATION: The supporting materials show that training BERTbase with 64 V100 GPUs for 79 hours results in 1,438 pounds of CO2e, directly matching the answer. The value 1,438 is explicitly listed in the data, confirming the emission estimate.\n",
      "################################################################################################\n",
      "QUESTION: According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?\n",
      "ANSWER: ML inference reportedly accounts for 80–90% of total compute demand.\n",
      "ref_ids: chung2025;luccioni2024;patterson2021\n",
      "EXPLANATION: The supporting materials directly cite multiple sources (references [12, 32, 58, 60]) that report the 80–90% figure, providing documented evidence for the claim about ML inference's share of total compute demand.\n",
      "################################################################################################\n",
      "QUESTION: How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?\n",
      "ANSWER: Training a 6.1B-parameter language model to completion is estimated to consume 103.5 MWh, equivalent to approximately 9.4 U.S. household-years of electricity consumption.\n",
      "ref_ids: dodge2022;jegham2025\n",
      "EXPLANATION: The supporting materials provide the training energy consumption (103.5 MWh from dodge2022) and a standard U.S. household annual electricity usage (~10.9–11 MWh/year, based on EIA 2021 and jegham2025 context). Dividing the training energy by this household usage yields approximately 9.4 household-years, aligning with the answer.\n",
      "################################################################################################\n",
      "QUESTION: True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.\n",
      "ANSWER: True. For NLP experiments in geo-distributed settings, egress costs can account for more than 90% of the total cost per VM, as seen with Google Cloud where external egress cost is $4.329/h, which is over 90% of the total cost per VM of $4.804/h.\n",
      "ref_ids: erben2023\n",
      "EXPLANATION: The supporting materials directly show that egress costs ($4.329/h) exceed 90% of the total cost per VM ($4.804/h), confirming that such costs can indeed dominate the budget in geo-distributed NLP experiments.\n",
      "################################################################################################\n",
      "QUESTION: Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.\n",
      "[diag][q280] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?\n",
      "ANSWER: Water consumption\n",
      "ref_ids: li2025b\n",
      "EXPLANATION: The supporting materials directly define \"water consumption\" as \"water withdrawal minus water discharge\" and specify that it includes water evaporated, transpired, or incorporated into products, which matches the description in the question.\n",
      "################################################################################################\n",
      "QUESTION: What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?\n",
      "ANSWER: The observed range of inference energy per second for LLaMA-65B across GPU shard configurations is 300 Watts to 1 Kilowatt, from 8 to 32 shards.\n",
      "ref_ids: samsi2024\n",
      "EXPLANATION: The supporting materials directly state that the energy per second for LLaMA-65B inference ranges from 300 Watts to 1 Kilowatt, specifically when scaling from 8 to 32 GPU shards. This matches the answer's quantitative range and configuration details, providing clear justification.\n",
      "################################################################################################\n",
      "QUESTION: When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?\n",
      "ANSWER: The 72B version of Qwen models consumed 8 times more energy than the 7B version in zero-shot classification.\n",
      "ref_ids: zschache2025\n",
      "EXPLANATION: The supporting materials state that the 7B version consumes only one-eighth of the energy of the 72B version. This implies that the 72B model uses 8 times more energy than the 7B model, directly justifying the answer.\n",
      "################################################################################################\n",
      "QUESTION: By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?\n",
      "[diag][q304] → Model returned is_blank (LLM could not answer).\n",
      "ANSWER: Unable to answer with confidence based on the provided documents.\n",
      "ref_ids: is_blank\n",
      "EXPLANATION: \n",
      "################################################################################################\n",
      "QUESTION: How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?\n",
      "ANSWER: The latest iteration of the ML.ENERGY Benchmark included energy measurements of 40 widely used model architectures across 6 different tasks.\n",
      "ref_ids: chung2025\n",
      "EXPLANATION: The supporting materials directly state that the benchmark includes \"energy measurements of 40 widely used model architectures across 6 different tasks,\" which matches the answer precisely. This confirms the number of model architectures and the scope of tasks evaluated.\n",
      "################################################################################################\n",
      "QUESTION: In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?\n",
      "ANSWER: The estimated health cost for training a Llama-3.1 scale model in Iowa is $2.5 million USD.\n",
      "ref_ids: han2024\n",
      "EXPLANATION: The supporting materials directly state that training in Iowa results in a health cost of $2.5 million USD, which is significantly higher than the $0.23 million cost in Oregon. This specific figure confirms the answer for Altoona, Iowa, as Altoona is located in Iowa.\n",
      "Wrote predictions to outputs/wattbot_solutions_bedrock.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The benchmark suite is called the ML.ENERGY Be...</td>\n",
       "      <td>ML.ENERGY</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>chung2025</td>\n",
       "      <td>https://arxiv.org/pdf/2505.06371</td>\n",
       "      <td>The ML.ENERGY Benchmark is the first inference...</td>\n",
       "      <td>The supporting materials explicitly name the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>The net CO2e emissions from training the GShar...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>patterson2021</td>\n",
       "      <td>https://arxiv.org/pdf/2104.10350</td>\n",
       "      <td>Training GShard-600B used 24 MWh and produced ...</td>\n",
       "      <td>The supporting materials state that training G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>True: Hyperscale data centers achieved more th...</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>wu2021b;wu2021a</td>\n",
       "      <td>https://arxiv.org/pdf/2108.06738;https://arxiv...</td>\n",
       "      <td>between traditional and highly optimized hyper...</td>\n",
       "      <td>The supporting materials directly state that h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  q003  What is the name of the benchmark suite presen...   \n",
       "1  q009  What were the net CO2e emissions from training...   \n",
       "2  q054  What is the model size in gigabytes (GB) for t...   \n",
       "3  q062  What was the total electricity consumption of ...   \n",
       "4  q075  True or False: Hyperscale data centers in 2020...   \n",
       "\n",
       "                                              answer answer_value answer_unit  \\\n",
       "0  The benchmark suite is called the ML.ENERGY Be...    ML.ENERGY    is_blank   \n",
       "1  The net CO2e emissions from training the GShar...          4.3    is_blank   \n",
       "2  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "3  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "4  True: Hyperscale data centers achieved more th...            1    is_blank   \n",
       "\n",
       "            ref_id                                            ref_url  \\\n",
       "0        chung2025                   https://arxiv.org/pdf/2505.06371   \n",
       "1    patterson2021                   https://arxiv.org/pdf/2104.10350   \n",
       "2         is_blank                                           is_blank   \n",
       "3         is_blank                                           is_blank   \n",
       "4  wu2021b;wu2021a  https://arxiv.org/pdf/2108.06738;https://arxiv...   \n",
       "\n",
       "                                supporting_materials  \\\n",
       "0  The ML.ENERGY Benchmark is the first inference...   \n",
       "1  Training GShard-600B used 24 MWh and produced ...   \n",
       "2                                           is_blank   \n",
       "3                                           is_blank   \n",
       "4  between traditional and highly optimized hyper...   \n",
       "\n",
       "                                         explanation  \n",
       "0  The supporting materials explicitly name the b...  \n",
       "1  The supporting materials state that training G...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4  The supporting materials directly state that h...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# For quick smoke tests, you can slice train_df (e.g., train_df.head(5))\n",
    "for _, row in train_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    print(\"#\" * 96)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "\n",
    "    out = run_single_qa_bedrock(\n",
    "        row=row,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        docid_to_url=docid_to_url,\n",
    "        top_k=20,\n",
    "        retrieval_threshold=0.1,\n",
    "        model_id=bedrock_model_id,\n",
    "    )\n",
    "\n",
    "    answer = out[\"answer\"]\n",
    "    ref_ids = out[\"ref_id\"]\n",
    "    explanation = out[\"explanation\"]\n",
    "\n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    print(f\"ref_ids: {ref_ids}\")\n",
    "    print(f\"EXPLANATION: {explanation}\")\n",
    "\n",
    "    results.append(out)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"wattbot_solutions_bedrock.csv\")\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Wrote predictions to {output_path}\")\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "172697da-013c-40dd-9b9b-3b06fba000d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"True\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"False\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",   # can also pass \"is_blank\" or None\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,    # can pass \"is_blank\", or leave None to auto\n",
    "    n_examples=10,          # how many incorrect examples to print\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "\n",
    "    NA logic:\n",
    "    - If an explicit NA column is found/used (e.g. is_NA), we use it via _to_bool_flag.\n",
    "    - If you pass gt_is_na_col=\"is_blank\" or pred_is_na_col=\"is_blank\",\n",
    "      we *derive* NA from answer_value == \"is_blank\" instead of expecting a real column.\n",
    "    - If no NA column is available at all, we derive from answer_value == \"is_blank\".\n",
    "\n",
    "    Also prints up to `n_examples` rows where the model is not perfect\n",
    "    (answer_score < 1, ref_id_score < 1, or is_NA_score < 1).\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "\n",
    "    # ----- ground truth NA flags -----\n",
    "    if gt_is_na_col is not None and gt_is_na_col in merged.columns:\n",
    "        # Use explicit column (e.g. \"is_NA\")\n",
    "        gt_is_na_series = merged[gt_is_na_col].map(_to_bool_flag)\n",
    "    elif gt_is_na_col is not None and gt_is_na_col.lower() == \"is_blank\":\n",
    "        # Special meaning: derive NA from answer_value_gt == \"is_blank\"\n",
    "        gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "    else:\n",
    "        # Fallback: if we have is_NA or is_blank col, use it; else derive\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "\n",
    "    # ----- prediction NA flags -----\n",
    "    if pred_is_na_col is not None and pred_is_na_col in merged.columns:\n",
    "        pred_is_na_series = merged[pred_is_na_col].map(_to_bool_flag)\n",
    "    elif pred_is_na_col is not None and pred_is_na_col.lower() == \"is_blank\":\n",
    "        # Same convention: derive from answer_value_pred\n",
    "        pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "    else:\n",
    "        # Auto-detect or derive if no NA column in preds\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "\n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = bool(gt_is_na_series.iloc[idx])\n",
    "        pred_is_na = bool(pred_is_na_series.iloc[idx])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    # ----- Show some incorrect examples -----\n",
    "    incorrect = merged[\n",
    "        (merged[\"answer_score\"] < 1.0)\n",
    "        | (merged[\"ref_id_score\"] < 1.0)\n",
    "        | (merged[\"is_NA_score\"] < 1.0)\n",
    "    ]\n",
    "    \n",
    "    if not incorrect.empty and n_examples > 0:\n",
    "        print(\"\\nExamples of incorrect / partially correct responses \"\n",
    "              f\"(up to {n_examples} rows):\\n\")\n",
    "        # Grab up to n_examples \"worst\" rows by wattbot_score\n",
    "        for _, row in incorrect.sort_values(\"wattbot_score\").head(n_examples).iterrows():\n",
    "            q = row[\"question_gt\"] if \"question_gt\" in row.index else None\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"id: {row[id_col]}\")\n",
    "            if q is not None:\n",
    "                print(f\"Question: {q}\")\n",
    "            print(f\"GT answer_value:   {row[f'{gt_answer_col}_gt']}\")\n",
    "            print(f\"Pred answer_value: {row[f'{pred_answer_col}_pred']}\")\n",
    "            print(f\"GT ref_id:         {row[f'{gt_ref_col}_gt']}\")\n",
    "            print(f\"Pred ref_id:       {row[f'{pred_ref_col}_pred']}\")\n",
    "            print(f\"answer_score: {row['answer_score']:.3f}, \"\n",
    "                  f\"ref_id_score: {row['ref_id_score']:.3f}, \"\n",
    "                  f\"is_NA_score: {row['is_NA_score']:.3f}, \"\n",
    "                  f\"wattbot_score: {row['wattbot_score']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7aa64ca8-f758-4510-8082-fedd08454f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Normalize reference IDs + answer ranges after results are created\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from typing import Any\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def normalize_ref_ids(refs: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize reference IDs to a Python-list-style string.\n",
    "\n",
    "    Output format examples:\n",
    "      Input                       → Output\n",
    "      ---------------------------------------------------------\n",
    "      \"chen2024\"                 → \"['chen2024']\"\n",
    "      ['chen2024']               → \"['chen2024']\"\n",
    "      \"[chen2024]\"               → \"['chen2024']\"\n",
    "      \"['chen2024']\"             → \"['chen2024']\"\n",
    "\n",
    "      \"chen2024;smith2023\"       → \"['chen2024', 'smith2023']\"\n",
    "      \"chen2024, smith2023\"      → \"['chen2024', 'smith2023']\"\n",
    "      \"[wu2021b;wu2021a]\"        → \"['wu2021b', 'wu2021a']\"\n",
    "      ['wu2021b','wu2021a']      → \"['wu2021b', 'wu2021a']\"\n",
    "\n",
    "      None                       → \"is_blank\"\n",
    "      \"is_blank\"                 → \"is_blank\"\n",
    "\n",
    "    Rules:\n",
    "      - \"is_blank\" stays exactly \"is_blank\".\n",
    "      - Semicolons are treated as separators (→ commas).\n",
    "      - Strips stray brackets, quotes, spaces.\n",
    "      - Produces Python-list-style: ['id'] or ['id1', 'id2'].\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # ----- 1. Handle blanks -----\n",
    "    if refs is None or str(refs).strip() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # ----- 2. True iterable input -----\n",
    "    if isinstance(refs, (list, tuple, np.ndarray)):\n",
    "        cleaned = [str(x).strip().strip(\"[]'\\\" \") for x in refs if str(x).strip()]\n",
    "        return \"[\" + \", \".join(f\"'{c}'\" for c in cleaned) + \"]\"\n",
    "\n",
    "    # ----- 3. Treat as string -----\n",
    "    s = str(refs).strip()\n",
    "\n",
    "    # Strip outer brackets if present (e.g., \"[chen2024]\" or \"['chen2024']\")\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1].strip()\n",
    "\n",
    "    # Replace semicolons with commas\n",
    "    s = s.replace(\";\", \",\")\n",
    "\n",
    "    # Split, strip quotes/spaces\n",
    "    parts = [p.strip().strip(\"'\\\"\") for p in s.split(\",\") if p.strip()]\n",
    "\n",
    "    if len(parts) == 0:\n",
    "        return \"is_blank\"\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        return f\"['{parts[0]}']\"\n",
    "\n",
    "    return \"[\" + \", \".join(f\"'{p}'\" for p in parts) + \"]\"\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer_value(val: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize answer_value so that:\n",
    "      - single numbers stay as-is (300 -> \"300\")\n",
    "      - ranges get bracketed (\"300-1000\" -> \"[300,1000]\")\n",
    "      - lists/tuples become bracketed ranges\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    # list / tuple / array → always a range\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        vals = []\n",
    "        for v in val:\n",
    "            # convert ints cleanly\n",
    "            if isinstance(v, (int, float)) and float(v).is_integer():\n",
    "                vals.append(str(int(v)))\n",
    "            else:\n",
    "                vals.append(str(v))\n",
    "        return \"[\" + \",\".join(vals) + \"]\"\n",
    "\n",
    "    # numeric scalar → leave alone\n",
    "    if isinstance(val, (int, float)):\n",
    "        if float(val).is_integer():\n",
    "            return str(int(val))\n",
    "        return str(val)\n",
    "\n",
    "    # string cases\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "\n",
    "        # already bracketed\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            return s\n",
    "\n",
    "        # detect range: 300-1000 or 300 – 1000\n",
    "        m = re.match(r\"^\\s*([0-9]+(?:\\.[0-9]+)?)\\s*[-–—]\\s*([0-9]+(?:\\.[0-9]+)?)\\s*$\", s)\n",
    "        if m:\n",
    "            a, b = m.groups()\n",
    "            # strip trailing .0\n",
    "            a = a.rstrip(\".0\")\n",
    "            b = b.rstrip(\".0\")\n",
    "            return f\"[{a},{b}]\"\n",
    "\n",
    "        # otherwise single value → leave alone\n",
    "        return s\n",
    "\n",
    "    # fallback: return string without brackets\n",
    "    return str(val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bbe8b1ab-1b9b-4b43-b3c0-e3341409a4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The benchmark suite is called the ML.ENERGY Be...</td>\n",
       "      <td>ML.ENERGY</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>chung2025</td>\n",
       "      <td>https://arxiv.org/pdf/2505.06371</td>\n",
       "      <td>The ML.ENERGY Benchmark is the first inference...</td>\n",
       "      <td>The supporting materials explicitly name the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>The net CO2e emissions from training the GShar...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>patterson2021</td>\n",
       "      <td>https://arxiv.org/pdf/2104.10350</td>\n",
       "      <td>Training GShard-600B used 24 MWh and produced ...</td>\n",
       "      <td>The supporting materials state that training G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>True: Hyperscale data centers achieved more th...</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>wu2021b;wu2021a</td>\n",
       "      <td>https://arxiv.org/pdf/2108.06738;https://arxiv...</td>\n",
       "      <td>between traditional and highly optimized hyper...</td>\n",
       "      <td>The supporting materials directly state that h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  q003  What is the name of the benchmark suite presen...   \n",
       "1  q009  What were the net CO2e emissions from training...   \n",
       "2  q054  What is the model size in gigabytes (GB) for t...   \n",
       "3  q062  What was the total electricity consumption of ...   \n",
       "4  q075  True or False: Hyperscale data centers in 2020...   \n",
       "\n",
       "                                              answer answer_value answer_unit  \\\n",
       "0  The benchmark suite is called the ML.ENERGY Be...    ML.ENERGY    is_blank   \n",
       "1  The net CO2e emissions from training the GShar...          4.3    is_blank   \n",
       "2  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "3  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "4  True: Hyperscale data centers achieved more th...            1    is_blank   \n",
       "\n",
       "            ref_id                                            ref_url  \\\n",
       "0        chung2025                   https://arxiv.org/pdf/2505.06371   \n",
       "1    patterson2021                   https://arxiv.org/pdf/2104.10350   \n",
       "2         is_blank                                           is_blank   \n",
       "3         is_blank                                           is_blank   \n",
       "4  wu2021b;wu2021a  https://arxiv.org/pdf/2108.06738;https://arxiv...   \n",
       "\n",
       "                                supporting_materials  \\\n",
       "0  The ML.ENERGY Benchmark is the first inference...   \n",
       "1  Training GShard-600B used 24 MWh and produced ...   \n",
       "2                                           is_blank   \n",
       "3                                           is_blank   \n",
       "4  between traditional and highly optimized hyper...   \n",
       "\n",
       "                                         explanation  \n",
       "0  The supporting materials explicitly name the b...  \n",
       "1  The supporting materials state that training G...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  The supporting materials directly state that h...  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "solutions_df = pd.read_csv(output_dir + \"/wattbot_solutions_bedrock.csv\")\n",
    "solutions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dc4e4c6b-3a6c-4467-b5ee-fc017a1b476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The benchmark suite is called the ML.ENERGY Be...</td>\n",
       "      <td>ML.ENERGY</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>https://arxiv.org/pdf/2505.06371</td>\n",
       "      <td>The ML.ENERGY Benchmark is the first inference...</td>\n",
       "      <td>The supporting materials explicitly name the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>The net CO2e emissions from training the GShar...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>https://arxiv.org/pdf/2104.10350</td>\n",
       "      <td>Training GShard-600B used 24 MWh and produced ...</td>\n",
       "      <td>The supporting materials state that training G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>True: Hyperscale data centers achieved more th...</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['wu2021b', 'wu2021a']</td>\n",
       "      <td>https://arxiv.org/pdf/2108.06738;https://arxiv...</td>\n",
       "      <td>between traditional and highly optimized hyper...</td>\n",
       "      <td>The supporting materials directly state that h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  q003  What is the name of the benchmark suite presen...   \n",
       "1  q009  What were the net CO2e emissions from training...   \n",
       "2  q054  What is the model size in gigabytes (GB) for t...   \n",
       "3  q062  What was the total electricity consumption of ...   \n",
       "4  q075  True or False: Hyperscale data centers in 2020...   \n",
       "\n",
       "                                              answer answer_value answer_unit  \\\n",
       "0  The benchmark suite is called the ML.ENERGY Be...    ML.ENERGY    is_blank   \n",
       "1  The net CO2e emissions from training the GShar...          4.3    is_blank   \n",
       "2  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "3  Unable to answer with confidence based on the ...     is_blank    is_blank   \n",
       "4  True: Hyperscale data centers achieved more th...            1    is_blank   \n",
       "\n",
       "                   ref_id                                            ref_url  \\\n",
       "0           ['chung2025']                   https://arxiv.org/pdf/2505.06371   \n",
       "1       ['patterson2021']                   https://arxiv.org/pdf/2104.10350   \n",
       "2                is_blank                                           is_blank   \n",
       "3                is_blank                                           is_blank   \n",
       "4  ['wu2021b', 'wu2021a']  https://arxiv.org/pdf/2108.06738;https://arxiv...   \n",
       "\n",
       "                                supporting_materials  \\\n",
       "0  The ML.ENERGY Benchmark is the first inference...   \n",
       "1  Training GShard-600B used 24 MWh and produced ...   \n",
       "2                                           is_blank   \n",
       "3                                           is_blank   \n",
       "4  between traditional and highly optimized hyper...   \n",
       "\n",
       "                                         explanation  \n",
       "0  The supporting materials explicitly name the b...  \n",
       "1  The supporting materials state that training G...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  The supporting materials directly state that h...  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions_df[\"ref_id\"] = solutions_df[\"ref_id\"].apply(normalize_ref_ids)\n",
    "solutions_df[\"answer_value\"] = solutions_df[\"answer_value\"].apply(normalize_answer_value)\n",
    "solutions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3c039a87-7e60-4f0b-a1be-b5196eb72e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_df.to_csv(output_dir + \"/solutions_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3c3c7d5d-556c-4974-8f3c-d15ff9912eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows compared: 41\n",
      "Mean answer_value score: 0.6098\n",
      "Mean ref_id score:       0.7317\n",
      "Mean is_NA score:        0.8537\n",
      "Overall WattBot score:   0.6524\n",
      "\n",
      "Examples of incorrect / partially correct responses (up to 20 rows):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "id: q054\n",
      "Question: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "GT answer_value:   64.7\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['chen2024']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q280\n",
      "Question: Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.\n",
      "GT answer_value:   13\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['shen2024']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q170\n",
      "Question: How many days of CO₂ emissions from an average American life are equivalent to training BERT base?\n",
      "GT answer_value:   14.4\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['strubell2019']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q200\n",
      "Question: True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\n",
      "GT answer_value:   0\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['patterson2021']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q202\n",
      "Question: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "GT answer_value:   Financial Sentiment Analysis\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['khan2025']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q304\n",
      "Question: By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?\n",
      "GT answer_value:   55.6\n",
      "Pred answer_value: is_blank\n",
      "GT ref_id:         ['khan2025']\n",
      "Pred ref_id:       is_blank\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 0.000, wattbot_score: 0.000\n",
      "--------------------------------------------------------------------------------\n",
      "id: q272\n",
      "Question: How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?\n",
      "GT answer_value:   1.3\n",
      "Pred answer_value: 9.4\n",
      "GT ref_id:         ['dodge2022','strubell2019']\n",
      "Pred ref_id:       ['dodge2022', 'jegham2025']\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q263\n",
      "Question: According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?\n",
      "GT answer_value:   [80,90]\n",
      "Pred answer_value: [8,9]\n",
      "GT ref_id:         ['chung2025']\n",
      "Pred ref_id:       ['chung2025', 'luccioni2024', 'patterson2021']\n",
      "answer_score: 0.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.100\n",
      "--------------------------------------------------------------------------------\n",
      "id: q003\n",
      "Question: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "GT answer_value:   ML.ENERGY Benchmark\n",
      "Pred answer_value: ML.ENERGY\n",
      "GT ref_id:         ['chung2025']\n",
      "Pred ref_id:       ['chung2025']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q297\n",
      "Question: When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?\n",
      "GT answer_value:   8.720430108\n",
      "Pred answer_value: 8\n",
      "GT ref_id:         ['zschache2025']\n",
      "Pred ref_id:       ['zschache2025']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q296\n",
      "Question: What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?\n",
      "GT answer_value:   [300,1000]\n",
      "Pred answer_value: [3,1]\n",
      "GT ref_id:         ['samsi2024']\n",
      "Pred ref_id:       ['samsi2024']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q282\n",
      "Question: What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?\n",
      "GT answer_value:   Water consumption\n",
      "Pred answer_value: water\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       ['li2025b']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q316\n",
      "Question: In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?\n",
      "GT answer_value:   2510000\n",
      "Pred answer_value: 2500000\n",
      "GT ref_id:         ['han2024']\n",
      "Pred ref_id:       ['han2024']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q124\n",
      "Question: What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?\n",
      "GT answer_value:   5439000\n",
      "Pred answer_value: 5.4\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       ['li2025b']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q078\n",
      "Question: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "GT answer_value:   [0.02,0.1]\n",
      "Pred answer_value: [1,5]\n",
      "GT ref_id:         ['li2025b']\n",
      "Pred ref_id:       ['li2025b']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q215\n",
      "Question: Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?\n",
      "GT answer_value:   2\n",
      "Pred answer_value: 6\n",
      "GT ref_id:         ['schwartz2019']\n",
      "Pred ref_id:       ['schwartz2019']\n",
      "answer_score: 0.000, ref_id_score: 1.000, is_NA_score: 1.000, wattbot_score: 0.250\n",
      "--------------------------------------------------------------------------------\n",
      "id: q153\n",
      "Question: True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\n",
      "GT answer_value:   1\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['strubell2019']\n",
      "Pred ref_id:       ['cottier2024', 'strubell2019']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.850\n",
      "--------------------------------------------------------------------------------\n",
      "id: q075\n",
      "Question: True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "GT answer_value:   1\n",
      "Pred answer_value: 1\n",
      "GT ref_id:         ['wu2021b','patterson2021']\n",
      "Pred ref_id:       ['wu2021b', 'wu2021a']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.850\n",
      "--------------------------------------------------------------------------------\n",
      "id: q207\n",
      "Question: True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.\n",
      "GT answer_value:   0\n",
      "Pred answer_value: 0\n",
      "GT ref_id:         luccioni2025b\n",
      "Pred ref_id:       ['luccioni2025b']\n",
      "answer_score: 1.000, ref_id_score: 0.000, is_NA_score: 1.000, wattbot_score: 0.850\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=output_dir + \"/solutions_normalized.csv\",\n",
    "    gt_is_na_col=\"is_NA\",   # or \"is_blank\" / None depending on how you mark NAs\n",
    "    n_examples=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a52c8b",
   "metadata": {},
   "source": [
    "## Wrap‑up: comparing Bedrock to GPU‑based runs\n",
    "\n",
    "At this point you should have three versions of the WattBot evaluation:\n",
    "\n",
    "1. **Episode 01 – Notebook GPU instance** using a locally loaded open‑source model.  \n",
    "2. **Episode 02 – SageMaker Processing job** running the same model in batch with on-demand compute. \n",
    "3. **Episode 03 – Bedrock** using a hosted Claude 3 model with per‑token billing.\n",
    "\n",
    "When deciding between these options in practice:\n",
    "\n",
    "- Use **Bedrock or other hosted APIs** when:\n",
    "  - You want to try the latest frontier models quickly.  \n",
    "  - You only need to run a modest number of questions, or you are still prototyping.  \n",
    "  - You prefer a simple, token‑based cost model and don’t want to manage GPU capacity.\n",
    "\n",
    "- Use **self‑hosted models on GPU instances** when:\n",
    "  - You expect to run large batches repeatedly (e.g., many thousands of questions).  \n",
    "  - You want tight control over which architectures/checkpoints you run or fine‑tune.  \n",
    "  - You already have institutional access to cost‑effective on‑prem or cloud GPUs.\n",
    "\n",
    "The core **RAG evaluation logic stays identical** across all three episodes, which is the main takeaway:\n",
    "once you have a clean retrieval + normalization pipeline (like WattBot’s), swapping out the generator\n",
    "is mostly a matter of re‑implementing `answer_phase_for_question` and `explanation_phase_for_question`\n",
    "for each compute option you care about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705d94c",
   "metadata": {},
   "source": [
    "## Concluding remarks: Bedrock models are one piece of the RAG puzzle\n",
    "\n",
    "In this episode we swapped in Bedrock-hosted models for **both** embedding and\n",
    "generation. Larger, higher-quality models can definitely help a ton — especially on\n",
    "messy real-world questions — but it's important to remember that they are still just\n",
    "**one component** in your RAG system.\n",
    "\n",
    "- **Bigger or newer models do not magically fix weak retrieval.** If your chunks\n",
    "  are poorly aligned with the questions, a very strong LLM will still struggle.\n",
    "- **Most of the long‑term accuracy gains in RAG systems come from the plumbing\n",
    "  around the LLMs**, including:\n",
    "  - smarter / semantic chunking strategies\n",
    "  - good metadata and filtering\n",
    "  - reranking or multi‑stage retrieval\n",
    "  - domain‑specific heuristics and post‑processing\n",
    "- **Cost and latency live in tension with quality.** Larger models (or higher\n",
    "  token budgets) often improve answers, but at the cost of more inference time\n",
    "  and higher per‑request spend. Bedrock makes it easier to experiment with that\n",
    "  tradeoff by switching models without rewriting your pipeline.\n",
    "\n",
    "As you adapt this notebook to your own projects, treat the LLM choice as **one\n",
    "tunable component** in a larger system. Iterating on chunking, indexing, and\n",
    "retrieval policies will almost always give you more headroom than swapping\n",
    "between already-good models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
