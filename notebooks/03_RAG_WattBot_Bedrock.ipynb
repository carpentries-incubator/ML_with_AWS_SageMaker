{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc942269",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with Bedrock\"\n",
    "teaching: 30\n",
    "exercises: 20\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- TODO\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- TODO\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "In the previous episodes you built a basic RAG pipeline for WattBot using a local GPU instance and then an offline SageMaker Processing job. Both approaches gave you full control over the models, but you were responsible for provisioning compute and keeping model versions up to date.\n",
    "\n",
    "In this episode we move the core model work — **both text embeddings and answer generation** — onto **Amazon Bedrock**. We'll use:\n",
    "\n",
    "- an **Amazon Titan Text Embeddings V2** model to turn WattBot chunks into vectors, and  \n",
    "- an **Anthropic Claude** model hosted on Bedrock to generate answers and explanations.\n",
    "\n",
    "The retrieval, evaluation, and WattBot scoring logic are exactly the same as before; we're just\n",
    "swapping out the underlying models and where they run. This lets you experiment with hosted,\n",
    "state‑of‑the‑art models without having to manage GPUs or container images yourself.\n",
    "\n",
    "## Why Bedrock for WattBot?\n",
    "\n",
    "For the GPU instance and Processing Job episodes, you were responsible for picking a model,\n",
    "managing versions, and making sure your instance had enough VRAM. That’s fine for experiments,\n",
    "but it can get painful once multiple teams or challenges want to reuse the same pipeline.\n",
    "\n",
    "Running your **embedding + generation** steps on Amazon Bedrock gives you a few nice properties:\n",
    "\n",
    "- **Managed, up‑to‑date models.** You can use high‑quality models from Anthropic, Amazon, and\n",
    "  others without worrying about container images or CUDA versions.\n",
    "- **Pay for what you use (in tokens).** Instead of paying for a GPU instance that might sit\n",
    "  idle, you pay per token (input + output) when you call the model. For some workloads this\n",
    "  is cheaper; for large offline batches with smaller models, a dedicated GPU can still win.\n",
    "- **Easier sharing and governance.** It’s easier to standardize on a small set of Bedrock\n",
    "  models across courses, hackathons, or labs than to manage many separate GPU instances.\n",
    "\n",
    "In this notebook, we’ll keep the same WattBot training questions and scoring helper you used\n",
    "before, and we’ll simply move both the **embedding** and **answer/explanation** steps onto\n",
    "Bedrock-hosted models.\n",
    "\n",
    "\n",
    "## Setup: what you should already have\n",
    "\n",
    "This notebook assumes you have already run the earlier WattBot episodes so that:\n",
    "\n",
    "- the WattBot corpus has been chunked into `chunks.jsonl`\n",
    "- the WattBot training questions `train_QA.csv` and `metadata.csv` live under a `data/` folder\n",
    "- (optionally) you have a local embedding file from earlier experiments, e.g. `embeddings.npy`\n",
    "\n",
    "In this episode we’ll recompute embeddings **using an Amazon Titan Text Embeddings V2 model\n",
    "on Bedrock**, and we’ll save those vectors out as `embeddings_bedrock.npy`. That keeps this\n",
    "notebook self‑contained while still letting you compare against the earlier GPU / Processing\n",
    "Job runs if you want.\n",
    "\n",
    "\n",
    "### Models used in this episode\n",
    "\n",
    "We’ll work with **Amazon Bedrock–hosted foundation models** for both embedding and generation:\n",
    "\n",
    "- **Amazon Titan Text Embeddings V2** (`amazon.titan-embed-text-v2:0`)\n",
    "\n",
    "  - General‑purpose text‑embedding model for semantic search, retrieval, clustering, and classification.\n",
    "  - Supports configurable embedding dimensions (for example 256–8,192) and has presets tuned for retrieval or binary indexing.\n",
    "  - AWS does not publish the exact number of parameters for Titan models; you can treat it as a modern transformer specialized for embeddings rather than free‑form text generation. \n",
    "\n",
    "- **Anthropic Claude 3 Haiku** (`anthropic.claude-3-haiku-20240307-v1:0` via Bedrock)\n",
    "\n",
    "  - A fast, mid‑sized Claude model that balances cost and quality for workloads like RAG, chat, and lightweight analysis.\n",
    "  - Particularly useful when you want many calls (e.g., one per question) and care about low latency and lower per‑token pricing compared to flagship models such as Claude Opus or Claude 3.5 Sonnet. \n",
    "  - Anthropic does not publish exact parameter counts for Claude models; Haiku sits in the “smallest / fastest” tier within the Claude 3 family.\n",
    "\n",
    "- **(Optional) Multimodal models for tables and figures**\n",
    "\n",
    "  - Bedrock also exposes **multimodal models** that can reason over images, charts, and document layouts (for example, Claude 3.5 Sonnet with vision, or Amazon Titan Multimodal Embeddings). These are a good fit if much of your evidence lives in **figures, tables, or scanned PDFs**.\n",
    "  - To use them from Bedrock you send **both text and image content** in a single request:\n",
    "      - Pre‑process PDFs by rendering pages (or cropping individual tables/figures) to images using a tool like `pdf2image` or a headless browser.\n",
    "      - Base64‑encode those images and include them as image parts alongside text in the model request.\n",
    "      - For multimodal embeddings, you call a Titan multimodal embedding model with an `inputImage` (and optionally `inputText`) payload to obtain a single vector that mixes visual and textual information.\n",
    "  - This notebook stays with **text‑only** embeddings + generation to keep the workflow simple, but the same RAG pattern extends naturally to multimodal models once you add an image‑extraction step to your preprocessing pipeline.\n",
    "\n",
    "For a full catalog of available models (including other Claude variants, Amazon models, and partner models), open the **Model catalog** in the Amazon Bedrock console. Each entry provides a model card with capabilities, typical use cases, and pricing details so learners can explore alternatives for their own RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2bebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# ---- AWS configuration ----\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Claude 3 Haiku is a good starting point for batch evaluation.\n",
    "# Swap for Sonnet/Opus if you have access and want higher quality.\n",
    "bedrock_model_id = \"deepseek.v3-v1:0\"\n",
    "\n",
    "# S3 bucket + keys where Episode 02 wrote the artifacts.\n",
    "# TODO: Update these keys to match your pipeline.\n",
    "bucket_name = \"chris-rag\"  # <-- change to your bucket\n",
    "chunks_key = \"chunks.jsonl\"\n",
    "# embeddings_key = \"embeddings/embeddings.npy\"\n",
    "train_key = \"train_QA.csv\"\n",
    "metadata_key = \"metadata.csv\"\n",
    "\n",
    "# Local working directory for downloaded artifacts\n",
    "local_data_dir = \"bedrock\"\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "\n",
    "# AWS clients\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_s3(key: str, local_name: str) -> str:\n",
    "    \"\"\"Download a file from S3 to local_data_dir and return the local path.\"\"\"\n",
    "    local_path = os.path.join(local_data_dir, local_name)\n",
    "    print(f\"Downloading s3://{bucket_name}/{key} -> {local_path}\")\n",
    "    s3.download_file(bucket_name, key, local_path)\n",
    "    return local_path\n",
    "\n",
    "\n",
    "chunks_path = download_from_s3(chunks_key, \"chunks.jsonl\")\n",
    "# emb_path = download_from_s3(embeddings_key, \"embeddings.npy\")\n",
    "train_qa_path = download_from_s3(train_key, \"train_QA.csv\")\n",
    "metadata_path = download_from_s3(metadata_key, \"metadata.csv\")\n",
    "\n",
    "# Load artifacts\n",
    "with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunked_docs = [json.loads(line) for line in f]\n",
    "\n",
    "# chunk_embeddings = np.load(emb_path)\n",
    "train_df = pd.read_csv(train_qa_path)\n",
    "\n",
    "# Robust metadata load: handle possible non-UTF-8 characters\n",
    "try:\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "except UnicodeDecodeError:\n",
    "    metadata_df = pd.read_csv(metadata_path, encoding=\"latin1\")\n",
    "\n",
    "print(f\"Chunks: {len(chunked_docs)}\")\n",
    "print(f\"Train QAs: {len(train_df)}\")\n",
    "# print(\"Embeddings shape:\", chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14aaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_for_question_bedrock(\n",
    "    question: str,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve top-k chunks for a question using Bedrock embeddings.\n",
    "\n",
    "    We call the Bedrock embedding model (via `bedrock_embed_text`) to\n",
    "    embed the question, then compute cosine similarity against the\n",
    "    pre-computed `chunk_embeddings` array.\n",
    "    \"\"\"\n",
    "    # Embed the question with the same Bedrock model used for chunks\n",
    "    q_emb = bedrock_embed_text(question)\n",
    "\n",
    "    # Use the same cosine similarity + top-k helper as before\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    return retrieved, q_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ab239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping from doc_id -> URL so we can surface links in our outputs\n",
    "docid_to_url = {}\n",
    "for _, row in metadata_df.iterrows():\n",
    "    doc_id = str(row.get(\"id\", \"\")).strip()\n",
    "    url = row.get(\"url\", \"\")\n",
    "    if doc_id and isinstance(url, str) and url.strip():\n",
    "        docid_to_url[doc_id] = url.strip()\n",
    "\n",
    "print(f\"docid_to_url has {len(docid_to_url)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebacb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Bedrock embeddings for WattBot chunks\n",
    "# ----------------------------------------------------------------------------------\n",
    "embedding_model_id_bedrock = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "emb_save_path = data_dir / \"embeddings_bedrock.npy\"\n",
    "\n",
    "def bedrock_embed_text(text: str, model_id: str = embedding_model_id_bedrock):\n",
    "    \"\"\"Call a Bedrock embedding model for a single input string.\"\"\"\n",
    "    body = json.dumps({\"inputText\": text})\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body,\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    if embedding is None:\n",
    "        raise ValueError(f\"No 'embedding' found in response: {response_body}\")\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# If an embedding file already exists, skip recomputing and load it instead\n",
    "# -------------------------------------------------------------------------\n",
    "if emb_save_path.exists():\n",
    "    print(f\"Found existing embeddings at {emb_save_path}. Skipping re-computation.\")\n",
    "    chunk_embeddings = np.load(emb_save_path)\n",
    "else:\n",
    "    print(\"No existing embeddings found. Computing via Bedrock...\")\n",
    "\n",
    "    all_embeddings = []\n",
    "    for idx, ch in enumerate(chunked_docs):\n",
    "        if (idx + 1) % 250 == 0:\n",
    "            print(f\"Embedding chunk {idx+1} / {len(chunked_docs)}\")\n",
    "        text = ch.get(\"text\", \"\")\n",
    "        emb = bedrock_embed_text(text)\n",
    "        all_embeddings.append(emb)\n",
    "\n",
    "    chunk_embeddings = np.array(all_embeddings, dtype=\"float32\")\n",
    "\n",
    "    # Save embeddings for reuse\n",
    "    np.save(emb_save_path, chunk_embeddings)\n",
    "    print(f\"Saved embeddings to {emb_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings so we can reuse them later without re-calling Bedrock\n",
    "np.save(emb_save_path, chunk_embeddings)\n",
    "print(\"Saved Bedrock chunk embeddings to\", emb_save_path)\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- similarity + retrieval ----------------------\n",
    "\n",
    "# ---------------------- similarity + retrieval ----------------------\n",
    "\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Cosine similarity between two sets of vectors.\n",
    "\n",
    "    This helper is intentionally defensive: it will accept Python lists,\n",
    "    list-of-lists, or NumPy arrays and cast everything to float32 arrays\n",
    "    before computing similarities.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=\"float32\")\n",
    "    b = np.asarray(b, dtype=\"float32\")\n",
    "\n",
    "    # Ensure 2D\n",
    "    if a.ndim == 1:\n",
    "        a = a.reshape(1, -1)\n",
    "    if b.ndim == 1:\n",
    "        b = b.reshape(1, -1)\n",
    "\n",
    "    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return np.matmul(a_norm, b_norm.T)\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_top_k(\n",
    "    query_embedding: np.ndarray,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    k: int = 8,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return the top–k chunks for a single query embedding.\n",
    "\n",
    "    Accepts query/collection embeddings as either NumPy arrays or lists.\n",
    "    \"\"\"\n",
    "    # Defensive casting in case we accidentally pass in lists\n",
    "    query = np.asarray(query_embedding, dtype=\"float32\").reshape(1, -1)\n",
    "    chunks = np.asarray(chunk_embeddings, dtype=\"float32\")\n",
    "\n",
    "    sims = cosine_similarity_matrix(query, chunks)[0]\n",
    "\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        ch = chunked_docs[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"text\": ch[\"text\"],\n",
    "                \"doc_id\": ch.get(\"doc_id\", \"\"),\n",
    "                \"title\": ch.get(\"title\", \"\"),\n",
    "                \"url\": ch.get(\"url\", \"\"),\n",
    "                \"page_num\": ch.get(\"page_num\", None),\n",
    "                \"page_label\": ch.get(\"page_label\", None),\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def format_context_for_prompt(retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"Turn retrieved chunk dicts into a compact context string for the LLM.\"\"\"\n",
    "    lines = []\n",
    "    for i, ch in enumerate(retrieved_chunks, start=1):\n",
    "        label = ch.get(\"doc_id\", f\"chunk_{i}\")\n",
    "        page = ch.get(\"page_label\", ch.get(\"page_num\", \"\"))\n",
    "        header = f\"[{label}, page {page}]\".strip()\n",
    "        txt = ch[\"text\"].replace(\"\\n\", \" \")\n",
    "        lines.append(f\"{header}: {txt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def retrieve_context_for_question(\n",
    "    question: str,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"Use Bedrock embeddings to retrieve the top-k chunks for a question.\"\"\"\n",
    "    # Embed question with Bedrock and make sure we end up with a 1D float32 vector\n",
    "    q_vec = bedrock_embed_text(question)\n",
    "    q_emb = np.asarray(q_vec, dtype=\"float32\")\n",
    "\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    return retrieved, q_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- answer normalization ----------------------\n",
    "\n",
    "def normalize_answer_value(raw_value: str) -> str:\n",
    "    \"\"\"Normalize answer_value according to WattBot conventions.\"\"\"\n",
    "    if raw_value is None:\n",
    "        return \"is_blank\"\n",
    "\n",
    "    s = str(raw_value).strip()\n",
    "\n",
    "    if not s or s.lower() == \"none\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        return s\n",
    "\n",
    "    if s.lower() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # If there is whitespace, keep only the first token\n",
    "    if \" \" in s:\n",
    "        first, *_ = s.split()\n",
    "        s = first\n",
    "\n",
    "    # Remove commas\n",
    "    s = s.replace(\",\", \"\")\n",
    "\n",
    "    try:\n",
    "        val = float(s)\n",
    "        if val.is_integer():\n",
    "            return str(int(val))\n",
    "        return f\"{val:.10g}\"  # avoid scientific notation\n",
    "    except ValueError:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e750c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bedrock_claude(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    model_id: str = bedrock_model_id,\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.3,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call a Bedrock chat model (Anthropic 4.x / Claude 3.5 / Llama 3.x, etc.)\n",
    "    that uses the OpenAI-style chat completions schema.\n",
    "    \"\"\"\n",
    "    # OpenAI-style chat body – this is what your error message is asking for\n",
    "    body = {\n",
    "        \"model\": model_id,  # some models allow omitting this, but it's safe to include\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    request = json.dumps(body)\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "    except ClientError as e:\n",
    "        print(f\"ERROR calling Bedrock model {model_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # OpenAI-style response: choices[0].message.content\n",
    "    try:\n",
    "        text = model_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception:\n",
    "        # Fallback / debug\n",
    "        print(\"Unexpected model response:\", model_response)\n",
    "        raise\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- explanation helpers ----------------------\n",
    "\n",
    "def explanation_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an AI assistant that explains how evidence supports answers about \"\n",
    "        \"energy, water, and carbon footprint of AI models.\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Write 1–3 sentences.\\n\"\n",
    "        \"- Directly explain how the cited supporting materials justify the answer.\\n\"\n",
    "        \"- Do NOT include any planning text, meta-reasoning, or tags like <reasoning>.\\n\"\n",
    "        \"- Do NOT start with phrases like 'We need to answer'—just give the explanation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def explanation_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an AI assistant that explains how evidence supports answers about \"\n",
    "        \"energy, water, and carbon footprint. Focus on clear, factual reasoning, \"\n",
    "        \"and refer directly to the cited documents when appropriate.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def bedrock_explanation_phase_for_question(\n",
    "    qid: str,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    supporting_materials: str,\n",
    "    model_id: str = bedrock_model_id,\n",
    ") -> str:\n",
    "    sys_prompt = explanation_system_prompt()\n",
    "    prompt = build_explanation_prompt(question, answer, supporting_materials)\n",
    "    raw_explanation = call_bedrock_claude(\n",
    "        system_prompt=sys_prompt,\n",
    "        user_prompt=prompt,\n",
    "        model_id=model_id,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    return raw_explanation.strip()\n",
    "\n",
    "\n",
    "# ---------------------- answer phase (JSON contract) ----------------------\n",
    "\n",
    "def bedrock_answer_phase_for_question(\n",
    "    qid: str,\n",
    "    question: str,\n",
    "    retrieved_chunks: List[Dict[str, Any]],\n",
    "    model_id: str = bedrock_model_id,\n",
    "):\n",
    "    \"\"\"Use Bedrock to answer a single WattBot question given retrieved chunks.\"\"\"\n",
    "    context = format_context_for_prompt(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are WattBot, a question-answering assistant for energy, water, and carbon footprint.\\n\"\n",
    "        \"You must answer questions using ONLY the provided context from scientific papers.\\n\"\n",
    "        \"If the context does not contain enough information to answer or infer,\\n\"\n",
    "        \"you must mark the question as unanswerable.\\n\\n\"\n",
    "        \"You must respond with a single JSON object with the following keys:\\n\"\n",
    "        \"- answer: natural language answer, including numeric value and units if applicable.\\n\"\n",
    "        \"- answer_value: normalized numeric (0 for false, 1 for true), or categorical value with NO units or symbols;\\n\"\n",
    "        \"  use 'is_blank' if the question is unanswerable.\\n\"\n",
    "        \"- answer_unit: unit string (e.g., kWh, gCO2, %, is_blank).\\n\"\n",
    "        \"- ref_id: list of document IDs that support the answer, e.g., ['ID1', 'ID2'].\\n\"\n",
    "        \"- is_blank: true if unanswerable, false otherwise.\\n\"\n",
    "        \"- supporting_materials: short quote or table/figure pointer from the context.\\n\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        \"Use the context below to answer the question. \"\n",
    "        \"Return ONLY a JSON object, no extra commentary.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\"\n",
    "    )\n",
    "\n",
    "    raw_answer = call_bedrock_claude(\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        model_id=model_id,\n",
    "        max_tokens=512,\n",
    "    )\n",
    "\n",
    "    parsed = {\n",
    "        \"answer\": \"\",\n",
    "        \"answer_value\": \"is_blank\",\n",
    "        \"answer_unit\": \"is_blank\",\n",
    "        \"ref_id\": [],\n",
    "        \"is_blank\": True,\n",
    "        \"supporting_materials\": \"is_blank\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        first_brace = raw_answer.find(\"{\")\n",
    "        last_brace = raw_answer.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            json_str = raw_answer[first_brace : last_brace + 1]\n",
    "        else:\n",
    "            json_str = raw_answer\n",
    "\n",
    "        candidate = json.loads(json_str)\n",
    "\n",
    "        parsed[\"answer\"] = candidate.get(\"answer\", \"\").strip()\n",
    "        parsed[\"answer_value\"] = normalize_answer_value(candidate.get(\"answer_value\", \"is_blank\"))\n",
    "        parsed[\"answer_unit\"] = str(candidate.get(\"answer_unit\", \"is_blank\")).strip() or \"is_blank\"\n",
    "\n",
    "        ref_id = candidate.get(\"ref_id\", [])\n",
    "        if isinstance(ref_id, str):\n",
    "            ref_ids = [ref_id]\n",
    "        elif isinstance(ref_id, list):\n",
    "            ref_ids = [str(x).strip() for x in ref_id if x]\n",
    "        else:\n",
    "            ref_ids = []\n",
    "        parsed[\"ref_id\"] = ref_ids\n",
    "\n",
    "        is_blank_flag = candidate.get(\"is_blank\", False)\n",
    "        parsed[\"is_blank\"] = bool(is_blank_flag)\n",
    "\n",
    "        supp = candidate.get(\"supporting_materials\", \"is_blank\")\n",
    "        parsed[\"supporting_materials\"] = str(supp).strip() or \"is_blank\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parse error for question {qid}; defaulting to is_blank. Error: {e}\")\n",
    "\n",
    "    return (\n",
    "        parsed[\"answer\"],\n",
    "        parsed[\"answer_value\"],\n",
    "        parsed[\"is_blank\"],\n",
    "        parsed[\"ref_id\"],\n",
    "        parsed[\"supporting_materials\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_qa_bedrock(\n",
    "    row,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs,\n",
    "    docid_to_url: dict,\n",
    "    top_k: int = 8,\n",
    "    retrieval_threshold: float = 0.25,\n",
    "    model_id: str = \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline for a single question using Bedrock for both retrieval-time\n",
    "    embeddings and generation.\n",
    "    \"\"\"\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "\n",
    "    # 1. Retrieve supporting chunks using Bedrock embeddings for the query\n",
    "    retrieved, q_emb = retrieve_context_for_question_bedrock(\n",
    "        question=question,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n",
    "\n",
    "    # 2. Call Bedrock Claude to produce answer JSON\n",
    "    (\n",
    "        answer,\n",
    "        answer_value,\n",
    "        is_blank_llm,\n",
    "        ref_ids,\n",
    "        supporting_materials,\n",
    "    ) = bedrock_answer_phase_for_question(\n",
    "        qid=qid,\n",
    "        question=question,\n",
    "        retrieved_chunks=retrieved,\n",
    "        model_id=model_id,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. DECISION: retrieval_threshold OR model blank?\n",
    "    # --------------------------------------------------------\n",
    "    # NOTE: we only tell the user when it *actually* gets blanked.\n",
    "    if is_blank_llm:\n",
    "        print(f\"[diag][{qid}] → Model returned is_blank (LLM could not answer).\")\n",
    "    elif top_score < retrieval_threshold:\n",
    "        print(\n",
    "            f\"[diag][{qid}] → Retrieval blocked: top cosine={top_score:.3f} \"\n",
    "            f\"< threshold={retrieval_threshold:.3f}\"\n",
    "        )\n",
    "    is_blank = bool(is_blank_llm) or (top_score < retrieval_threshold)\n",
    "\n",
    "    if is_blank:\n",
    "        answer = \"Unable to answer with confidence based on the provided documents.\"\n",
    "        answer_value = \"is_blank\"\n",
    "        answer_unit = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "        supporting_materials = \"is_blank\"\n",
    "        explanation = \"\"\n",
    "    else:\n",
    "        answer_value = normalize_answer_value(answer_value)\n",
    "        answer_unit = \"is_blank\"\n",
    "\n",
    "        if isinstance(ref_ids, list) and ref_ids:\n",
    "            ref_id_str = \";\".join(ref_ids)\n",
    "            urls = []\n",
    "            for rid in ref_ids:\n",
    "                url = docid_to_url.get(str(rid), \"\")\n",
    "                if url:\n",
    "                    urls.append(url)\n",
    "            ref_url_str = \";\".join(urls) if urls else \"is_blank\"\n",
    "        else:\n",
    "            ref_id_str = \"is_blank\"\n",
    "            ref_url_str = \"is_blank\"\n",
    "\n",
    "        explanation = bedrock_explanation_phase_for_question(\n",
    "            qid=qid,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            supporting_materials=supporting_materials,\n",
    "            model_id=model_id,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_value\": answer_value,\n",
    "        \"answer_unit\": answer_unit,\n",
    "        \"ref_id\": ref_id_str,\n",
    "        \"ref_url\": ref_url_str,\n",
    "        \"supporting_materials\": supporting_materials,\n",
    "        \"explanation\": explanation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74508d36",
   "metadata": {},
   "source": [
    "## Run the WattBot evaluation with Bedrock\n",
    "\n",
    "Now we can loop over all questions in `train_QA.csv`, run retrieval + Bedrock\n",
    "generation, and write a `wattbot_solutions_bedrock.csv` file.\n",
    "\n",
    "This mirrors the logic from Episode 02 – the only difference is that the answer\n",
    "and explanation phases call a hosted Claude 3 model instead of a local Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# For quick smoke tests, you can slice train_df (e.g., train_df.head(5))\n",
    "for _, row in train_df.iterrows():\n",
    "    question = row[\"question\"]\n",
    "    print(\"#\" * 96)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "\n",
    "    out = run_single_qa_bedrock(\n",
    "        row=row,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        docid_to_url=docid_to_url,\n",
    "        top_k=20,\n",
    "        retrieval_threshold=0.1,\n",
    "        model_id=bedrock_model_id,\n",
    "    )\n",
    "\n",
    "    answer = out[\"answer\"]\n",
    "    ref_ids = out[\"ref_id\"]\n",
    "    explanation = out[\"explanation\"]\n",
    "\n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    print(f\"ref_ids: {ref_ids}\")\n",
    "    print(f\"EXPLANATION: {explanation}\")\n",
    "\n",
    "    results.append(out)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"wattbot_solutions_bedrock.csv\")\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Wrote predictions to {output_path}\")\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23976045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"True\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"False\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",   # can also pass \"is_blank\" or None\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,    # can pass \"is_blank\", or leave None to auto\n",
    "    n_examples=10,          # how many incorrect examples to print\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "\n",
    "    NA logic:\n",
    "    - If an explicit NA column is found/used (e.g. is_NA), we use it via _to_bool_flag.\n",
    "    - If you pass gt_is_na_col=\"is_blank\" or pred_is_na_col=\"is_blank\",\n",
    "      we *derive* NA from answer_value == \"is_blank\" instead of expecting a real column.\n",
    "    - If no NA column is available at all, we derive from answer_value == \"is_blank\".\n",
    "\n",
    "    Also prints up to `n_examples` rows where the model is not perfect\n",
    "    (answer_score < 1, ref_id_score < 1, or is_NA_score < 1).\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "\n",
    "    # ----- ground truth NA flags -----\n",
    "    if gt_is_na_col is not None and gt_is_na_col in merged.columns:\n",
    "        # Use explicit column (e.g. \"is_NA\")\n",
    "        gt_is_na_series = merged[gt_is_na_col].map(_to_bool_flag)\n",
    "    elif gt_is_na_col is not None and gt_is_na_col.lower() == \"is_blank\":\n",
    "        # Special meaning: derive NA from answer_value_gt == \"is_blank\"\n",
    "        gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "    else:\n",
    "        # Fallback: if we have is_NA or is_blank col, use it; else derive\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "\n",
    "    # ----- prediction NA flags -----\n",
    "    if pred_is_na_col is not None and pred_is_na_col in merged.columns:\n",
    "        pred_is_na_series = merged[pred_is_na_col].map(_to_bool_flag)\n",
    "    elif pred_is_na_col is not None and pred_is_na_col.lower() == \"is_blank\":\n",
    "        # Same convention: derive from answer_value_pred\n",
    "        pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "    else:\n",
    "        # Auto-detect or derive if no NA column in preds\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "\n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = bool(gt_is_na_series.iloc[idx])\n",
    "        pred_is_na = bool(pred_is_na_series.iloc[idx])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    # ----- Show some incorrect examples -----\n",
    "    incorrect = merged[\n",
    "        (merged[\"answer_score\"] < 1.0)\n",
    "        | (merged[\"ref_id_score\"] < 1.0)\n",
    "        | (merged[\"is_NA_score\"] < 1.0)\n",
    "    ]\n",
    "    \n",
    "    if not incorrect.empty and n_examples > 0:\n",
    "        print(\"\\nExamples of incorrect / partially correct responses \"\n",
    "              f\"(up to {n_examples} rows):\\n\")\n",
    "        # Grab up to n_examples \"worst\" rows by wattbot_score\n",
    "        for _, row in incorrect.sort_values(\"wattbot_score\").head(n_examples).iterrows():\n",
    "            q = row[\"question_gt\"] if \"question_gt\" in row.index else None\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"id: {row[id_col]}\")\n",
    "            if q is not None:\n",
    "                print(f\"Question: {q}\")\n",
    "            print(f\"GT answer_value:   {row[f'{gt_answer_col}_gt']}\")\n",
    "            print(f\"Pred answer_value: {row[f'{pred_answer_col}_pred']}\")\n",
    "            print(f\"GT ref_id:         {row[f'{gt_ref_col}_gt']}\")\n",
    "            print(f\"Pred ref_id:       {row[f'{pred_ref_col}_pred']}\")\n",
    "            print(f\"answer_score: {row['answer_score']:.3f}, \"\n",
    "                  f\"ref_id_score: {row['ref_id_score']:.3f}, \"\n",
    "                  f\"is_NA_score: {row['is_NA_score']:.3f}, \"\n",
    "                  f\"wattbot_score: {row['wattbot_score']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f88fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Normalize reference IDs + answer ranges after results are created\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from typing import Any\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def normalize_ref_ids(refs: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize reference IDs to a Python-list-style string.\n",
    "\n",
    "    Output format examples:\n",
    "      Input                       → Output\n",
    "      ---------------------------------------------------------\n",
    "      \"chen2024\"                 → \"['chen2024']\"\n",
    "      ['chen2024']               → \"['chen2024']\"\n",
    "      \"[chen2024]\"               → \"['chen2024']\"\n",
    "      \"['chen2024']\"             → \"['chen2024']\"\n",
    "\n",
    "      \"chen2024;smith2023\"       → \"['chen2024', 'smith2023']\"\n",
    "      \"chen2024, smith2023\"      → \"['chen2024', 'smith2023']\"\n",
    "      \"[wu2021b;wu2021a]\"        → \"['wu2021b', 'wu2021a']\"\n",
    "      ['wu2021b','wu2021a']      → \"['wu2021b', 'wu2021a']\"\n",
    "\n",
    "      None                       → \"is_blank\"\n",
    "      \"is_blank\"                 → \"is_blank\"\n",
    "\n",
    "    Rules:\n",
    "      - \"is_blank\" stays exactly \"is_blank\".\n",
    "      - Semicolons are treated as separators (→ commas).\n",
    "      - Strips stray brackets, quotes, spaces.\n",
    "      - Produces Python-list-style: ['id'] or ['id1', 'id2'].\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # ----- 1. Handle blanks -----\n",
    "    if refs is None or str(refs).strip() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # ----- 2. True iterable input -----\n",
    "    if isinstance(refs, (list, tuple, np.ndarray)):\n",
    "        cleaned = [str(x).strip().strip(\"[]'\\\" \") for x in refs if str(x).strip()]\n",
    "        return \"[\" + \", \".join(f\"'{c}'\" for c in cleaned) + \"]\"\n",
    "\n",
    "    # ----- 3. Treat as string -----\n",
    "    s = str(refs).strip()\n",
    "\n",
    "    # Strip outer brackets if present (e.g., \"[chen2024]\" or \"['chen2024']\")\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        s = s[1:-1].strip()\n",
    "\n",
    "    # Replace semicolons with commas\n",
    "    s = s.replace(\";\", \",\")\n",
    "\n",
    "    # Split, strip quotes/spaces\n",
    "    parts = [p.strip().strip(\"'\\\"\") for p in s.split(\",\") if p.strip()]\n",
    "\n",
    "    if len(parts) == 0:\n",
    "        return \"is_blank\"\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        return f\"['{parts[0]}']\"\n",
    "\n",
    "    return \"[\" + \", \".join(f\"'{p}'\" for p in parts) + \"]\"\n",
    "\n",
    "\n",
    "\n",
    "def normalize_answer_value(val: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize answer_value so that:\n",
    "      - single numbers stay as-is (300 -> \"300\")\n",
    "      - ranges get bracketed (\"300-1000\" -> \"[300,1000]\")\n",
    "      - lists/tuples become bracketed ranges\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "\n",
    "    # list / tuple / array → always a range\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        vals = []\n",
    "        for v in val:\n",
    "            # convert ints cleanly\n",
    "            if isinstance(v, (int, float)) and float(v).is_integer():\n",
    "                vals.append(str(int(v)))\n",
    "            else:\n",
    "                vals.append(str(v))\n",
    "        return \"[\" + \",\".join(vals) + \"]\"\n",
    "\n",
    "    # numeric scalar → leave alone\n",
    "    if isinstance(val, (int, float)):\n",
    "        if float(val).is_integer():\n",
    "            return str(int(val))\n",
    "        return str(val)\n",
    "\n",
    "    # string cases\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "\n",
    "        # already bracketed\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            return s\n",
    "\n",
    "        # detect range: 300-1000 or 300 – 1000\n",
    "        m = re.match(r\"^\\s*([0-9]+(?:\\.[0-9]+)?)\\s*[-–—]\\s*([0-9]+(?:\\.[0-9]+)?)\\s*$\", s)\n",
    "        if m:\n",
    "            a, b = m.groups()\n",
    "            # strip trailing .0\n",
    "            a = a.rstrip(\".0\")\n",
    "            b = b.rstrip(\".0\")\n",
    "            return f\"[{a},{b}]\"\n",
    "\n",
    "        # otherwise single value → leave alone\n",
    "        return s\n",
    "\n",
    "    # fallback: return string without brackets\n",
    "    return str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd532eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "solutions_df = pd.read_csv(output_dir + \"/wattbot_solutions_bedrock.csv\")\n",
    "solutions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_df[\"ref_id\"] = solutions_df[\"ref_id\"].apply(normalize_ref_ids)\n",
    "solutions_df[\"answer_value\"] = solutions_df[\"answer_value\"].apply(normalize_answer_value)\n",
    "solutions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_df.to_csv(output_dir + \"/solutions_normalized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc5314",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=output_dir + \"/solutions_normalized.csv\",\n",
    "    gt_is_na_col=\"is_NA\",   # or \"is_blank\" / None depending on how you mark NAs\n",
    "    n_examples=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2988953",
   "metadata": {},
   "source": [
    "## Wrap‑up: comparing Bedrock to GPU‑based runs\n",
    "\n",
    "At this point you should have three versions of the WattBot evaluation:\n",
    "\n",
    "1. **Episode 01 – Notebook GPU instance** using a locally loaded open‑source model.  \n",
    "2. **Episode 02 – SageMaker Processing job** running the same model in batch with on-demand compute. \n",
    "3. **Episode 03 – Bedrock** using a hosted Claude 3 model with per‑token billing.\n",
    "\n",
    "When deciding between these options in practice:\n",
    "\n",
    "- Use **Bedrock or other hosted APIs** when:\n",
    "  - You want to try the latest frontier models quickly.  \n",
    "  - You only need to run a modest number of questions, or you are still prototyping.  \n",
    "  - You prefer a simple, token‑based cost model and don’t want to manage GPU capacity.\n",
    "\n",
    "- Use **self‑hosted models on GPU instances** when:\n",
    "  - You expect to run large batches repeatedly (e.g., many thousands of questions).  \n",
    "  - You want tight control over which architectures/checkpoints you run or fine‑tune.  \n",
    "  - You already have institutional access to cost‑effective on‑prem or cloud GPUs.\n",
    "\n",
    "The core **RAG evaluation logic stays identical** across all three episodes, which is the main takeaway:\n",
    "once you have a clean retrieval + normalization pipeline (like WattBot’s), swapping out the generator\n",
    "is mostly a matter of re‑implementing `answer_phase_for_question` and `explanation_phase_for_question`\n",
    "for each compute option you care about.\n",
    "\n",
    "\n",
    "## Concluding remarks: Bedrock models are one piece of the RAG puzzle\n",
    "\n",
    "In this episode we swapped in Bedrock-hosted models for **both** embedding and\n",
    "generation. Larger, higher-quality models can definitely help a ton — especially on\n",
    "messy real-world questions — but it's important to remember that they are still just\n",
    "**one component** in your RAG system.\n",
    "\n",
    "- **Bigger or newer models do not magically fix weak retrieval.** If your chunks\n",
    "  are poorly aligned with the questions, a very strong LLM will still struggle.\n",
    "- **Most of the long‑term accuracy gains in RAG systems come from the plumbing\n",
    "  around the LLMs**, including:\n",
    "  - smarter / semantic chunking strategies\n",
    "  - good metadata and filtering\n",
    "  - reranking or multi‑stage retrieval\n",
    "  - domain‑specific heuristics and post‑processing\n",
    "- **Cost and latency live in tension with quality.** Larger models (or higher\n",
    "  token budgets) often improve answers, but at the cost of more inference time\n",
    "  and higher per‑request spend. Bedrock makes it easier to experiment with that\n",
    "  tradeoff by switching models without rewriting your pipeline.\n",
    "\n",
    "As you adapt this notebook to your own projects, treat the LLM choice as **one\n",
    "tunable component** in a larger system. Iterating on chunking, indexing, and\n",
    "retrieval policies will almost always give you more headroom than swapping\n",
    "between already-good models.\n",
    "\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- TODO\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
