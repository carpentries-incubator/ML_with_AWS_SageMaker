{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce2cdf5",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with a Notebook GPU\"\n",
    "teaching: 30\n",
    "exercises: 15\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How can we run a basic Retrieval-Augmented Generation (RAG) pipeline entirely from a single GPU-backed SageMaker notebook?\n",
    "- How do we go from raw PDFs and CSV files to a searchable embedding space for WattBot documents?\n",
    "- How can we generate WattBot-style answers (including citations and evidence) that follow the competition’s scoring conventions?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Verify that our SageMaker notebook instance has a working GPU and compatible Python environment.\n",
    "- Load the WattBot metadata and question–answer files from local storage and inspect their structure.\n",
    "- Download all referenced PDFs from `metadata.csv` and turn them into a collection of text pages with useful metadata attached.\n",
    "- Implement a simple, explicit “from scratch” text-chunking and embedding pipeline without relying on FAISS or production vector DBs.\n",
    "- Build a small retrieval helper that finds the most relevant chunks for a question using cosine similarity in embedding space.\n",
    "- Wire the retriever to a local Qwen 7B-style generator to produce WattBot-format answers (including `answer`, `ref_id`, `ref_url`, and `supporting_materials`).\n",
    "- Add a second LLM pass that generates short explanations and marks whether the evidence comes from text, figures, tables, or a combination.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8167de7",
   "metadata": {},
   "source": [
    "\n",
    "## Working with AWS for RAG Experiments \n",
    "\n",
    "In the previous episode, we briefly introduced several approaches for implementing RAG in AWS. Here, we are simply selecting a good GPU instance that can handle whatever RAG system we want to build. \n",
    "\n",
    "Here we reinforce several important best‑practice ideas before building our RAG system.\n",
    "\n",
    "### **1. Start Small — Use a Modest GPU First**\n",
    "Whenever possible, begin experiments on a **smaller GPU instance** such as:\n",
    "\n",
    "- `ml.g4dn.xlarge`  \n",
    "- `ml.g5.xlarge`\n",
    "\n",
    "These are affordable, flexible instances suitable for **initial debugging**, **retrieval testing**, and **small‑model prototyping**.  \n",
    "Only scale up once you confirm the pipeline is working correctly.\n",
    "\n",
    "### **2. Test the RAG System Locally When You Can**\n",
    "Your workflow should follow this pattern:\n",
    "\n",
    "1. Build a miniature version of the RAG pipeline **locally** using:\n",
    "   - a small embedding model  \n",
    "   - a tiny chunked dataset  \n",
    "   - local CPU/GPU resources  \n",
    "2. Validate retrieval  \n",
    "3. Validate parsing  \n",
    "4. Validate JSON formatting  \n",
    "5. Validate full `run_single_qa`\n",
    "\n",
    "Once the system works end‑to‑end, **then** run it on AWS with larger models and more documents.\n",
    "\n",
    "This minimizes cost and ensures that AWS compute time is used only for meaningful full‑scale runs.\n",
    "\n",
    "### **3. Remember to Shut Down Your AWS Instance**\n",
    "GPU notebook instances continue billing **even when idle**.  \n",
    "Always:\n",
    "\n",
    "- Save your work  \n",
    "- Shut down or stop the instance  \n",
    "- Verify the status in the AWS console  \n",
    "\n",
    "This habit prevents accidental ongoing GPU charges.\n",
    "\n",
    "### **4. What This Episode Focuses On**\n",
    "This notebook uses a **simple “always-on” GPU instance** to run the full RAG workflow.  \n",
    "This approach is:\n",
    "\n",
    "- Very easy to understand  \n",
    "- Ideal for learning retrieval and generation steps  \n",
    "- Great for experimentation and debugging  \n",
    "\n",
    "However, it is **not the most cost‑efficient method**.\n",
    "\n",
    "### **5. What Comes Next**\n",
    "In upcoming episodes we will introduce **more efficient and production‑aligned GPU strategies**, including:\n",
    "\n",
    "- On-demand GPU tasks  \n",
    "- Fully managed asynchronous jobs  \n",
    "- Serverless or streaming LLM inference  \n",
    "- SageMaker batch transform & RAG pipelines  \n",
    "- Embedding jobs that run only when needed  \n",
    "\n",
    "Those techniques bring you closer to best practice for scalable and budget‑friendly research computing.\n",
    "\n",
    "These reminders help frame the purpose of the current notebook:  \n",
    "**learn the RAG components clearly first, then optimize compute workflows in the next episodes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75251503",
   "metadata": {},
   "source": [
    "## Overview: WattBot RAG on a single notebook GPU\n",
    "\n",
    "In this episode we build a **minimal but realistic RAG pipeline** from the [WattBot 2025](https://www.kaggle.com/competitions/WattBot2025/overview) challenge that runs entirely from a single GPU-backed SageMaker notebook.\n",
    "\n",
    "In this episode we will:\n",
    "\n",
    "1. **Work directly with the WattBot data.**\n",
    "   - Use `train_QA.csv` and `metadata.csv` from the competition dataset.\n",
    "   - Download all referenced PDFs (our RAG corpus) using the URLs in `metadata.csv`.\n",
    "2. **Implement the core RAG steps explicitly in code.**\n",
    "   - Read PDFs, extract per-page text, and attach document metadata.\n",
    "   - Chunk text into overlapping segments suitable for embedding.\n",
    "   - Embed chunks with a sentence-transformer (`thenlper/gte-base`)\n",
    "   - Implement cosine-similarity search over the embedding matrix.\n",
    "3. **Connect to a local Qwen-style generator.**\n",
    "   - Use a quantized 7B model on a GPU-backed instance (e.g., `ml.g5.xlarge`).\n",
    "   - Construct WattBot-style answers that we can compare against `train_QA.csv`.\n",
    "4. **Add an explanation pass.**\n",
    "   - Use an LLM to look at the retrieved evidence, the answer, and citations.\n",
    "   - Generate a short explanation and label the **evidence type**: `[Quote]`, `[Table]`, `[Figure]`, or `[Mixed]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1884b7",
   "metadata": {},
   "source": [
    "## Notebook + dataset setup\n",
    "\n",
    "For this episode, we assume you are running on an AWS SageMaker notebook instance with a GPU, such as:\n",
    "\n",
    "- `ml.g5.xlarge` (recommended) or\n",
    "- `ml.g4dn.xlarge` (may work with smaller models / more aggressive quantization).\n",
    "\n",
    "See [Instances for ML](https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/instances-for-ML.html) for further guidance.\n",
    "\n",
    "\n",
    "### Step 1 – Download `data.zip` locally\n",
    "\n",
    "We’ll use the **WattBot 2025** dataset. Download the workshop data archive to your laptop or desktop:\n",
    "\n",
    "- Open this link in your browser: https://github.com/carpentries-incubator/ML_with_AWS_SageMaker/blob/main/data/data.zip\n",
    "- Save `data.zip` somewhere you can find it easily and unzip the folder contents\n",
    "\n",
    "This archive should include a `data/wattbot/` folder containing:\n",
    "\n",
    "- `metadata.csv` – index of all WattBot papers.\n",
    "- `train_QA.csv` – labeled questions + ground truth answers.\n",
    "\n",
    "### Step 2 – Create a WattBot S3 bucket\n",
    "\n",
    "In the AWS console:\n",
    "\n",
    "1. Go to **S3**.\n",
    "2. Create a new bucket named something like:  \n",
    "   `teamname-yourname-wattbot`\n",
    "3. Keep **Block all public access** enabled.\n",
    "4. (Optional, but recommended) Add tags so we can track costs:  \n",
    "   - `Project = your-team-name`  \n",
    "   - `Name = your-name`  \n",
    "   - `Purpose = RAG-demo`\n",
    "\n",
    "### Step 3 – Upload the WattBot files to S3\n",
    "\n",
    "1. In your new bucket, click **Upload**.\n",
    "2. Drag **the `data/wattbot/` folder** from `data.zip` into the upload dialog.\n",
    "3. Upload it so that your bucket contains paths like:\n",
    "\n",
    "   - `wattbot/metadata.csv`\n",
    "   - `wattbot/train_QA.csv`\n",
    "\n",
    "We’ll pull these files from S3 into the notebook in the next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e640bd0",
   "metadata": {},
   "source": [
    "###  Verify GPU and basic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1126203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 25 22:49:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   25C    P8             15W /  300W |       0MiB /  23028MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi || echo \"No GPU detected – please switch to a GPU-backed instance (e.g., ml.g5.xlarge) before running this notebook.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b44834d-cce7-4b5e-91fd-46b1d0ccb272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda available: True\n",
      "num gpus: 1\n"
     ]
    }
   ],
   "source": [
    "# also verify you've selected teh conda_pytorch_p310 kernel\n",
    "import torch\n",
    "print(\"torch cuda available:\", torch.cuda.is_available())\n",
    "print(\"num gpus:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35017931-9d55-4d46-bce3-c10568c58e09",
   "metadata": {},
   "source": [
    "## Import data from bucket into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed95e20-3218-4d3d-9738-189607c3468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-2\n",
      "Role: arn:aws:iam::183295408236:role/ml-sagemaker-bedrock-use\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Initialize SageMaker + AWS basics\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role()\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d49c6c-30fe-4a0a-8bc1-96dff6e49e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_object(bucket: str, key: str, local_path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    print(f\"Downloading s3://{bucket}/{key} -> {local_path}\")\n",
    "    s3_client.download_file(bucket, key, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be74a78-1145-477c-9724-c5c7165c4e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local data dir: ./data\n"
     ]
    }
   ],
   "source": [
    "# TODO: update this to your bucket name\n",
    "bucket_name = \"chris-rag\"  # <-- EDIT ME\n",
    "\n",
    "# Local working directory in the notebook instance\n",
    "local_data_dir = \"./data\"\n",
    "\n",
    "print(\"Local data dir:\", local_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1284c00e-0e9d-44e0-8565-1e1fdd472bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading s3://chris-rag/metadata.csv -> ./data/metadata.csv\n",
      "Downloading s3://chris-rag/train_QA.csv -> ./data/train_QA.csv\n"
     ]
    }
   ],
   "source": [
    "# Download metadata.csv and train_QA.csv\n",
    "metadata_key = \"metadata.csv\"\n",
    "train_qa_key = \"train_QA.csv\"\n",
    "\n",
    "metadata_path = os.path.join(local_data_dir, metadata_key)\n",
    "train_qa_path = os.path.join(local_data_dir, train_qa_key)\n",
    "\n",
    "download_s3_object(bucket_name, metadata_key, metadata_path)\n",
    "download_s3_object(bucket_name, train_qa_key, train_qa_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff33713",
   "metadata": {},
   "source": [
    "## Step 1 – Imports, paths, and safe CSV loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727a3211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import zipfile\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7acb9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_QA.csv columns: ['id', 'question', 'answer', 'answer_value', 'answer_unit', 'ref_id', 'ref_url', 'supporting_materials', 'explanation']\n",
      "metadata.csv columns: ['id', 'type', 'title', 'year', 'citation', 'url']\n",
      "\n",
      "Number of training QAs: 41\n",
      "Number of metadata rows: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The ML.ENERGY Benchmark</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2505.06371']</td>\n",
       "      <td>We present the ML.ENERGY Benchmark, a benchmar...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>4.3 tCO2e</td>\n",
       "      <td>4.3</td>\n",
       "      <td>tCO2e</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>\"Training GShard-600B used 24 MWh and produced...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>64.7 GB</td>\n",
       "      <td>64.7</td>\n",
       "      <td>GB</td>\n",
       "      <td>['chen2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2405.01814']</td>\n",
       "      <td>Table 3: Large language models used for evalua...</td>\n",
       "      <td>Table 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td>Unable to answer with confidence based on the ...</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>MWh</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['wu2021b','patterson2021']</td>\n",
       "      <td>['https://arxiv.org/abs/2108.06738','https://a...</td>\n",
       "      <td>Wu 2021, body text near Fig. 1: \"…between trad...</td>\n",
       "      <td>The &gt;40% statement is explicit in Wu. Patterso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q078</td>\n",
       "      <td>For every medium-length GPT-3 completion (prom...</td>\n",
       "      <td>0.02 to 0.1 bottles</td>\n",
       "      <td>[0.02,0.1]</td>\n",
       "      <td>500 mL bottles</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>\"Additionally, GPT-3 needs to -drink- (i.e., c...</td>\n",
       "      <td>The paper states that one 500ml bottle is cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q091</td>\n",
       "      <td>From a sample of 60 papers from top AI confere...</td>\n",
       "      <td>55%</td>\n",
       "      <td>55</td>\n",
       "      <td>percent</td>\n",
       "      <td>['schwartz2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1907.10597']</td>\n",
       "      <td>\"A large majority of the papers target accurac...</td>\n",
       "      <td>Requires calculation (75-20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q102</td>\n",
       "      <td>True or False: The AI Act makes energy consump...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>0</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Section 4.3 Transparency: 'Where the Act does ...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q105</td>\n",
       "      <td>What is the projected maximum batch size (in s...</td>\n",
       "      <td>28 samples per batch</td>\n",
       "      <td>28</td>\n",
       "      <td>samples</td>\n",
       "      <td>['xia2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2408.04693']</td>\n",
       "      <td>Figure 13</td>\n",
       "      <td>Figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q106</td>\n",
       "      <td>What was the approximate speedup in inference ...</td>\n",
       "      <td>2x</td>\n",
       "      <td>2</td>\n",
       "      <td>multiplier</td>\n",
       "      <td>['samsi2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2310.03003']</td>\n",
       "      <td>\"anywhere from a 2 times (7B) … increase … on ...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q124</td>\n",
       "      <td>What is the estimated total operational water ...</td>\n",
       "      <td>5.439 million liters</td>\n",
       "      <td>5439000</td>\n",
       "      <td>liters</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>Table 1</td>\n",
       "      <td>Table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q135</td>\n",
       "      <td>True or False: The authors propose that sustai...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Section 5.4 Sustainability Impact Assessments:...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q139</td>\n",
       "      <td>As of 2023, what was the water use effectivene...</td>\n",
       "      <td>0.18 L/kWh</td>\n",
       "      <td>0.18</td>\n",
       "      <td>L/kWh</td>\n",
       "      <td>['amazon2023']</td>\n",
       "      <td>['https://sustainability.aboutamazon.com/2023-...</td>\n",
       "      <td>0.18 Liters of water per kilowatt-hour (L/kWh)...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q146</td>\n",
       "      <td>True or False: Local inference was emphasized ...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['khan2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2504.06307']</td>\n",
       "      <td>Section III.B.1: 'local inference allows model...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q153</td>\n",
       "      <td>True or False: Tracking the runtime of a train...</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['strubell2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1906.02243']</td>\n",
       "      <td>\"Authors should report training time and sensi...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0   q003  What is the name of the benchmark suite presen...   \n",
       "1   q009  What were the net CO2e emissions from training...   \n",
       "2   q054  What is the model size in gigabytes (GB) for t...   \n",
       "3   q062  What was the total electricity consumption of ...   \n",
       "4   q075  True or False: Hyperscale data centers in 2020...   \n",
       "5   q078  For every medium-length GPT-3 completion (prom...   \n",
       "6   q091  From a sample of 60 papers from top AI confere...   \n",
       "7   q102  True or False: The AI Act makes energy consump...   \n",
       "8   q105  What is the projected maximum batch size (in s...   \n",
       "9   q106  What was the approximate speedup in inference ...   \n",
       "10  q124  What is the estimated total operational water ...   \n",
       "11  q135  True or False: The authors propose that sustai...   \n",
       "12  q139  As of 2023, what was the water use effectivene...   \n",
       "13  q146  True or False: Local inference was emphasized ...   \n",
       "14  q153  True or False: Tracking the runtime of a train...   \n",
       "\n",
       "                                               answer         answer_value  \\\n",
       "0                             The ML.ENERGY Benchmark  ML.ENERGY Benchmark   \n",
       "1                                           4.3 tCO2e                  4.3   \n",
       "2                                             64.7 GB                 64.7   \n",
       "3   Unable to answer with confidence based on the ...             is_blank   \n",
       "4                                                TRUE                    1   \n",
       "5                                 0.02 to 0.1 bottles           [0.02,0.1]   \n",
       "6                                                 55%                   55   \n",
       "7                                               FALSE                    0   \n",
       "8                                28 samples per batch                   28   \n",
       "9                                                  2x                    2   \n",
       "10                               5.439 million liters              5439000   \n",
       "11                                               TRUE                    1   \n",
       "12                                         0.18 L/kWh                 0.18   \n",
       "13                                               TRUE                    1   \n",
       "14                                               TRUE                    1   \n",
       "\n",
       "       answer_unit                       ref_id  \\\n",
       "0         is_blank                ['chung2025']   \n",
       "1            tCO2e            ['patterson2021']   \n",
       "2               GB                 ['chen2024']   \n",
       "3              MWh                     is_blank   \n",
       "4         is_blank  ['wu2021b','patterson2021']   \n",
       "5   500 mL bottles                  ['li2025b']   \n",
       "6          percent             ['schwartz2019']   \n",
       "7         is_blank                ['ebert2024']   \n",
       "8          samples                  ['xia2024']   \n",
       "9       multiplier                ['samsi2024']   \n",
       "10          liters                  ['li2025b']   \n",
       "11        is_blank                ['ebert2024']   \n",
       "12           L/kWh               ['amazon2023']   \n",
       "13        is_blank                 ['khan2025']   \n",
       "14        is_blank             ['strubell2019']   \n",
       "\n",
       "                                              ref_url  \\\n",
       "0                ['https://arxiv.org/pdf/2505.06371']   \n",
       "1                ['https://arxiv.org/pdf/2104.10350']   \n",
       "2                ['https://arxiv.org/pdf/2405.01814']   \n",
       "3                                            is_blank   \n",
       "4   ['https://arxiv.org/abs/2108.06738','https://a...   \n",
       "5                ['https://arxiv.org/pdf/2304.03271']   \n",
       "6                ['https://arxiv.org/pdf/1907.10597']   \n",
       "7                ['https://arxiv.org/pdf/2410.06681']   \n",
       "8                ['https://arxiv.org/pdf/2408.04693']   \n",
       "9                ['https://arxiv.org/pdf/2310.03003']   \n",
       "10               ['https://arxiv.org/pdf/2304.03271']   \n",
       "11               ['https://arxiv.org/pdf/2410.06681']   \n",
       "12  ['https://sustainability.aboutamazon.com/2023-...   \n",
       "13               ['https://arxiv.org/pdf/2504.06307']   \n",
       "14               ['https://arxiv.org/pdf/1906.02243']   \n",
       "\n",
       "                                 supporting_materials  \\\n",
       "0   We present the ML.ENERGY Benchmark, a benchmar...   \n",
       "1   \"Training GShard-600B used 24 MWh and produced...   \n",
       "2   Table 3: Large language models used for evalua...   \n",
       "3                                            is_blank   \n",
       "4   Wu 2021, body text near Fig. 1: \"…between trad...   \n",
       "5   \"Additionally, GPT-3 needs to -drink- (i.e., c...   \n",
       "6   \"A large majority of the papers target accurac...   \n",
       "7   Section 4.3 Transparency: 'Where the Act does ...   \n",
       "8                                           Figure 13   \n",
       "9   \"anywhere from a 2 times (7B) … increase … on ...   \n",
       "10                                            Table 1   \n",
       "11  Section 5.4 Sustainability Impact Assessments:...   \n",
       "12  0.18 Liters of water per kilowatt-hour (L/kWh)...   \n",
       "13  Section III.B.1: 'local inference allows model...   \n",
       "14  \"Authors should report training time and sensi...   \n",
       "\n",
       "                                          explanation  \n",
       "0                                               Quote  \n",
       "1                                               Quote  \n",
       "2                                             Table 3  \n",
       "3                                            is_blank  \n",
       "4   The >40% statement is explicit in Wu. Patterso...  \n",
       "5   The paper states that one 500ml bottle is cons...  \n",
       "6                        Requires calculation (75-20)  \n",
       "7                                               Quote  \n",
       "8                                              Figure  \n",
       "9                                               Quote  \n",
       "10                                              Table  \n",
       "11                                              Quote  \n",
       "12                                              Quote  \n",
       "13                                              Quote  \n",
       "14                                              Quote  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smart_read_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Try several encodings when reading a CSV file.\n",
    "\n",
    "    Some CSVs (especially those with special characters in author names or titles)\n",
    "    may not be valid UTF-8. This helper rotates through common encodings and raises\n",
    "    the last error only if all fail.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
    "    last_error = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "    if last_error is not None:\n",
    "        raise last_error\n",
    "    raise RuntimeError(f\"Unable to read CSV at {path}\")\n",
    "\n",
    "\n",
    "train_df = smart_read_csv(train_qa_path)\n",
    "metadata_df = smart_read_csv(metadata_path)\n",
    "\n",
    "print(\"train_QA.csv columns:\", train_df.columns.tolist())\n",
    "print(\"metadata.csv columns:\", metadata_df.columns.tolist())\n",
    "print(\"\\nNumber of training QAs:\", len(train_df))\n",
    "print(\"Number of metadata rows:\", len(metadata_df))\n",
    "\n",
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d942d8",
   "metadata": {},
   "source": [
    "## Step 2 – Download all PDFs from `metadata.csv`\n",
    "\n",
    "Next we will...\n",
    "\n",
    "1. Read the `url` column from `metadata.csv`.\n",
    "2. Download each PDF via HTTP and save it locally as `<id>.pdf` under `pdfs/`.\n",
    "3. Report any failures (e.g., missing or malformed URLs) at the end.\n",
    "4. Upload zipped version of corpus to S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c4edbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving PDFs to: ./data/pdfs\n",
      "\n",
      "Downloading amazon2023 from https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf ...\n",
      "Downloading chen2024 from https://arxiv.org/pdf/2405.01814 ...\n",
      "Downloading chung2025 from https://arxiv.org/pdf/2505.06371 ...\n",
      "Downloading cottier2024 from https://arxiv.org/pdf/2405.21015 ...\n",
      "Downloading dodge2022 from https://arxiv.org/pdf/2206.05229 ...\n",
      "Downloading ebert2024 from https://arxiv.org/pdf/2410.06681 ...\n",
      "Downloading erben2023 from https://arxiv.org/pdf/2306.03163 ...\n",
      "Downloading fernandez2025 from https://arxiv.org/pdf/2504.17674 ...\n",
      "Downloading griggs2024 from https://arxiv.org/pdf/2404.14527 ...\n",
      "Downloading han2024 from https://arxiv.org/pdf/2412.06288 ...\n",
      "Downloading jegham2025 from https://arxiv.org/pdf/2505.09598 ...\n",
      "Downloading khan2025 from https://arxiv.org/pdf/2504.06307 ...\n",
      "Downloading kim2025 from https://arxiv.org/pdf/2504.11816 ...\n",
      "Downloading li2025a from https://arxiv.org/pdf/2309.03852 ...\n",
      "Downloading li2025b from https://arxiv.org/pdf/2304.03271 ...\n",
      "Downloading luccioni2023 from https://arxiv.org/pdf/2302.08476 ...\n",
      "Downloading luccioni2024 from https://arxiv.org/pdf/2311.16863 ...\n",
      "Downloading luccioni2025a from https://arxiv.org/pdf/2501.16548 ...\n",
      "Downloading luccioni2025b from https://arxiv.org/pdf/2504.00797 ...\n",
      "Downloading luccioni2025c from https://arxiv.org/pdf/2506.15572 ...\n",
      "Downloading morrison2025 from https://arxiv.org/pdf/2503.05804 ...\n",
      "Downloading patterson2021 from https://arxiv.org/pdf/2104.10350 ...\n",
      "Downloading rubei2025 from https://arxiv.org/pdf/2501.05899 ...\n",
      "Downloading samsi2024 from https://arxiv.org/pdf/2310.03003 ...\n",
      "Downloading schwartz2019 from https://arxiv.org/pdf/1907.10597 ...\n",
      "Downloading shen2024 from https://arxiv.org/pdf/2404.07413 ...\n",
      "Downloading stone2022 from https://arxiv.org/pdf/2211.06318 ...\n",
      "Downloading strubell2019 from https://arxiv.org/pdf/1906.02243 ...\n",
      "Downloading wu2021a from https://arxiv.org/pdf/2111.00364 ...\n",
      "Downloading wu2021b from https://arxiv.org/pdf/2108.06738 ...\n",
      "Downloading xia2024 from https://arxiv.org/pdf/2408.04693 ...\n",
      "Downloading zschache2025 from https://arxiv.org/pdf/2508.14170 ...\n",
      "\n",
      "All PDFs downloaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF_DIR = os.path.join(local_data_dir, \"pdfs\")\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "def download_all_pdfs_from_urls(\n",
    "    metadata: pd.DataFrame,\n",
    "    local_pdf_dir: str,\n",
    "    url_col: str = \"url\",\n",
    "    id_col: str = \"id\",\n",
    "    timeout: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"Download all PDFs referenced in `metadata` using their URLs.\n",
    "\n",
    "    - Saves each file as `<id>.pdf` in `local_pdf_dir`.\n",
    "    - Strips whitespace from the URL (to avoid trailing spaces becoming `%20`).\n",
    "    - Skips rows with missing or non-HTTP URLs.\n",
    "    - Prints a short summary of any failures.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_pdf_dir, exist_ok=True)\n",
    "    errors: List[Tuple[str, str]] = []\n",
    "\n",
    "    print(f\"Saving PDFs to: {local_pdf_dir}\\n\")\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[id_col]).strip()\n",
    "\n",
    "        raw_url = row.get(url_col, None)\n",
    "        if not isinstance(raw_url, str):\n",
    "            errors.append((doc_id, \"URL is not a string\"))\n",
    "            continue\n",
    "\n",
    "        pdf_url = raw_url.strip()  # important: strip trailing whitespace\n",
    "        if not pdf_url.startswith(\"http\"):\n",
    "            errors.append((doc_id, f\"Invalid URL: {pdf_url!r}\"))\n",
    "            continue\n",
    "\n",
    "        local_path = os.path.join(local_pdf_dir, f\"{doc_id}.pdf\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {doc_id} from {pdf_url} ...\")\n",
    "            resp = requests.get(pdf_url, timeout=timeout, allow_redirects=True)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "            if \"pdf\" not in content_type.lower() and not pdf_url.lower().endswith(\".pdf\"):\n",
    "                print(f\"  Warning: Content-Type for {doc_id} does not look like PDF ({content_type})\")\n",
    "\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> FAILED for {doc_id}: {e}\")\n",
    "            errors.append((doc_id, str(e)))\n",
    "\n",
    "    if errors:\n",
    "        print(\"\\nSome PDFs could not be downloaded:\")\n",
    "        for doc_id, err in errors:\n",
    "            print(f\"  {doc_id}: {err}\")\n",
    "    else:\n",
    "        print(\"\\nAll PDFs downloaded successfully!\")\n",
    "\n",
    "\n",
    "download_all_pdfs_from_urls(\n",
    "    metadata_df,\n",
    "    PDF_DIR,\n",
    "    url_col=\"url\",\n",
    "    id_col=\"id\",\n",
    "    timeout=20,\n",
    ")\n",
    "\n",
    "len(os.listdir(PDF_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a250c",
   "metadata": {},
   "source": [
    "### Zip all PDFs and upload to S3\n",
    "\n",
    "Once we have all PDFs locally, it can be convenient and efficient to:\n",
    "\n",
    "1. Zip them into a single file (e.g., `wattbot_pdfs.zip`).  \n",
    "2. Upload that ZIP archive to an S3 bucket, such as `s3://<your-wattbot-bucket>/data/wattbot/wattbot_pdfs.zip`.\n",
    "\n",
    "We’ll include a short code example here, but feel free to skip this during the workshop if time is tight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e631bd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to ZIP: shen2024.pdf\n",
      "Added to ZIP: fernandez2025.pdf\n",
      "Added to ZIP: stone2022.pdf\n",
      "Added to ZIP: erben2023.pdf\n",
      "Added to ZIP: rubei2025.pdf\n",
      "Added to ZIP: luccioni2023.pdf\n",
      "Added to ZIP: xia2024.pdf\n",
      "Added to ZIP: wu2021b.pdf\n",
      "Added to ZIP: amazon2023.pdf\n",
      "Added to ZIP: cottier2024.pdf\n",
      "Added to ZIP: patterson2021.pdf\n",
      "Added to ZIP: wu2021a.pdf\n",
      "Added to ZIP: luccioni2025a.pdf\n",
      "Added to ZIP: khan2025.pdf\n",
      "Added to ZIP: morrison2025.pdf\n",
      "Added to ZIP: griggs2024.pdf\n",
      "Added to ZIP: kim2025.pdf\n",
      "Added to ZIP: chung2025.pdf\n",
      "Added to ZIP: dodge2022.pdf\n",
      "Added to ZIP: zschache2025.pdf\n",
      "Added to ZIP: chen2024.pdf\n",
      "Added to ZIP: jegham2025.pdf\n",
      "Added to ZIP: ebert2024.pdf\n",
      "Added to ZIP: luccioni2025b.pdf\n",
      "Added to ZIP: luccioni2024.pdf\n",
      "Added to ZIP: strubell2019.pdf\n",
      "Added to ZIP: li2025b.pdf\n",
      "Added to ZIP: samsi2024.pdf\n",
      "Added to ZIP: schwartz2019.pdf\n",
      "Added to ZIP: luccioni2025c.pdf\n",
      "Added to ZIP: li2025a.pdf\n",
      "Added to ZIP: han2024.pdf\n",
      "\n",
      "ZIP created: ./data/pdfs/corpus.zip\n",
      "Uploading to s3://chris-rag/corpus.zip ...\n",
      "Upload complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import boto3\n",
    "\n",
    "def zip_and_upload_pdfs(\n",
    "    local_pdf_dir: str,\n",
    "    bucket: str,\n",
    "    zip_name: str = \"corpus.zip\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Zips all PDFs in local_pdf_dir and uploads the ZIP file to:\n",
    "        s3://<bucket>/<prefix>/<zip_name>\n",
    "\n",
    "    Returns the full S3 URI of the uploaded zip file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure directory exists\n",
    "    if not os.path.exists(local_pdf_dir):\n",
    "        raise ValueError(f\"Directory not found: {local_pdf_dir}\")\n",
    "\n",
    "    # Path for the ZIP file\n",
    "    zip_path = os.path.join(local_pdf_dir, zip_name)\n",
    "\n",
    "    # Create ZIP archive\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for fname in os.listdir(local_pdf_dir):\n",
    "            if fname.lower().endswith(\".pdf\"):\n",
    "                fpath = os.path.join(local_pdf_dir, fname)\n",
    "                zipf.write(fpath, arcname=fname)\n",
    "                print(f\"Added to ZIP: {fname}\")\n",
    "\n",
    "    print(f\"\\nZIP created: {zip_path}\")\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_key = f\"{zip_name}\"\n",
    "\n",
    "    print(f\"Uploading to s3://{bucket}/{s3_key} ...\")\n",
    "    s3_client.upload_file(zip_path, bucket, s3_key)\n",
    "    print(\"Upload complete.\")\n",
    "\n",
    "    return f\"s3://{bucket}/{s3_key}\"\n",
    "\n",
    "\n",
    "zip_s3_uri = zip_and_upload_pdfs(\n",
    "    local_pdf_dir=PDF_DIR,\n",
    "    bucket=bucket_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071c5ae",
   "metadata": {},
   "source": [
    "## Step 3 – Turn PDFs into page-level “documents”\n",
    "\n",
    "Next, we convert each PDF into a list of **page-level records**. Each record stores:\n",
    "\n",
    "- `text`: page text (as extracted by `pypdf`).\n",
    "- `doc_id`: short ID from `metadata.csv` (e.g., `strubell2019`).\n",
    "- `title`: title of the document.\n",
    "- `url`: original PDF URL.\n",
    "- `page_num`: zero-based page index.\n",
    "- `page_label`: label used inside the PDF (often 1-based).\n",
    "\n",
    "Later, we will **chunk these pages** into smaller overlapping segments for embedding.\n",
    "\n",
    "### Why we page-chunk first\n",
    "\n",
    "We split the PDF into **pages before chunking** because pages give us a stable, easy-to-interpret unit.  \n",
    "This helps with:\n",
    "\n",
    "- **Keeping metadata** (doc ID, URL, page labels) tied to the text.  \n",
    "- **Debugging retrieval** — it’s much easier to understand what the model saw if we know which page(s) were used.  \n",
    "- **Cleaning text** before making smaller overlapping chunks.  \n",
    "- **Flexibility later** — once pages are structured, we can try different chunk sizes or strategies without re-extracting the PDF.\n",
    "\n",
    "In short: **pages first → then chunks** keeps the workflow cleaner and easier to reason about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa67a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 639 page-level records from 32 PDFs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Amazon \\nSustainability \\nReport\\n2023',\n",
       " 'doc_id': 'amazon2023',\n",
       " 'title': '2023 Amazon Sustainability Report',\n",
       " 'url': 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n",
       " 'page_num': 0,\n",
       " 'page_label': '1',\n",
       " 'total_pages': 98}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def pdfs_to_page_docs(metadata: pd.DataFrame, pdf_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load each PDF into a list of page-level dictionaries.\n",
    "\n",
    "    Each dict has keys: text, doc_id, title, url, page_num, page_label, total_pages.\n",
    "    \"\"\"\n",
    "    page_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[\"id\"]).strip()\n",
    "        title = str(row.get(\"title\", \"\")).strip()\n",
    "        url = str(row.get(\"url\", \"\")).strip()\n",
    "\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{doc_id}.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"Missing PDF for {doc_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {pdf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        total_pages = len(reader.pages)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text = page.extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract text from {doc_id} page {i}: {e}\")\n",
    "                text = \"\"\n",
    "\n",
    "            text = text.strip()\n",
    "            if not text:\n",
    "                # Still keep the page so we know it exists, but mark it as empty\n",
    "                text = \"[[EMPTY PAGE TEXT – see original PDF for tables/figures]]\"\n",
    "\n",
    "            page_docs.append(\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"page_num\": i,\n",
    "                    \"page_label\": str(i + 1),\n",
    "                    \"total_pages\": total_pages,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return page_docs\n",
    "\n",
    "\n",
    "page_docs = pdfs_to_page_docs(metadata_df, PDF_DIR)\n",
    "print(f\"Loaded {len(page_docs)} page-level records from {len(metadata_df)} PDFs.\")\n",
    "page_docs[0] if page_docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1396526f",
   "metadata": {},
   "source": [
    "## Step 4 – Simple, explicit text chunking\n",
    "\n",
    "RAG systems typically break documents into **chunks** so that:\n",
    "\n",
    "- Each chunk is long enough to carry meaningful context.\n",
    "- No chunk is so long that it blows up the embedding/LLM context window.\n",
    "\n",
    "For this workshop we will implement a **simple sliding-window chunker** that operates on characters:\n",
    "\n",
    "- `chunk_size_chars`: maximum characters per chunk (e.g., 1,000–1,500).\n",
    "- `chunk_overlap_chars`: overlap between consecutive chunks (e.g., 200).\n",
    "\n",
    "In our own work, you may wish to plug in more sophisticated *semantic chunking*  methods(e.g., splitting on headings, section titles, or sentence boundaries). For now, we'll keep the implementation explicit and easy to debug.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54f92d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pages: 639\n",
      "Chunked docs: 2874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Amazon \\nSustainability \\nReport\\n2023',\n",
       " 'doc_id': 'amazon2023',\n",
       " 'title': '2023 Amazon Sustainability Report',\n",
       " 'url': 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n",
       " 'page_num': 0,\n",
       " 'page_label': '1',\n",
       " 'total_pages': 98,\n",
       " 'chunk_idx_in_page': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_text_into_chunks(\n",
    "    text: str,\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[str]:\n",
    "    \"\"\"Split `text` into overlapping character-based chunks.\n",
    "\n",
    "    This is a simple baseline; more advanced versions might:\n",
    "    - split on sentence boundaries, or\n",
    "    - merge short paragraphs and respect section headings.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    while start < text_len:\n",
    "        end = min(start + chunk_size_chars, text_len)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == text_len:\n",
    "            break\n",
    "        # Move the window forward, keeping some overlap\n",
    "        start = end - chunk_overlap_chars\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def make_chunked_docs(\n",
    "    page_docs: List[Dict[str, Any]],\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Turn page-level records into smaller overlapping text chunks.\n",
    "\n",
    "    Each chunk keeps a pointer back to its document and page metadata.\n",
    "    \"\"\"\n",
    "    chunked: List[Dict[str, Any]] = []\n",
    "    for page in page_docs:\n",
    "        page_text = page[\"text\"]\n",
    "        chunks = split_text_into_chunks(\n",
    "            page_text,\n",
    "            chunk_size_chars=chunk_size_chars,\n",
    "            chunk_overlap_chars=chunk_overlap_chars,\n",
    "        )\n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            chunked.append(\n",
    "                {\n",
    "                    \"text\": chunk_text,\n",
    "                    \"doc_id\": page[\"doc_id\"],\n",
    "                    \"title\": page[\"title\"],\n",
    "                    \"url\": page[\"url\"],\n",
    "                    \"page_num\": page[\"page_num\"],\n",
    "                    \"page_label\": page[\"page_label\"],\n",
    "                    \"total_pages\": page[\"total_pages\"],\n",
    "                    \"chunk_idx_in_page\": idx,\n",
    "                }\n",
    "            )\n",
    "    return chunked\n",
    "\n",
    "\n",
    "chunked_docs = make_chunked_docs(page_docs)\n",
    "print(\"Raw pages:\", len(page_docs))\n",
    "print(\"Chunked docs:\", len(chunked_docs))\n",
    "chunked_docs[0] if chunked_docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90609c33",
   "metadata": {},
   "source": [
    "## Step 5 – Build an embedding matrix\n",
    "\n",
    "Now we embed each chunk into a vector using a **sentence-transformer** model. For WattBot, a strong and relatively efficient choice is:\n",
    "\n",
    "### `thenlper/gte-large` (Recommended baseline embedder)\n",
    "\n",
    "- Size / parameters:  ~335M parameters, roughly 1.3–1.4 GB in BF16/FP16 when loaded on GPU. Fits cleanly on T4 (16 GB), L4, A10G, A10, A100, and all g5.* instances.  Offers noticeably better retrieval quality than smaller 100M–150M models without requiring high-end GPU memory. Runs comfortably on g4dn.xlarge, g5.xlarge, or g5.2xlarge during workshops. Lets participants see meaningful improvements from chunking and retrieval methods without excessive compute cost.\n",
    "\n",
    "- Intended use:  General-purpose retrieval and semantic search across academic PDFs, sustainability reports, and mixed-domain long-form documents. Stronger semantic coherence than gte-base or MiniLM, but still lightweight enough for workshop hardware.\n",
    "\n",
    "- Throughput expectations:\n",
    "  - CPU only: workable for small corpora (<2k chunks) but slow for anything larger.  \n",
    "  - GPU (T4, L4, A10G, A100) with batch sizes around 64–128:  \n",
    "    - 20k–40k chunks/min on L4 or A10G  \n",
    "    - 10k–15k chunks/min on T4  \n",
    "    - 50k+ chunks/min on A100  \n",
    "      \n",
    "We will:\n",
    "\n",
    "1. Load the embedding model on GPU if available.\n",
    "2. Encode all chunks in batches.\n",
    "3. Store the resulting matrix as a `torch.Tensor` or `numpy.ndarray` along with the original `chunked_docs` list.\n",
    "\n",
    "Later, we’ll implement a small retrieval helper that does cosine-similarity search over this matrix—no additional indexing library required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a98a3724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available for embeddings: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# We'll use a stronger embedding model now that we have a GPU.\n",
    "# This model has ~335M parameters and benefits from GPU acceleration,\n",
    "# but is still reasonable to run on a single 24 GB GPU.\n",
    "embedding_model_id = \"thenlper/gte-large\"\n",
    "\n",
    "use_cuda_for_embeddings = torch.cuda.is_available()\n",
    "print(\"CUDA available for embeddings:\", use_cuda_for_embeddings)\n",
    "\n",
    "# Single shared embedder object that we can pass around.\n",
    "embedder = SentenceTransformer(\n",
    "    embedding_model_id,\n",
    "    device=\"cuda\" if use_cuda_for_embeddings else \"cpu\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be97cfd-8f33-4c11-8938-cad8f54c51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(embedder, docs, batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"Embed all chunk texts into a dense matrix of shape (N, D).\"\"\"\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    all_embeddings = []\n",
    "    start = time.time()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        emb = embedder.encode(\n",
    "            batch,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        all_embeddings.append(emb)\n",
    "    embeddings = np.vstack(all_embeddings) if all_embeddings else np.zeros((0, 768))\n",
    "    print(f\"Computed embeddings for {len(texts)} chunks in {time.time() - start:.1f}s\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c9ae32-c357-498d-9ab8-0b5c3862db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings for 2874 chunks in 51.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2874, 1024)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chunk_embeddings = embed_texts(embedder, chunked_docs)\n",
    "chunk_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab8bdf",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Build a simple retrieval step (cosine similarity)\n",
    "\n",
    "We are **not** using a heavy vector database in this first episode.\n",
    "\n",
    "Instead, we:\n",
    "\n",
    "1. Embed each chunk with `thenlper/gte-large` (done above).\n",
    "2. Embed each question.\n",
    "3. Compute cosine similarity between the question embedding and all chunk embeddings.\n",
    "4. Take the top–k most similar chunks as our retrieved context.\n",
    "\n",
    "This keeps the retrieval logic completely transparent for teaching, while still matching the *spirit* of\n",
    "production systems that use FAISS, Chroma, Weaviate, etc.\n",
    "\n",
    "#### When might FAISS or a vector database be worth exploring?\n",
    "\n",
    "For small–to–medium experiments (a few thousand to maybe tens of thousands of chunks), this \"plain NumPy + cosine\n",
    "similarity\" approach is usually enough. You might consider FAISS or a full vector DB when:\n",
    "\n",
    "- **Your corpus gets big**  \n",
    "  Once you’re in the hundreds of thousands to millions of chunks, brute-force similarity search can become slow\n",
    "  and memory-hungry. FAISS and friends provide *approximate nearest neighbor* search that scales much better.\n",
    "\n",
    "- **You need low-latency, repeated queries**  \n",
    "  If many users (or a web app) will hit your RAG system concurrently, you’ll want:\n",
    "  - fast indexing,\n",
    "  - efficient caching, and\n",
    "  - sub-second query latency.  \n",
    "  Vector DBs are designed for this use case.\n",
    "\n",
    "- **You need rich filtering or metadata search**  \n",
    "  Vector DBs often support:\n",
    "  - filtering by metadata (e.g., `paper = \"chung2025\"`, `year > 2021`),\n",
    "  - combining keyword + vector search (“hybrid search”),\n",
    "  - role-based access control and multi-tenant setups.\n",
    "\n",
    "- **You want to share an index across services**  \n",
    "  If multiple notebooks, microservices, or teams need to reuse the **same embedding index**, a shared FAISS index or\n",
    "  hosted vector DB is much easier to manage than passing around `.npy` files.\n",
    "\n",
    "- **You need GPU-accelerated or distributed search**  \n",
    "  FAISS can use GPUs and sharding to speed up search on very large embedding collections. This is overkill for our\n",
    "  teaching demo (and the Wattbot project in general), but very relevant for production-scale systems.\n",
    "\n",
    "In this episode we deliberately stick with a simple in-memory index so the retrieval step is easy to inspect and\n",
    "debug. In later episodes (or your own projects), you can **swap out the retrieval layer** for FAISS or a vector DB\n",
    "without changing the overall RAG architecture: the model still sees “top–k retrieved chunks” as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b7e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute cosine similarity between rows of a and rows of b.\"\"\"\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def retrieve_top_k(\n",
    "    query_embedding: np.ndarray,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    k: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return top-k most similar chunks for a query embedding.\"\"\"\n",
    "    if chunk_embeddings.shape[0] == 0:\n",
    "        return []\n",
    "\n",
    "    # query_embedding is 1D (D,)\n",
    "    sims = cosine_similarity_matrix(query_embedding.reshape(1, -1), chunk_embeddings)[0]\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for idx in top_idx:\n",
    "        doc = chunked_docs[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"page_num\": doc[\"page_num\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "            }\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbd5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample question: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "Top 3 retrieved chunks:\n",
      "- score=0.923 | doc_id=chung2025 | page=0 | snippet=The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization Jae-Won Chung Jeff J. Ma Ruofan Wu Jiachen Liu Oh Jun Kweon Yuxuan Xia Z...\n",
      "- score=0.921 | doc_id=chung2025 | page=9 | snippet=e generalizability and reproducibility of the results (Section 2.1). The ML.ENERGY Benchmark is the first inference energy benchmark for modern generative AI mo...\n",
      "- score=0.917 | doc_id=chung2025 | page=9 | snippet=ially, requires direct access to the system under test to physically install the power analyzer, which significantly limits who can run the benchmarks (Section ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Quick sanity check for `retrieve_top_k` on the first training question\n",
    "first_row = train_df.iloc[0]\n",
    "test_question = first_row[\"question\"]\n",
    "print(\"Sample question:\", test_question)\n",
    "\n",
    "test_q_emb = embedder.encode(\n",
    "    [test_question],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")[0]\n",
    "\n",
    "test_retrieved = retrieve_top_k(\n",
    "    query_embedding=test_q_emb,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    chunked_docs=chunked_docs,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "print(f\"Top {len(test_retrieved)} retrieved chunks:\")\n",
    "for r in test_retrieved:\n",
    "    snippet = r[\"text\"].replace(\"\\n\", \" \")\n",
    "    if len(snippet) > 160:\n",
    "        snippet = snippet[:160] + \"...\"\n",
    "    print(f\"- score={r['score']:.3f} | doc_id={r['doc_id']} | page={r['page_num']} | snippet={snippet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb1836",
   "metadata": {},
   "source": [
    "\n",
    "### 7. Load the Qwen model for answer generation\n",
    "\n",
    "For this episode we use **Qwen2.5-7B-Instruct** via the Hugging Face `transformers` library.\n",
    "\n",
    "- Parameter count: ~7 billion.\n",
    "- VRAM needs: ~14–16 GB in bfloat16 / 4-bit; fine for `ml.g5.xlarge` or a similar single-GPU instance.\n",
    "- Intended use here: short, grounded answers plus a normalized `answer_value`.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Call Qwen once to propose an answer and supporting evidence.\n",
    "2. Call Qwen a **second time** with a smaller prompt to generate a short explanation (<= 100 characters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c14fb37e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available for LLM: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581dc2600d394154a9271daf425e756e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen model and helper loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "qwen_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "use_cuda_for_llm = torch.cuda.is_available()\n",
    "print(\"CUDA available for LLM:\", use_cuda_for_llm)\n",
    "\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(qwen_model_id)\n",
    "\n",
    "if use_cuda_for_llm:\n",
    "    llm_dtype = torch.bfloat16\n",
    "    model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "        qwen_model_id,\n",
    "        dtype=llm_dtype,\n",
    "        device_map=None,  # load on a single GPU\n",
    "    ).to(\"cuda\")\n",
    "    generation_device = 0\n",
    "else:\n",
    "    llm_dtype = torch.float32\n",
    "    model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "        qwen_model_id,\n",
    "        dtype=llm_dtype,\n",
    "        device_map=None,\n",
    "    )\n",
    "    generation_device = -1  # CPU\n",
    "\n",
    "qwen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    device=generation_device,\n",
    "    max_new_tokens=384,\n",
    ")\n",
    "\n",
    "def call_qwen_chat(system_prompt: str, user_prompt: str, max_new_tokens: int = 384) -> str:\n",
    "    \"\"\"Use Qwen chat template and return only the newly generated text.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    prompt_text = tokenizer_qwen.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = qwen_pipe(\n",
    "        prompt_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    full = outputs[0][\"generated_text\"]\n",
    "    generated = full[len(prompt_text):]\n",
    "    return generated.strip()\n",
    "\n",
    "print(\"Generator model and helper loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "108f35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen test response: 2 + 2 is 4.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Quick sanity check for `call_qwen_chat`\n",
    "test_system_prompt = \"You are a concise assistant who answers simple questions clearly.\"\n",
    "test_user_prompt = \"What is 2 + 2? Answer in one short sentence.\"\n",
    "\n",
    "test_response = call_qwen_chat(\n",
    "    system_prompt=test_system_prompt,\n",
    "    user_prompt=test_user_prompt,\n",
    "    max_new_tokens=32,\n",
    ")\n",
    "print(f\"Generator ({qwen_model_id}) test response: {test_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff9ae6",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Build prompts for answers and explanations\n",
    "\n",
    "We keep the prompts **very explicit**:\n",
    "\n",
    "- The first call asks Qwen to return JSON with:\n",
    "  - `answer` (short text),\n",
    "  - `answer_value` (normalized scalar or category),\n",
    "  - `ref_id` (comma‑separated doc ids, e.g. `\"jegham2025\"`),\n",
    "  - `supporting_material` (short quote or paraphrase).\n",
    "\n",
    "- The second call asks Qwen to generate a **single sentence explanation** (<= 100 characters).\n",
    "  We will prepend an evidence type tag (e.g. `[text]` or `[text+table]`) in code rather than\n",
    "  asking the model to output it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e35c0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_context_for_prompt(retrieved_chunks):\n",
    "    \"\"\"Format retrieved chunks so the LLM can see where text came from.\"\"\"\n",
    "    blocks = []\n",
    "    for r in retrieved_chunks:\n",
    "        header = f\"[DOC {r['doc_id']} | page {r['page_num']} | score {r['score']:.3f}]\"\n",
    "        blocks.append(header + \"\\n\" + r[\"text\"])\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "explanation_system_prompt = (\n",
    "    \"You are helping annotate how an answer is supported by a research paper. \"\n",
    "    \"You will see a question, an answer, and the supporting text used. \"\n",
    "    \"Your job is to (1) choose the MAIN type of evidence and \"\n",
    "    \"(2) give a VERY short explanation (<= 100 characters). \"\n",
    "    \"Valid evidence types are: text, figure, table, text+figure, table+figure, etc. \"\n",
    "    \"Respond in the strict format: evidence_type: explanation\"\n",
    ")\n",
    "\n",
    "def build_explanation_prompt(question, answer, supporting_materials, ref_id_list):\n",
    "    ref_str = \", \".join(ref_id_list) if ref_id_list else \"unknown\"\n",
    "    return f\"\"\"Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Supporting materials:\n",
    "{supporting_materials}\n",
    "\n",
    "Cited document ids: {ref_str}\n",
    "\n",
    "Remember:\n",
    "- evidence_type in [text, figure, table, text+figure, table+figure, etc.]\n",
    "- explanation <= 100 characters\n",
    "- Format: evidence_type: explanation\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bec8d4",
   "metadata": {},
   "source": [
    "\n",
    "### 9. Run over the full WattBot training set\n",
    "\n",
    "Now we:\n",
    "\n",
    "1. Iterate over **all** questions in `train_QA.csv`.\n",
    "2. Retrieve the top-\\(k\\) chunks for each question.\n",
    "3. Ask Qwen for an answer proposal (JSON).\n",
    "4. Derive:\n",
    "   - `answer` and `answer_value` from the JSON,\n",
    "   - `answer_unit` **copied directly from the ground truth** (never guessed),\n",
    "   - `ref_id` from the JSON,\n",
    "   - `ref_url` by mapping `ref_id` to `metadata.csv`,\n",
    "   - `supporting_material` from the JSON,\n",
    "   - `evidence_type` from the supporting text,\n",
    "   - `explanation` via a second Qwen call, prefixed with `[evidence_type]`.\n",
    "5. Save `wattbot_solutions.csv` in the project folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe5dde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from decimal import Decimal\n",
    "\n",
    "def normalize_answer_value(raw_answer_value, answer_text, answer_unit, is_blank):\n",
    "    \"\"\"\n",
    "    Normalize answer_value into the conventions used by train_QA:\n",
    "      - 'is_blank' for unanswerable questions\n",
    "      - plain numeric strings without units, commas, or scientific notation\n",
    "      - booleans as 1/0\n",
    "      - categorical strings (e.g., 'ML.ENERGY Benchmark') unchanged\n",
    "      - ranges like '[0.02,0.1]' preserved as-is\n",
    "    \"\"\"\n",
    "    s = str(raw_answer_value).strip()\n",
    "    if is_blank:\n",
    "        return \"is_blank\"\n",
    "    if not s or s.lower() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # Preserve ranges like [0.02,0.1]\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        return s\n",
    "\n",
    "    lower = s.lower()\n",
    "\n",
    "    # Booleans -> 1/0\n",
    "    if lower in {\"true\", \"false\"}:\n",
    "        return \"1\" if lower == \"true\" else \"0\"\n",
    "\n",
    "    # Pure categorical (no digits) -> leave as-is\n",
    "    if not any(ch.isdigit() for ch in s):\n",
    "        return s\n",
    "\n",
    "    # Try to extract the first numeric token from either the raw string or the answer text\n",
    "    txt_candidates = [s, str(answer_text)]\n",
    "    match = None\n",
    "    for txt in txt_candidates:\n",
    "        if not txt:\n",
    "            continue\n",
    "        match = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", str(txt).replace(\",\", \"\"))\n",
    "        if match:\n",
    "            break\n",
    "\n",
    "    if not match:\n",
    "        # Fallback: strip obvious formatting characters\n",
    "        cleaned = s.replace(\",\", \"\").replace(\"%\", \"\").strip()\n",
    "        return cleaned or \"is_blank\"\n",
    "\n",
    "    num_str = match.group(0)\n",
    "\n",
    "    # Format without scientific notation, trim trailing zeros\n",
    "    try:\n",
    "        d = Decimal(num_str)\n",
    "        normalized = format(d.normalize(), \"f\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            f = float(num_str)\n",
    "            normalized = (\"%.15f\" % f).rstrip(\"0\").rstrip(\".\")\n",
    "        except Exception:\n",
    "            normalized = num_str\n",
    "\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4caf67",
   "metadata": {},
   "source": [
    "### Running the full RAG pipeline for one question\n",
    "\n",
    "At this point we have all the building blocks we need:\n",
    "\n",
    "- an **embedder** to turn questions into vectors  \n",
    "- a **retriever** (`retrieve_top_k`) to grab the most relevant text chunks  \n",
    "- a **chat helper** (`call_qwen_chat`) to talk to Qwen and get JSON back  \n",
    "- a small post-processing helper (`normalize_answer_value`) to clean numbers\n",
    "\n",
    "In the next few cells we tie these pieces together. We keep the code split into\n",
    "small helper functions so learners can follow each step:\n",
    "\n",
    "1. Retrieve context for a question.  \n",
    "2. Ask the LLM for an answer, references, and a quote.  \n",
    "3. Clean and normalize the structured fields (answer_value, ref_id, is_blank).  \n",
    "4. Ask a second LLM call for a short explanation and evidence type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18f94e",
   "metadata": {},
   "source": [
    "### 🔍 Retrieving Relevant Context\n",
    "This function embeds the question and fetches the top‐K most relevant text chunks. It’s the first step of the RAG pipeline and determines what evidence the LLM can see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e86d5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lookup from document id -> URL using metadata\n",
    "docid_to_url = {\n",
    "    str(row[\"id\"]).strip(): row[\"url\"]\n",
    "    for _, row in metadata_df.iterrows()\n",
    "    if isinstance(row.get(\"url\", None), str)\n",
    "}\n",
    "\n",
    "def retrieve_context_for_question(question, embedder, chunk_embeddings, chunked_docs, top_k: int = 8):\n",
    "    \"\"\"Embed the question and retrieve the top-k most similar chunks.\"\"\"\n",
    "    q_emb = embedder.encode(\n",
    "        [question],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )[0]\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    context = format_context_for_prompt(retrieved)\n",
    "    return retrieved, context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5cc9e",
   "metadata": {},
   "source": [
    "### First LLM Step: Producing an Answer\n",
    "Here we prompt the model to:\n",
    "- Decide if the question is answerable\n",
    "- Extract a numeric/categorical answer\n",
    "- Identify supporting evidence\n",
    "- Select relevant document IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc49a37a-12b4-43b0-84ad-3c9f92f79694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_phase_for_question(qid, question, answer_unit, context):\n",
    "    \"\"\"\n",
    "    First LLM call: ask Qwen for an answer, answer_value, is_blank, ref_ids,\n",
    "    and a short supporting quote. Then normalize these fields.\n",
    "    \"\"\"\n",
    "    # System prompt: what role Qwen should play\n",
    "    system_prompt_answer = (\n",
    "        \"You answer questions about AI energy, carbon, and water from academic papers.\\n\"\n",
    "        \"You are given:\\n\"\n",
    "        \"- a question\\n\"\n",
    "        \"- retrieved text chunks from the relevant paper(s)\\n\\n\"\n",
    "        \"You must:\\n\"\n",
    "        \"1. Decide if the question can be answered from the provided context.\\n\"\n",
    "        \"2. If answerable, extract a concise numeric or short-text answer_value.\\n\"\n",
    "        \"3. Use the provided answer_unit EXACTLY as given (do NOT invent units).\\n\"\n",
    "        \"4. Select one or more document ids as ref_id from the supplied chunks.\\n\"\n",
    "        \"5. Copy a short supporting quote (<= 300 chars) into supporting_materials.\\n\"\n",
    "        \"6. If the context is insufficient, mark is_blank = true and set all\\n\"\n",
    "        \"   other fields to 'is_blank' except answer_unit (keep it as provided).\\n\"\n",
    "        \"Return a JSON object with fields:\\n\"\n",
    "        \"  answer (string)\\n\"\n",
    "        \"  answer_value (string)\\n\"\n",
    "        \"  is_blank (true or false)\\n\"\n",
    "        \"  ref_id (list of doc_id strings)\\n\"\n",
    "        \"  supporting_materials (string)\\n\"\n",
    "    )\n",
    "\n",
    "    context_block = context if context.strip() else \"[NO CONTEXT FOUND]\"\n",
    "\n",
    "    # User prompt: question + unit hint + retrieved context\n",
    "    user_prompt_answer = f\"\"\"Question: {question}\n",
    "Expected answer unit: {answer_unit}\n",
    "\n",
    "Retrieved context:\n",
    "{context_block}\n",
    "\n",
    "Return JSON ONLY with keys:\n",
    "  answer (string)\n",
    "  answer_value (string)\n",
    "  is_blank (true or false)\n",
    "  ref_id (list of doc_id strings)\n",
    "  supporting_materials (string)\n",
    "\"\"\"\n",
    "\n",
    "    raw_answer = call_qwen_chat(system_prompt_answer, user_prompt_answer, max_new_tokens=384)\n",
    "\n",
    "    # Try to parse JSON from the model output\n",
    "    parsed = {\n",
    "        \"answer\": \"\",\n",
    "        \"answer_value\": \"is_blank\",\n",
    "        \"is_blank\": True,\n",
    "        \"ref_id\": [],\n",
    "        \"supporting_materials\": \"is_blank\",\n",
    "    }\n",
    "    try:\n",
    "        first_brace = raw_answer.find(\"{\")\n",
    "        last_brace = raw_answer.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            json_str = raw_answer[first_brace : last_brace + 1]\n",
    "        else:\n",
    "            json_str = raw_answer\n",
    "        candidate = json.loads(json_str)\n",
    "        if isinstance(candidate, dict):\n",
    "            parsed.update(candidate)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parse error for question {qid}: {e}\")\n",
    "        # fall back to defaults in `parsed`\n",
    "\n",
    "    # Normalize parsed fields\n",
    "    is_blank = bool(parsed.get(\"is_blank\", False))\n",
    "    ref_ids = parsed.get(\"ref_id\") or []\n",
    "    if isinstance(ref_ids, str):\n",
    "        ref_ids = [ref_ids]\n",
    "    ref_ids = [str(r).strip() for r in ref_ids if str(r).strip()]\n",
    "\n",
    "    answer = str(parsed.get(\"answer\", \"\")).strip()\n",
    "    answer_value = str(parsed.get(\"answer_value\", \"\")).strip() or \"is_blank\"\n",
    "    answer_value = normalize_answer_value(\n",
    "        raw_answer_value=answer_value,\n",
    "        answer_text=answer,\n",
    "        answer_unit=answer_unit,\n",
    "        is_blank=is_blank,\n",
    "    )\n",
    "    supporting_materials = str(parsed.get(\"supporting_materials\", \"\")).strip()\n",
    "\n",
    "    # If context is empty or model says blank, force is_blank behaviour\n",
    "    if not context.strip() or is_blank:\n",
    "        is_blank = True\n",
    "        answer = \"\"\n",
    "        answer_value = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        supporting_materials = \"is_blank\"\n",
    "\n",
    "    # String formatting for ref_id / ref_url to match training style\n",
    "    if not ref_ids:\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "    else:\n",
    "        ref_id_str = str(ref_ids)\n",
    "\n",
    "        # Resolve ref_url via metadata\n",
    "        ref_url = \"is_blank\"\n",
    "        for rid in ref_ids:\n",
    "            if rid in docid_to_url:\n",
    "                ref_url = docid_to_url[rid]\n",
    "                break\n",
    "        if not ref_url:\n",
    "            ref_url = \"is_blank\"\n",
    "        ref_url_str = str([ref_url])\n",
    "\n",
    "    return answer, answer_value, is_blank, ref_ids, supporting_materials, ref_id_str, ref_url_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4dd8c",
   "metadata": {},
   "source": [
    "### Second LLM Step: Explanation and Evidence Type\n",
    "Now that we have an answer, we produce a short explanation and classify the evidence type. This step matches WattBot’s expected metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b0b3109-544c-46e6-8de1-96b879d10974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_phase_for_question(question, answer, supporting_materials, ref_ids, is_blank):\n",
    "    \"\"\"\n",
    "    Second LLM call: ask Qwen to label an evidence_type and provide a short\n",
    "    explanation tying the answer back to the supporting materials.\n",
    "    \"\"\"\n",
    "    if is_blank:\n",
    "        # For unanswerable questions we just propagate a sentinel.\n",
    "        evidence_type = \"other\"\n",
    "        explanation = \"is_blank\"\n",
    "        return evidence_type, explanation\n",
    "\n",
    "    expl_user_prompt = build_explanation_prompt(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        supporting_materials=supporting_materials,\n",
    "        ref_id_list=ref_ids,\n",
    "    )\n",
    "    raw_expl = call_qwen_chat(\n",
    "        explanation_system_prompt,\n",
    "        expl_user_prompt,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    # Take the first non-empty line (we expect something like \"text: short reason\")\n",
    "    first_line = \"\"\n",
    "    for line in raw_expl.splitlines():\n",
    "        if line.strip():\n",
    "            first_line = line.strip()\n",
    "            break\n",
    "\n",
    "    if \":\" in first_line:\n",
    "        etype, expl = first_line.split(\":\", 1)\n",
    "        evidence_type = etype.strip().lower() or \"other\"\n",
    "        explanation = expl.strip()\n",
    "    else:\n",
    "        evidence_type = \"other\"\n",
    "        explanation = first_line.strip() or \"short justification\"\n",
    "\n",
    "    # Keep explanations short for the CSV\n",
    "    if len(explanation) > 100:\n",
    "        explanation = explanation[:100]\n",
    "\n",
    "    return evidence_type, explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818f41c",
   "metadata": {},
   "source": [
    "###  Orchestration: `run_single_qa`\n",
    "This high‐level function ties together retrieval, answering, normalization, and explanation into one full pass over a single question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15f91e",
   "metadata": {},
   "source": [
    "\n",
    "### Handling unanswerable questions\n",
    "\n",
    "Some WattBot questions truly **cannot** be answered from the retrieved papers.  \n",
    "We use a simple hybrid rule to detect these cases:\n",
    "\n",
    "- We look at the **top retrieval score** (cosine similarity).  \n",
    "- We also use the LLM's own `is_blank` flag from the first JSON response.  \n",
    "\n",
    "If **either** of these says the evidence is too weak, we mark the question as unanswerable\n",
    "and set all relevant fields (`answer_value`, `ref_id`, `supporting_materials`) to `is_blank`.\n",
    "\n",
    "The `THRESHOLD` inside `run_single_qa` controls how strict this behaviour is:\n",
    "\n",
    "- lower values → fewer questions marked unanswerable  \n",
    "- higher values → more questions marked unanswerable  \n",
    "\n",
    "You can change `THRESHOLD` and then re-run the notebook and `Score.py` to see\n",
    "how this trade-off affects your final WattBot score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3588b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_single_qa(\n",
    "    row,\n",
    "    embedder,\n",
    "    chunk_embeddings,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"Run retrieval + Qwen for a single training QA row.\n",
    "\n",
    "    This is the high-level orchestration function that calls three helpers:\n",
    "\n",
    "    1. retrieve_context_for_question  -> get relevant text chunks\n",
    "    2. answer_phase_for_question      -> generate answer from context, returning citations and supporting materials\n",
    "    3. explanation_phase_for_question -> evidence type + short explanation\n",
    "    \"\"\"\n",
    "\n",
    "    # Confidence threshold for retrieval.\n",
    "    # If the top similarity score is below this value, we treat the question\n",
    "    # as unanswerable, even if the LLM tried to produce an answer.\n",
    "    THRESHOLD = 0.25\n",
    "\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "    answer_unit = row.get(\"answer_unit\", \"\")\n",
    "\n",
    "    # 1. Retrieval step\n",
    "    retrieved, context = retrieve_context_for_question(\n",
    "        question=question,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n",
    "\n",
    "    # 2. Answer + refs + supporting materials (LLM's view)\n",
    "    (\n",
    "        answer,\n",
    "        answer_value,\n",
    "        is_blank_llm,\n",
    "        ref_ids,\n",
    "        supporting_materials,\n",
    "        ref_id_str,\n",
    "        ref_url_str,\n",
    "    ) = answer_phase_for_question(\n",
    "        qid=qid,\n",
    "        question=question,\n",
    "        answer_unit=answer_unit,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    # Hybrid is_blank decision:\n",
    "    # - if retrieval is weak (top_score < THRESHOLD)\n",
    "    # - OR the LLM marks is_blank = true\n",
    "    # then we treat the question as unanswerable.\n",
    "    is_blank = bool(is_blank_llm) or (top_score < THRESHOLD)\n",
    "\n",
    "    if is_blank:\n",
    "        answer = \"\"\n",
    "        answer_value = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "        supporting_materials = \"is_blank\"\n",
    "\n",
    "    # Always copy answer_unit from train_QA.csv (do NOT let the LLM invent it)\n",
    "    answer_unit = row.get(\"answer_unit\", \"\")\n",
    "\n",
    "    # 3. Explanation + evidence_type\n",
    "    evidence_type, explanation = explanation_phase_for_question(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        supporting_materials=supporting_materials,\n",
    "        ref_ids=ref_ids,\n",
    "        is_blank=is_blank,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_value\": answer_value,\n",
    "        \"answer_unit\": answer_unit,\n",
    "        \"is_blank\": \"true\" if is_blank else \"false\",\n",
    "        \"ref_id\": ref_id_str,\n",
    "        \"ref_url\": ref_url_str,\n",
    "        \"supporting_materials\": supporting_materials,\n",
    "        \"evidence_type\": evidence_type,\n",
    "        \"explanation\": explanation,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f9ef6f0-f584-4c03-82cf-2847b2d3697a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################\n",
      "QUESTION: What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?\n",
      "ANSWER: ML.ENERGY Benchmark\n",
      "ref_ids: ['chung2025']\n",
      "EXPLANATION: Introduction of ML.ENERGY Benchmark\n",
      "####################################################\n",
      "QUESTION: What were the net CO2e emissions from training the GShard-600B model?\n",
      "ANSWER: 4.3\n",
      "ref_ids: ['patterson2021']\n",
      "EXPLANATION: Table 4 shows 4.3 tCO2e emissions.\n",
      "####################################################\n",
      "QUESTION: What is the model size in gigabytes (GB) for the LLaMA-33B model?\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.\n",
      "ANSWER: True\n",
      "ref_ids: ['wu2021b']\n",
      "EXPLANATION: Figure 1 shows >40% efficiency for hyperscale.\n",
      "####################################################\n",
      "QUESTION: For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?\n",
      "ANSWER: 500ml\n",
      "ref_ids: ['li2025b']\n",
      "EXPLANATION: GPT-3 needs 500ml for 10-50 responses\n",
      "####################################################\n",
      "QUESTION: From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?\n",
      "ANSWER: Difference in percentage of CVPR papers targeting accuracy vs efficiency\n",
      "ref_ids: ['schwartz2019']\n",
      "EXPLANATION: Figure 2 shows the distribution.\n",
      "####################################################\n",
      "QUESTION: True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.\n",
      "ANSWER: False\n",
      "ref_ids: ['ebert2024']\n",
      "EXPLANATION: Restricts access to energy consumption data\n",
      "####################################################\n",
      "QUESTION: What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?\n",
      "ANSWER: 28\n",
      "ref_ids: ['xia2024']\n",
      "EXPLANATION: Predicted max batch size for 100GB GPU is 28.\n",
      "####################################################\n",
      "QUESTION: What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?\n",
      "JSON parse error for question q106: Unterminated string starting at: line 8 column 27 (char 130)\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?\n",
      "ANSWER: The estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers is 4.731 million liters.\n",
      "ref_ids: ['li2025b']\n",
      "EXPLANATION: Water consumption details table shows 4.731 million liters for U.S.\n",
      "####################################################\n",
      "QUESTION: True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.\n",
      "ANSWER: True\n",
      "ref_ids: ['ebert2024']\n",
      "EXPLANATION: Emphasizes SIAs for all AI systems\n",
      "####################################################\n",
      "QUESTION: As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?\n",
      "ANSWER: 0.18\n",
      "ref_ids: ['amazon2023', 'amazon2023']\n",
      "EXPLANATION: 5% improvement from 2022 and 28% from 2021\n",
      "####################################################\n",
      "QUESTION: True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.\n",
      "ANSWER: True\n",
      "ref_ids: ['khan2025']\n",
      "EXPLANATION: Tackles energy efficiency through local inference.\n",
      "####################################################\n",
      "QUESTION: True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.\n",
      "ANSWER: True\n",
      "ref_ids: ['strubell2019', 'cottier2024']\n",
      "EXPLANATION: Directly states the importance\n",
      "####################################################\n",
      "QUESTION: For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?\n",
      "ANSWER: up to a 13.2%\n",
      "ref_ids: ['chen2024']\n",
      "EXPLANATION: Illustrates latency reduction for LLaMA-65B\n",
      "####################################################\n",
      "QUESTION: How much does an elephant weigh?\n",
      "JSON parse error for question q164: Unterminated string starting at: line 49 column 5 (char 869)\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?\n",
      "ANSWER: GShard-600B\n",
      "ref_ids: ['patterson2021']\n",
      "EXPLANATION: Supporting details provided in the text.\n",
      "####################################################\n",
      "QUESTION: How many days of CO₂ emissions from an average American life are equivalent to training BERT base?\n",
      "ANSWER: 36,156\n",
      "ref_ids: ['strubell2019']\n",
      "EXPLANATION: American life, avg, 1 year\n",
      "####################################################\n",
      "QUESTION: True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.\n",
      "ANSWER: True\n",
      "ref_ids: ['erben2023|page 11', 'erben2023|page 9']\n",
      "EXPLANATION: Cost and throughput comparison explained\n",
      "####################################################\n",
      "QUESTION: True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.\n",
      "ANSWER: False\n",
      "ref_ids: ['luccioni2025b']\n",
      "EXPLANATION: Did not mention AI's greenhouse gas emissions nor energy usage.\n",
      "####################################################\n",
      "QUESTION: True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.\n",
      "ANSWER: True\n",
      "ref_ids: ['ebert2024']\n",
      "EXPLANATION: Supporting text mentions the 2023 Energy Efficiency Act but not the specific 100% renewable energy r\n",
      "####################################################\n",
      "QUESTION: Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?\n",
      "ANSWER: 10\n",
      "ref_ids: ['schwartz2019']\n",
      "EXPLANATION: Figure 2 shows 10% of ACL papers target efficiency.\n",
      "####################################################\n",
      "QUESTION: According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?\n",
      "ANSWER: up to 90%\n",
      "ref_ids: ['jegham2025']\n",
      "EXPLANATION: Recent estimates suggest inference can account for up to 90%...\n",
      "####################################################\n",
      "QUESTION: True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.\n",
      "ANSWER: False\n",
      "ref_ids: ['ebert2024']\n",
      "EXPLANATION: Explains the gap in coverage\n",
      "####################################################\n",
      "QUESTION: True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.\n",
      "ANSWER: False\n",
      "ref_ids: ['ebert2024']\n",
      "EXPLANATION: Only covers development phase, not inference phase\n",
      "####################################################\n",
      "QUESTION: True or False: New AI data centers often rely on air cooling due to high server power densities.\n",
      "ANSWER: False\n",
      "ref_ids: ['luccioni2025a', 'li2025b']\n",
      "EXPLANATION: Discusses water-intensive cooling methods, not air cooling.\n",
      "####################################################\n",
      "QUESTION: By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?\n",
      "ANSWER: By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?\n",
      "ref_ids: ['wu2021a']\n",
      "EXPLANATION: No specific factor mentioned, only improvement stated.\n",
      "####################################################\n",
      "QUESTION: What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?\n",
      "ANSWER: The estimated CO2 emissions in pounds from training a BERT base model for 79 hours using 64 V100 GPUs is 1438 lbs.\n",
      "ref_ids: ['strubell2019']\n",
      "EXPLANATION: CO2 emissions in Table 3\n",
      "####################################################\n",
      "QUESTION: According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?\n",
      "ANSWER: ML inference reportedly accounts for 80 to 90% of total ML cloud computing demand\n",
      "ref_ids: ['patterson2021', 'luccioni2024']\n",
      "EXPLANATION: AWS estimate shows 80-90% for inference.\n",
      "####################################################\n",
      "QUESTION: How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?\n",
      "ANSWER: 65 years\n",
      "ref_ids: ['morrison2025']\n",
      "EXPLANATION: Average U.S. household electricity use equivalent\n",
      "####################################################\n",
      "QUESTION: True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.\n",
      "ANSWER: True\n",
      "ref_ids: ['erben2023|page_7|score_0.937', 'erben2023|page_10|score_0.933', 'erben2023|page_7|score_0.922', 'erben2023|page_7|score_0.914', 'erben2023|page_7|score_0.911', 'erben2023|page_7|score_0.910', 'erben2023|page_10|score_0.908', 'erben2023|page_7|score_0.904']\n",
      "EXPLANATION: Egress costs exceeded spot/on-demand GPU costs in multi-zone training.\n",
      "####################################################\n",
      "QUESTION: Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.\n",
      "ANSWER: \n",
      "ref_ids: is_blank\n",
      "EXPLANATION: is_blank\n",
      "####################################################\n",
      "QUESTION: What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?\n",
      "ANSWER: water consumption\n",
      "ref_ids: ['li2025b']\n",
      "EXPLANATION: Explanation of water consumption definition\n",
      "####################################################\n",
      "QUESTION: What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?\n",
      "ANSWER: The observed range of inference energy per second for LLaMA-65B across GPU shard configurations is on the order of 300 Watts to 1 Kilowatt.\n",
      "ref_ids: ['samsi2024']\n",
      "EXPLANATION: Describes the energy range for LLaMA-65B inference.\n",
      "####################################################\n",
      "QUESTION: When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?\n",
      "ANSWER: multiplier\n",
      "ref_ids: ['luccioni2024']\n",
      "EXPLANATION: Not enough info in figure\n",
      "####################################################\n",
      "QUESTION: By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?\n",
      "ANSWER: reductions\n",
      "ref_ids: ['khan2025']\n",
      "EXPLANATION: Up to 45% reductions in energy consumption.\n",
      "####################################################\n",
      "QUESTION: How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?\n",
      "ANSWER: 40\n",
      "ref_ids: ['chung2025|page 22']\n",
      "EXPLANATION: Tables 2+3 list 40 models across 6 tasks.\n",
      "####################################################\n",
      "QUESTION: In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?\n",
      "ANSWER: $2.5 million\n",
      "ref_ids: ['han2024']\n",
      "EXPLANATION: Cost in Iowa is higher due to factors.\n",
      "Saved solutions for 41 questions to: ./data/train_solutions_qwen.csv\n",
      "Number of questions with errors (filled as blank): 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>is_blank</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>evidence_type</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2505.06371']</td>\n",
       "      <td>The ML.ENERGY Benchmark is the first inference...</td>\n",
       "      <td>text</td>\n",
       "      <td>Introduction of ML.ENERGY Benchmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>tCO2e</td>\n",
       "      <td>false</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>GShard-600B’s emissions (Table 4) are 4.3 tCO2...</td>\n",
       "      <td>table</td>\n",
       "      <td>Table 4 shows 4.3 tCO2e emissions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>GB</td>\n",
       "      <td>true</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>other</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q062</td>\n",
       "      <td>What was the total electricity consumption of ...</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>MWh</td>\n",
       "      <td>true</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>other</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q075</td>\n",
       "      <td>True or False: Hyperscale data centers in 2020...</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['wu2021b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2108.06738']</td>\n",
       "      <td>Furthermore, between traditional and highly op...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Figure 1 shows &gt;40% efficiency for hyperscale.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q078</td>\n",
       "      <td>For every medium-length GPT-3 completion (prom...</td>\n",
       "      <td>500ml</td>\n",
       "      <td>500</td>\n",
       "      <td>500 mL bottles</td>\n",
       "      <td>false</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>GPT-3 needs to “drink” (i.e., consume) a500ml ...</td>\n",
       "      <td>text</td>\n",
       "      <td>GPT-3 needs 500ml for 10-50 responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q091</td>\n",
       "      <td>From a sample of 60 papers from top AI confere...</td>\n",
       "      <td>Difference in percentage of CVPR papers target...</td>\n",
       "      <td>55</td>\n",
       "      <td>percent</td>\n",
       "      <td>false</td>\n",
       "      <td>['schwartz2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1907.10597']</td>\n",
       "      <td>As shown in Figure 2, in all conferences we co...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Figure 2 shows the distribution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q102</td>\n",
       "      <td>True or False: The AI Act makes energy consump...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Where the Act does mandate the disclosure of e...</td>\n",
       "      <td>text</td>\n",
       "      <td>Restricts access to energy consumption data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q105</td>\n",
       "      <td>What is the projected maximum batch size (in s...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>samples</td>\n",
       "      <td>false</td>\n",
       "      <td>['xia2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2408.04693']</td>\n",
       "      <td>For GPU memory capacities of 100GB and 120GB, ...</td>\n",
       "      <td>text</td>\n",
       "      <td>Predicted max batch size for 100GB GPU is 28.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q106</td>\n",
       "      <td>What was the approximate speedup in inference ...</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>multiplier</td>\n",
       "      <td>true</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>other</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q124</td>\n",
       "      <td>What is the estimated total operational water ...</td>\n",
       "      <td>The estimated total operational water consumpt...</td>\n",
       "      <td>4.731</td>\n",
       "      <td>liters</td>\n",
       "      <td>false</td>\n",
       "      <td>['li2025b']</td>\n",
       "      <td>['https://arxiv.org/pdf/2304.03271']</td>\n",
       "      <td>Water for Training(million L) Water for Each R...</td>\n",
       "      <td>table</td>\n",
       "      <td>Water consumption details table shows 4.731 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q135</td>\n",
       "      <td>True or False: The authors propose that sustai...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['ebert2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2410.06681']</td>\n",
       "      <td>Importantly, these assessments should not be l...</td>\n",
       "      <td>text</td>\n",
       "      <td>Emphasizes SIAs for all AI systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q139</td>\n",
       "      <td>As of 2023, what was the water use effectivene...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>L/kWh</td>\n",
       "      <td>false</td>\n",
       "      <td>['amazon2023', 'amazon2023']</td>\n",
       "      <td>['https://sustainability.aboutamazon.com/2023-...</td>\n",
       "      <td>Liters of water per kilowatt-hour (L/kWh) wate...</td>\n",
       "      <td>text</td>\n",
       "      <td>5% improvement from 2022 and 28% from 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q146</td>\n",
       "      <td>True or False: Local inference was emphasized ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['khan2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2504.06307']</td>\n",
       "      <td>While cost and carbon footprint reductions are...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Tackles energy efficiency through local infere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q153</td>\n",
       "      <td>True or False: Tracking the runtime of a train...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['strubell2019', 'cottier2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/1906.02243']</td>\n",
       "      <td>Tracking the runtime of a training job is an i...</td>\n",
       "      <td>text</td>\n",
       "      <td>Directly states the importance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>q158</td>\n",
       "      <td>For the LLaMA-65B model, what was the maximum ...</td>\n",
       "      <td>up to a 13.2%</td>\n",
       "      <td>13.2</td>\n",
       "      <td>percent</td>\n",
       "      <td>false</td>\n",
       "      <td>['chen2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2405.01814']</td>\n",
       "      <td>As illustrated in Figure 14, the LLaMA-65B mod...</td>\n",
       "      <td>figure</td>\n",
       "      <td>Illustrates latency reduction for LLaMA-65B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>q164</td>\n",
       "      <td>How much does an elephant weigh?</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>lbs</td>\n",
       "      <td>true</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>other</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>q166</td>\n",
       "      <td>Which of the following five large NLP DNNs has...</td>\n",
       "      <td>GShard-600B</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>false</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>GShard, and Switch Transformer from Google plu...</td>\n",
       "      <td>text</td>\n",
       "      <td>Supporting details provided in the text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>q170</td>\n",
       "      <td>How many days of CO₂ emissions from an average...</td>\n",
       "      <td>36,156</td>\n",
       "      <td>36156</td>\n",
       "      <td>days</td>\n",
       "      <td>false</td>\n",
       "      <td>['strubell2019']</td>\n",
       "      <td>['https://arxiv.org/pdf/1906.02243']</td>\n",
       "      <td>American life, avg, 1 year 36,156</td>\n",
       "      <td>text</td>\n",
       "      <td>American life, avg, 1 year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>q200</td>\n",
       "      <td>True or False: The Transformer architecture ev...</td>\n",
       "      <td></td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>true</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>other</td>\n",
       "      <td>is_blank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0   q003  What is the name of the benchmark suite presen...   \n",
       "1   q009  What were the net CO2e emissions from training...   \n",
       "2   q054  What is the model size in gigabytes (GB) for t...   \n",
       "3   q062  What was the total electricity consumption of ...   \n",
       "4   q075  True or False: Hyperscale data centers in 2020...   \n",
       "5   q078  For every medium-length GPT-3 completion (prom...   \n",
       "6   q091  From a sample of 60 papers from top AI confere...   \n",
       "7   q102  True or False: The AI Act makes energy consump...   \n",
       "8   q105  What is the projected maximum batch size (in s...   \n",
       "9   q106  What was the approximate speedup in inference ...   \n",
       "10  q124  What is the estimated total operational water ...   \n",
       "11  q135  True or False: The authors propose that sustai...   \n",
       "12  q139  As of 2023, what was the water use effectivene...   \n",
       "13  q146  True or False: Local inference was emphasized ...   \n",
       "14  q153  True or False: Tracking the runtime of a train...   \n",
       "15  q158  For the LLaMA-65B model, what was the maximum ...   \n",
       "16  q164                   How much does an elephant weigh?   \n",
       "17  q166  Which of the following five large NLP DNNs has...   \n",
       "18  q170  How many days of CO₂ emissions from an average...   \n",
       "19  q200  True or False: The Transformer architecture ev...   \n",
       "\n",
       "                                               answer         answer_value  \\\n",
       "0                                 ML.ENERGY Benchmark  ML.ENERGY Benchmark   \n",
       "1                                                 4.3                  4.3   \n",
       "2                                                                 is_blank   \n",
       "3                                                                 is_blank   \n",
       "4                                                True                   40   \n",
       "5                                               500ml                  500   \n",
       "6   Difference in percentage of CVPR papers target...                   55   \n",
       "7                                               False                    0   \n",
       "8                                                  28                   28   \n",
       "9                                                                 is_blank   \n",
       "10  The estimated total operational water consumpt...                4.731   \n",
       "11                                               True                    1   \n",
       "12                                               0.18                 0.18   \n",
       "13                                               True                    1   \n",
       "14                                               True                    1   \n",
       "15                                      up to a 13.2%                 13.2   \n",
       "16                                                                is_blank   \n",
       "17                                        GShard-600B             is_blank   \n",
       "18                                             36,156                36156   \n",
       "19                                                                is_blank   \n",
       "\n",
       "       answer_unit is_blank                           ref_id  \\\n",
       "0         is_blank    false                    ['chung2025']   \n",
       "1            tCO2e    false                ['patterson2021']   \n",
       "2               GB     true                         is_blank   \n",
       "3              MWh     true                         is_blank   \n",
       "4         is_blank    false                      ['wu2021b']   \n",
       "5   500 mL bottles    false                      ['li2025b']   \n",
       "6          percent    false                 ['schwartz2019']   \n",
       "7         is_blank    false                    ['ebert2024']   \n",
       "8          samples    false                      ['xia2024']   \n",
       "9       multiplier     true                         is_blank   \n",
       "10          liters    false                      ['li2025b']   \n",
       "11        is_blank    false                    ['ebert2024']   \n",
       "12           L/kWh    false     ['amazon2023', 'amazon2023']   \n",
       "13        is_blank    false                     ['khan2025']   \n",
       "14        is_blank    false  ['strubell2019', 'cottier2024']   \n",
       "15         percent    false                     ['chen2024']   \n",
       "16             lbs     true                         is_blank   \n",
       "17        is_blank    false                ['patterson2021']   \n",
       "18            days    false                 ['strubell2019']   \n",
       "19        is_blank     true                         is_blank   \n",
       "\n",
       "                                              ref_url  \\\n",
       "0                ['https://arxiv.org/pdf/2505.06371']   \n",
       "1                ['https://arxiv.org/pdf/2104.10350']   \n",
       "2                                            is_blank   \n",
       "3                                            is_blank   \n",
       "4                ['https://arxiv.org/pdf/2108.06738']   \n",
       "5                ['https://arxiv.org/pdf/2304.03271']   \n",
       "6                ['https://arxiv.org/pdf/1907.10597']   \n",
       "7                ['https://arxiv.org/pdf/2410.06681']   \n",
       "8                ['https://arxiv.org/pdf/2408.04693']   \n",
       "9                                            is_blank   \n",
       "10               ['https://arxiv.org/pdf/2304.03271']   \n",
       "11               ['https://arxiv.org/pdf/2410.06681']   \n",
       "12  ['https://sustainability.aboutamazon.com/2023-...   \n",
       "13               ['https://arxiv.org/pdf/2504.06307']   \n",
       "14               ['https://arxiv.org/pdf/1906.02243']   \n",
       "15               ['https://arxiv.org/pdf/2405.01814']   \n",
       "16                                           is_blank   \n",
       "17               ['https://arxiv.org/pdf/2104.10350']   \n",
       "18               ['https://arxiv.org/pdf/1906.02243']   \n",
       "19                                           is_blank   \n",
       "\n",
       "                                 supporting_materials evidence_type  \\\n",
       "0   The ML.ENERGY Benchmark is the first inference...          text   \n",
       "1   GShard-600B’s emissions (Table 4) are 4.3 tCO2...         table   \n",
       "2                                            is_blank         other   \n",
       "3                                            is_blank         other   \n",
       "4   Furthermore, between traditional and highly op...        figure   \n",
       "5   GPT-3 needs to “drink” (i.e., consume) a500ml ...          text   \n",
       "6   As shown in Figure 2, in all conferences we co...        figure   \n",
       "7   Where the Act does mandate the disclosure of e...          text   \n",
       "8   For GPU memory capacities of 100GB and 120GB, ...          text   \n",
       "9                                            is_blank         other   \n",
       "10  Water for Training(million L) Water for Each R...         table   \n",
       "11  Importantly, these assessments should not be l...          text   \n",
       "12  Liters of water per kilowatt-hour (L/kWh) wate...          text   \n",
       "13  While cost and carbon footprint reductions are...        figure   \n",
       "14  Tracking the runtime of a training job is an i...          text   \n",
       "15  As illustrated in Figure 14, the LLaMA-65B mod...        figure   \n",
       "16                                           is_blank         other   \n",
       "17  GShard, and Switch Transformer from Google plu...          text   \n",
       "18                  American life, avg, 1 year 36,156          text   \n",
       "19                                           is_blank         other   \n",
       "\n",
       "                                          explanation  \n",
       "0                 Introduction of ML.ENERGY Benchmark  \n",
       "1                  Table 4 shows 4.3 tCO2e emissions.  \n",
       "2                                            is_blank  \n",
       "3                                            is_blank  \n",
       "4      Figure 1 shows >40% efficiency for hyperscale.  \n",
       "5               GPT-3 needs 500ml for 10-50 responses  \n",
       "6                    Figure 2 shows the distribution.  \n",
       "7         Restricts access to energy consumption data  \n",
       "8       Predicted max batch size for 100GB GPU is 28.  \n",
       "9                                            is_blank  \n",
       "10  Water consumption details table shows 4.731 mi...  \n",
       "11                 Emphasizes SIAs for all AI systems  \n",
       "12         5% improvement from 2022 and 28% from 2021  \n",
       "13  Tackles energy efficiency through local infere...  \n",
       "14                     Directly states the importance  \n",
       "15        Illustrates latency reduction for LLaMA-65B  \n",
       "16                                           is_blank  \n",
       "17           Supporting details provided in the text.  \n",
       "18                         American life, avg, 1 year  \n",
       "19                                           is_blank  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run over max_N training questions (this can take a while!)\n",
    "# -------------------------------------------------------------------\n",
    "all_results = []\n",
    "error_log = []\n",
    "max_N = np.inf\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    if idx >= max_N:\n",
    "        break\n",
    "    question = row[\"question\"]\n",
    "    print(f\"########################################################################################################\\nQUESTION: {question}\")\n",
    "\n",
    "    res = run_single_qa(\n",
    "        row=row,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=8,\n",
    "    )\n",
    "    answer = res[\"answer\"]\n",
    "    ref_ids = res[\"ref_id\"]\n",
    "\n",
    "    explanation = res[\"explanation\"]\n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    print(f\"ref_ids: {ref_ids}\")\n",
    "    print(f\"EXPLANATION: {explanation}\")\n",
    "    \n",
    "    all_results.append(res)\n",
    "\n",
    "solutions_df = pd.DataFrame(all_results)\n",
    "solutions_path = os.path.join(local_data_dir, \"train_solutions_qwen.csv\")\n",
    "solutions_df.to_csv(solutions_path, index=False)\n",
    "\n",
    "print(f\"Saved solutions for {len(solutions_df)} questions to: {solutions_path}\")\n",
    "print(f\"Number of questions with errors (filled as blank): {len(error_log)}\")\n",
    "\n",
    "solutions_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b65a1-2255-4f1c-8d71-f35a09d0c9e0",
   "metadata": {},
   "source": [
    "### Compare answers to ground truth\n",
    "\n",
    "WattBot evaluates each prediction using a weighted score that combines three components. Most of the credit (0.75) comes from the `answer_value`, which must match the ground truth after normalization (numeric answers must be within ±0.1% relative tolerance; categorical values must match exactly). An additional 0.15 comes from `ref_id`, where partial credit is given based on the Jaccard overlap between your cited document IDs and the ground-truth set. The final 0.10 comes from correctly marking unanswerable questions: if a question is truly unanswerable, you must set all relevant fields (`answer_value`, `answer_unit`, `ref_id`, `ref_url`, `supporting_materials`) to `is_blank`. Any other combination scores zero for this component.\n",
    "\n",
    "| Component      | Weight | What counts as correct |\n",
    "|----------------|--------|------------------------|\n",
    "| answer_value   | 0.75   | Numeric within ±0.1% relative tolerance; categorical exact match; `is_blank` if unanswerable |\n",
    "| ref_id         | 0.15   | Jaccard overlap with the ground-truth reference set (case-insensitive) |\n",
    "| is_NA          | 0.10   | All required fields set to `is_blank` when the question is truly unanswerable |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "332387ca-49df-470c-bbf6-365b89a64947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    # Normalize to string for special tokens\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,   # if None, will auto-detect is_NA or is_blank\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "    Returns a DataFrame with per-row scores and prints summary stats.\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # If predictions column name for NA isn't given, auto-detect\n",
    "    if pred_is_na_col is None:\n",
    "        if \"is_NA\" in preds.columns:\n",
    "            pred_is_na_col = \"is_NA\"\n",
    "        elif \"is_blank\" in preds.columns:\n",
    "            pred_is_na_col = \"is_blank\"\n",
    "        else:\n",
    "            raise ValueError(\"Could not find is_NA or is_blank column in predictions.\")\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "    \n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for _, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = _to_bool_flag(row[gt_is_na_col])\n",
    "        pred_is_na = _to_bool_flag(row[pred_is_na_col])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9255d196-b6c2-4b02-b14e-ec250055fb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows compared: 41\n",
      "Mean answer_value score: 0.5366\n",
      "Mean ref_id score:       0.6098\n",
      "Mean is_NA score:        1.0000\n",
      "Overall WattBot score:   0.5939\n"
     ]
    }
   ],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=\"./data/train_solutions_qwen.csv\",\n",
    "    gt_is_na_col=\"is_blank\",\n",
    "    pred_is_na_col=\"is_blank\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35976c08",
   "metadata": {},
   "source": [
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- **Notebook setup**: Start by provisioning a GPU-backed notebook instance\n",
    "  (e.g., `ml.g5.xlarge`) so that both the embedding model and Qwen2.5-7B\n",
    "  can run comfortably.\n",
    "- **Local-first RAG**: For teaching (and small corpora), we avoid an external vector database\n",
    "  and instead perform cosine similarity search over in-memory embeddings.\n",
    "- **Ground-truth units**: The `answer_unit` column is always copied directly\n",
    "  from `train_QA.csv`, never guessed by the LLM.\n",
    "- **Two-stage LLM use**: One call focuses on *answering and citing*; a second,\n",
    "  lighter call produces a short explanation tagged with an evidence type.\n",
    "- **WattBot conventions**: We respect the Kaggle competition format,\n",
    "  using `is_blank` for unanswerable questions and for missing fields.\n",
    "- **Scalability path**: The same logic can later be swapped to FAISS/Chroma\n",
    "  and larger models, while preserving the interface used here.\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c5415-6cd4-40df-9701-2302d26a0949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6bdd63-0938-4548-b810-f78c262abf00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749f350-580d-4543-9bea-64b8e0afefb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
