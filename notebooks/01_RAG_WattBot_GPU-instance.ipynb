{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7824f19",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with a Notebook GPU\"\n",
    "teaching: 30\n",
    "exercises: 15\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- How can we run a basic Retrieval-Augmented Generation (RAG) pipeline entirely from a single GPU-backed SageMaker notebook?\n",
    "- How do we go from raw PDFs and CSV files to a searchable embedding space for WattBot documents?\n",
    "- How can we generate WattBot-style answers (including citations and evidence) that follow the competition‚Äôs scoring conventions?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Verify that our SageMaker notebook instance has a working GPU and compatible Python environment.\n",
    "- Load the WattBot metadata and question‚Äìanswer files from local storage and inspect their structure.\n",
    "- Download all referenced PDFs from `metadata.csv` and turn them into a collection of text pages with useful metadata attached.\n",
    "- Implement a simple, explicit ‚Äúfrom scratch‚Äù text-chunking and embedding pipeline without relying on FAISS or production vector DBs.\n",
    "- Build a small retrieval helper that finds the most relevant chunks for a question using cosine similarity in embedding space.\n",
    "- Wire the retriever to a local Qwen 7B-style generator to produce WattBot-format answers (including `answer`, `ref_id`, `ref_url`, and `supporting_materials`).\n",
    "- Add a second LLM pass that generates short explanations and marks whether the evidence comes from text, figures, tables, or a combination.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "\n",
    "## Working with AWS for RAG Experiments \n",
    "\n",
    "In the previous episode, we briefly introduced several approaches for implementing RAG in AWS. Here, we are simply selecting a good GPU instance that can handle whatever RAG system we want to build. This approach is:\n",
    "\n",
    "- Very easy to understand core on the AWS side of things (just select GPU instance and you're good to move on)\n",
    "- Ideal for learning retrieval and generation steps  \n",
    "- Great for experimentation and debugging  \n",
    "\n",
    "However, it is **not the most cost‚Äëefficient method**. In upcoming episodes we will introduce more efficient and production‚Äëaligned GPU strategies, including:\n",
    "\n",
    "- On-demand GPU tasks  \n",
    "- Fully managed asynchronous jobs  \n",
    "- Serverless or streaming LLM inference  \n",
    "- SageMaker batch transform & RAG pipelines  \n",
    "- Embedding jobs that run only when needed  \n",
    "\n",
    "Those techniques bring you closer to best practice for scalable and budget‚Äëfriendly research computing.\n",
    "\n",
    "**Remember to Shut Down Your AWS Instance**: GPU notebook instances continue billing **even when idle**.  Always:\n",
    "\n",
    "- Save your work  \n",
    "- Shut down or stop the instance when not in use\n",
    "- Verify the status in the AWS console  \n",
    "\n",
    "This habit prevents accidental ongoing GPU charges.\n",
    "\n",
    "\n",
    "## Overview: WattBot RAG on a single notebook GPU\n",
    "\n",
    "In this episode we build a **minimal but realistic RAG pipeline** from the [WattBot 2025](https://www.kaggle.com/competitions/WattBot2025/overview) challenge that runs entirely from a single GPU-backed SageMaker notebook.\n",
    "\n",
    "In this episode we will:\n",
    "\n",
    "1. **Work directly with the WattBot data.**\n",
    "   - Use `train_QA.csv` and `metadata.csv` from the competition dataset.\n",
    "   - Download all referenced PDFs (our RAG corpus) using the URLs in `metadata.csv`.\n",
    "2. **Implement the core RAG steps explicitly in code.**\n",
    "   - Read PDFs, extract per-page text, and attach document metadata.\n",
    "   - Chunk text into overlapping segments suitable for embedding.\n",
    "   - Embed chunks with a sentence-transformer (`thenlper/gte-base`)\n",
    "   - Implement cosine-similarity search over the embedding matrix.\n",
    "3. **Connect to a local Qwen-style generator.**\n",
    "   - Use a quantized 7B model on a GPU-backed instance (e.g., `ml.g5.xlarge`).\n",
    "   - Construct WattBot-style answers that we can compare against `train_QA.csv`.\n",
    "4. **Add an explanation pass.**\n",
    "   - Use an LLM to look at the retrieved evidence, the answer, and citations.\n",
    "   - Generate a short explanation and label the **evidence type**: `[Quote]`, `[Table]`, `[Figure]`, or `[Mixed]`.\n",
    "\n",
    "\n",
    "## Notebook + dataset setup\n",
    "\n",
    "For this episode, we assume you are running on an AWS SageMaker notebook instance with a GPU, such as:\n",
    "\n",
    "- `ml.g5.xlarge` (recommended) or\n",
    "- `ml.g4dn.xlarge` (may work with smaller models / more aggressive quantization).\n",
    "\n",
    "See [Instances for ML](https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/instances-for-ML.html) for further guidance.\n",
    "\n",
    "\n",
    "### Step 1 ‚Äì Download `data.zip` locally\n",
    "\n",
    "We‚Äôll use the **WattBot 2025** dataset. Download the workshop data archive to your laptop or desktop:\n",
    "\n",
    "- Open this link in your browser: https://github.com/carpentries-incubator/ML_with_AWS_SageMaker/blob/main/data/data.zip\n",
    "- Save `data.zip` somewhere you can find it easily and unzip the folder contents\n",
    "\n",
    "This archive should include a `data/wattbot/` folder containing:\n",
    "\n",
    "- `metadata.csv` ‚Äì index of all WattBot papers.\n",
    "- `train_QA.csv` ‚Äì labeled questions + ground truth answers.\n",
    "\n",
    "### Step 2 ‚Äì Create a WattBot S3 bucket\n",
    "\n",
    "In the AWS console:\n",
    "\n",
    "1. Go to **S3**.\n",
    "2. Create a new bucket named something like:  \n",
    "   `teamname-yourname-wattbot`\n",
    "3. Keep **Block all public access** enabled.\n",
    "4. Add tags so we can track costs:  \n",
    "   - `Project = your-team-name`  \n",
    "   - `Name = your-name`  \n",
    "   - `Purpose = RAG-demo`\n",
    "5. Once the bucket is created, you'll be brought to a page that shows all of your current buckets (and those on our shared account). We'll have to edit our bucket's policy to allow ourselves proper access to any files stored there (e.g., read from bucket, write to bucket). To set these permissions...\n",
    "\n",
    "a. Click on the name of your bucket to bring up additional options and settings.\n",
    "   b. Click the Permissions tab\n",
    "   c. Scroll down to Bucket policy and click Edit. Paste the following policy, editing the bucket name \"sinkorswim-doejohn-wattbot\" to reflect your bucket's nameAs we did in the \"setting up S3 episode, edit your bucket's policy to include the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Principal\": {\n",
    "\t\t\t    \"AWS\": [\n",
    "\t\t\t        \"arn:aws:iam::183295408236:role/ml-sagemaker-use\",\n",
    "\t\t\t        \"arn:aws:iam::183295408236:role/ml-sagemaker-bedrock-use\"\n",
    "\t\t        ]\n",
    "\t\t\t},\n",
    "\t\t\t\"Action\": [\n",
    "\t\t\t\t\"s3:GetObject\",\n",
    "\t\t\t\t\"s3:PutObject\",\n",
    "\t\t\t\t\"s3:DeleteObject\",\n",
    "\t\t\t\t\"s3:ListMultipartUploadParts\"\n",
    "\t\t\t],\n",
    "\t\t\t\"Resource\": [\n",
    "\t\t\t\t\"arn:aws:s3:::sinkorswim-chrisendemann-titanic\",\n",
    "\t\t\t\t\"arn:aws:s3:::sinkorswim-chrisendemann-titanic/*\"\n",
    "\t\t\t]\n",
    "\t\t}\n",
    "\t]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cf523",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äì Upload the WattBot files to S3\n",
    "\n",
    "1. In your new bucket, click **Upload**.\n",
    "2. Drag the `data/wattbot/` folder contents from `data.zip` into the upload dialog.\n",
    "3. Upload it so that your bucket contains paths like:\n",
    "\n",
    "   - `metadata.csv`\n",
    "   - `train_QA.csv`\n",
    "\n",
    "We‚Äôll pull these files from S3 into the notebook in the next steps.\n",
    "\n",
    "\n",
    "###  Verify GPU and basic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi || echo \"No GPU detected ‚Äì please switch to a GPU-backed instance (e.g., ml.g5.xlarge) before running this notebook.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also verify you've selected teh conda_pytorch_p310 kernel\n",
    "import torch\n",
    "print(\"torch cuda available:\", torch.cuda.is_available())\n",
    "print(\"num gpus:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c286e52",
   "metadata": {},
   "source": [
    "## Import data from bucket into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Initialize SageMaker + AWS basics\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role()\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_object(bucket: str, key: str, local_path: str) -> None:\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    print(f\"Downloading s3://{bucket}/{key} -> {local_path}\")\n",
    "    s3_client.download_file(bucket, key, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update this to your bucket name\n",
    "bucket_name = \"chris-rag\"  # <-- EDIT ME\n",
    "\n",
    "# Local working directory in the notebook instance\n",
    "local_data_dir = \"./data\"\n",
    "\n",
    "print(\"Local data dir:\", local_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ba181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download metadata.csv and train_QA.csv\n",
    "metadata_key = \"metadata.csv\"\n",
    "train_qa_key = \"train_QA.csv\"\n",
    "\n",
    "metadata_path = os.path.join(local_data_dir, metadata_key)\n",
    "train_qa_path = os.path.join(local_data_dir, train_qa_key)\n",
    "\n",
    "download_s3_object(bucket_name, metadata_key, metadata_path)\n",
    "download_s3_object(bucket_name, train_qa_key, train_qa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0b6e4",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äì Imports, paths, and safe CSV loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import zipfile\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_read_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Try several encodings when reading a CSV file.\n",
    "\n",
    "    Some CSVs (especially those with special characters in author names or titles)\n",
    "    may not be valid UTF-8. This helper rotates through common encodings and raises\n",
    "    the last error only if all fail.\n",
    "    \"\"\"\n",
    "    encodings = [\"utf-8\", \"latin1\", \"ISO-8859-1\", \"cp1252\"]\n",
    "    last_error = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "    if last_error is not None:\n",
    "        raise last_error\n",
    "    raise RuntimeError(f\"Unable to read CSV at {path}\")\n",
    "\n",
    "\n",
    "train_df = smart_read_csv(train_qa_path)\n",
    "metadata_df = smart_read_csv(metadata_path)\n",
    "\n",
    "print(\"train_QA.csv columns:\", train_df.columns.tolist())\n",
    "print(\"metadata.csv columns:\", metadata_df.columns.tolist())\n",
    "print(\"\\nNumber of training QAs:\", len(train_df))\n",
    "print(\"Number of metadata rows:\", len(metadata_df))\n",
    "\n",
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad473a",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äì Download all PDFs from `metadata.csv`\n",
    "\n",
    "Next we will...\n",
    "\n",
    "1. Read the `url` column from `metadata.csv`.\n",
    "2. Download each PDF via HTTP and save it locally as `<id>.pdf` under `pdfs/`.\n",
    "3. Report any failures (e.g., missing or malformed URLs) at the end.\n",
    "4. Upload zipped version of corpus to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e539764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = os.path.join(local_data_dir, \"pdfs\")\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "def download_all_pdfs_from_urls(\n",
    "    metadata: pd.DataFrame,\n",
    "    local_pdf_dir: str,\n",
    "    url_col: str = \"url\",\n",
    "    id_col: str = \"id\",\n",
    "    timeout: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"Download all PDFs referenced in `metadata` using their URLs.\n",
    "\n",
    "    - Saves each file as `<id>.pdf` in `local_pdf_dir`.\n",
    "    - Strips whitespace from the URL (to avoid trailing spaces becoming `%20`).\n",
    "    - Skips rows with missing or non-HTTP URLs.\n",
    "    - Prints a short summary of any failures.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_pdf_dir, exist_ok=True)\n",
    "    errors: List[Tuple[str, str]] = []\n",
    "\n",
    "    print(f\"Saving PDFs to: {local_pdf_dir}\\n\")\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[id_col]).strip()\n",
    "\n",
    "        raw_url = row.get(url_col, None)\n",
    "        if not isinstance(raw_url, str):\n",
    "            errors.append((doc_id, \"URL is not a string\"))\n",
    "            continue\n",
    "\n",
    "        pdf_url = raw_url.strip()  # important: strip trailing whitespace\n",
    "        if not pdf_url.startswith(\"http\"):\n",
    "            errors.append((doc_id, f\"Invalid URL: {pdf_url!r}\"))\n",
    "            continue\n",
    "\n",
    "        local_path = os.path.join(local_pdf_dir, f\"{doc_id}.pdf\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {doc_id} from {pdf_url} ...\")\n",
    "            resp = requests.get(pdf_url, timeout=timeout, allow_redirects=True)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "            if \"pdf\" not in content_type.lower() and not pdf_url.lower().endswith(\".pdf\"):\n",
    "                print(f\"  Warning: Content-Type for {doc_id} does not look like PDF ({content_type})\")\n",
    "\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> FAILED for {doc_id}: {e}\")\n",
    "            errors.append((doc_id, str(e)))\n",
    "\n",
    "    if errors:\n",
    "        print(\"\\nSome PDFs could not be downloaded:\")\n",
    "        for doc_id, err in errors:\n",
    "            print(f\"  {doc_id}: {err}\")\n",
    "    else:\n",
    "        print(\"\\nAll PDFs downloaded successfully!\")\n",
    "\n",
    "\n",
    "download_all_pdfs_from_urls(\n",
    "    metadata_df,\n",
    "    PDF_DIR,\n",
    "    url_col=\"url\",\n",
    "    id_col=\"id\",\n",
    "    timeout=20,\n",
    ")\n",
    "\n",
    "len(os.listdir(PDF_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea373bb",
   "metadata": {},
   "source": [
    "### Zip all PDFs and upload to S3\n",
    "\n",
    "Once we have all PDFs locally, it can be convenient and efficient to:\n",
    "\n",
    "1. Zip them into a single file (e.g., `wattbot_pdfs.zip`).  \n",
    "2. Upload that ZIP archive to an S3 bucket, such as `s3://<your-wattbot-bucket>/data/wattbot/wattbot_pdfs.zip`.\n",
    "\n",
    "We‚Äôll include a short code example here, but feel free to skip this during the workshop if time is tight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459aaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import boto3\n",
    "\n",
    "def zip_and_upload_pdfs(\n",
    "    local_pdf_dir: str,\n",
    "    bucket: str,\n",
    "    zip_name: str = \"corpus.zip\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Zips all PDFs in local_pdf_dir and uploads the ZIP file to:\n",
    "        s3://<bucket>/<prefix>/<zip_name>\n",
    "\n",
    "    Returns the full S3 URI of the uploaded zip file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure directory exists\n",
    "    if not os.path.exists(local_pdf_dir):\n",
    "        raise ValueError(f\"Directory not found: {local_pdf_dir}\")\n",
    "\n",
    "    # Path for the ZIP file\n",
    "    zip_path = os.path.join(local_pdf_dir, zip_name)\n",
    "\n",
    "    # Create ZIP archive\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for fname in os.listdir(local_pdf_dir):\n",
    "            if fname.lower().endswith(\".pdf\"):\n",
    "                fpath = os.path.join(local_pdf_dir, fname)\n",
    "                zipf.write(fpath, arcname=fname)\n",
    "                print(f\"Added to ZIP: {fname}\")\n",
    "\n",
    "    print(f\"\\nZIP created: {zip_path}\")\n",
    "\n",
    "    # Upload to S3\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_key = f\"{zip_name}\"\n",
    "\n",
    "    print(f\"Uploading to s3://{bucket}/{s3_key} ...\")\n",
    "    s3_client.upload_file(zip_path, bucket, s3_key)\n",
    "    print(\"Upload complete.\")\n",
    "\n",
    "    return f\"s3://{bucket}/{s3_key}\"\n",
    "\n",
    "\n",
    "zip_s3_uri = zip_and_upload_pdfs(\n",
    "    local_pdf_dir=PDF_DIR,\n",
    "    bucket=bucket_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c35f5",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äì Turn PDFs into page-level ‚Äúdocuments‚Äù\n",
    "\n",
    "Next, we convert each PDF into a list of **page-level records**. Each record stores:\n",
    "\n",
    "- `text`: page text (as extracted by `pypdf`).\n",
    "- `doc_id`: short ID from `metadata.csv` (e.g., `strubell2019`).\n",
    "- `title`: title of the document.\n",
    "- `url`: original PDF URL.\n",
    "- `page_num`: zero-based page index.\n",
    "- `page_label`: label used inside the PDF (often 1-based).\n",
    "\n",
    "Later, we will **chunk these pages** into smaller overlapping segments for embedding.\n",
    "\n",
    "### Why we page-chunk first\n",
    "\n",
    "We split the PDF into **pages before chunking** because pages give us a stable, easy-to-interpret unit.  \n",
    "This helps with:\n",
    "\n",
    "- **Keeping metadata** (doc ID, URL, page labels) tied to the text.  \n",
    "- **Debugging retrieval** ‚Äî it‚Äôs much easier to understand what the model saw if we know which page(s) were used.  \n",
    "- **Cleaning text** before making smaller overlapping chunks.  \n",
    "- **Flexibility later** ‚Äî once pages are structured, we can try different chunk sizes or strategies without re-extracting the PDF.\n",
    "\n",
    "In short: **pages first ‚Üí then chunks** keeps the workflow cleaner and easier to reason about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d103fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756433dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def pdfs_to_page_docs(metadata: pd.DataFrame, pdf_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load each PDF into a list of page-level dictionaries.\n",
    "\n",
    "    Each dict has keys: text, doc_id, title, url, page_num, page_label, total_pages.\n",
    "    \"\"\"\n",
    "    page_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[\"id\"]).strip()\n",
    "        title = str(row.get(\"title\", \"\")).strip()\n",
    "        url = str(row.get(\"url\", \"\")).strip()\n",
    "\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{doc_id}.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"Missing PDF for {doc_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {pdf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        total_pages = len(reader.pages)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text = page.extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract text from {doc_id} page {i}: {e}\")\n",
    "                text = \"\"\n",
    "\n",
    "            text = text.strip()\n",
    "            if not text:\n",
    "                # Still keep the page so we know it exists, but mark it as empty\n",
    "                text = \"[[EMPTY PAGE TEXT ‚Äì see original PDF for tables/figures]]\"\n",
    "\n",
    "            page_docs.append(\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"page_num\": i,\n",
    "                    \"page_label\": str(i + 1),\n",
    "                    \"total_pages\": total_pages,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return page_docs\n",
    "\n",
    "\n",
    "page_docs = pdfs_to_page_docs(metadata_df, PDF_DIR)\n",
    "print(f\"Loaded {len(page_docs)} page-level records from {len(metadata_df)} PDFs.\")\n",
    "page_docs[0] if page_docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648729b5",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äì Simple, explicit text chunking\n",
    "\n",
    "RAG systems typically break documents into **chunks** so that:\n",
    "\n",
    "- Each chunk is long enough to carry meaningful context.\n",
    "- No chunk is so long that it blows up the embedding/LLM context window.\n",
    "\n",
    "For this workshop we will implement a **simple sliding-window chunker** that operates on characters:\n",
    "\n",
    "- `chunk_size_chars`: maximum characters per chunk (e.g., 1,000‚Äì1,500).\n",
    "- `chunk_overlap_chars`: overlap between consecutive chunks (e.g., 200).\n",
    "\n",
    "In our own work, you may wish to plug in more sophisticated *semantic chunking*  methods(e.g., splitting on headings, section titles, or sentence boundaries). For now, we'll keep the implementation explicit and easy to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543365f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(\n",
    "    text: str,\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[str]:\n",
    "    \"\"\"Split `text` into overlapping character-based chunks.\n",
    "\n",
    "    This is a simple baseline; more advanced versions might:\n",
    "    - split on sentence boundaries, or\n",
    "    - merge short paragraphs and respect section headings.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    while start < text_len:\n",
    "        end = min(start + chunk_size_chars, text_len)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == text_len:\n",
    "            break\n",
    "        # Move the window forward, keeping some overlap\n",
    "        start = end - chunk_overlap_chars\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def make_chunked_docs(\n",
    "    page_docs: List[Dict[str, Any]],\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Turn page-level records into smaller overlapping text chunks.\n",
    "\n",
    "    Each chunk keeps a pointer back to its document and page metadata.\n",
    "    \"\"\"\n",
    "    chunked: List[Dict[str, Any]] = []\n",
    "    for page in page_docs:\n",
    "        page_text = page[\"text\"]\n",
    "        chunks = split_text_into_chunks(\n",
    "            page_text,\n",
    "            chunk_size_chars=chunk_size_chars,\n",
    "            chunk_overlap_chars=chunk_overlap_chars,\n",
    "        )\n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            chunked.append(\n",
    "                {\n",
    "                    \"text\": chunk_text,\n",
    "                    \"doc_id\": page[\"doc_id\"],\n",
    "                    \"title\": page[\"title\"],\n",
    "                    \"url\": page[\"url\"],\n",
    "                    \"page_num\": page[\"page_num\"],\n",
    "                    \"page_label\": page[\"page_label\"],\n",
    "                    \"total_pages\": page[\"total_pages\"],\n",
    "                    \"chunk_idx_in_page\": idx,\n",
    "                }\n",
    "            )\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c664029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "chunks_s3_key = 'chunks.jsonl'\n",
    "chunks_jsonl_path = os.path.join(local_data_dir, chunks_s3_key)\n",
    "\n",
    "def save_chunked_docs_jsonl(path, chunks):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in chunks:\n",
    "            json.dump(rec, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def load_chunked_docs_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Cached chunking logic\n",
    "# -------------------------------------------------------------------\n",
    "if os.path.exists(chunks_jsonl_path):\n",
    "    print(f\"Found existing chunk file: {chunks_jsonl_path}\")\n",
    "    chunked_docs = load_chunked_docs_jsonl(chunks_jsonl_path)\n",
    "    print(\"Loaded chunked docs:\", len(chunked_docs))\n",
    "else:\n",
    "    print(\"No chunk file found. Running chunking step...\")\n",
    "    chunked_docs = make_chunked_docs(page_docs)\n",
    "    save_chunked_docs_jsonl(chunks_jsonl_path, chunked_docs)\n",
    "    print(f\"Saved chunked docs to {chunks_jsonl_path}\")\n",
    "\n",
    "# Show first chunk\n",
    "print(\"Raw pages:\", len(page_docs))\n",
    "print(\"Chunked docs:\", len(chunked_docs))\n",
    "chunked_docs[0] if chunked_docs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c62d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 so future runs (or other instances) can reuse\n",
    "print(f\"Uploading chunked docs to s3 ...\")\n",
    "s3_client.upload_file(chunks_jsonl_path, bucket_name, chunks_s3_key)\n",
    "print(\"Upload complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844c83c",
   "metadata": {},
   "source": [
    "## Step 5 ‚Äì Build an embedding matrix\n",
    "\n",
    "Now we embed each chunk into a vector using a **sentence-transformer** model. For WattBot, a strong and relatively efficient choice is:\n",
    "\n",
    "### `thenlper/gte-large` (Recommended baseline embedder)\n",
    "\n",
    "- Size / parameters:  ~335M parameters, roughly 1.3‚Äì1.4 GB in BF16/FP16 when loaded on GPU. Fits cleanly on T4 (16 GB), L4, A10G, A10, A100, and all g5.* instances.  Offers noticeably better retrieval quality than smaller 100M‚Äì150M models without requiring high-end GPU memory. Runs comfortably on g4dn.xlarge, g5.xlarge, or g5.2xlarge during workshops. Lets participants see meaningful improvements from chunking and retrieval methods without excessive compute cost.\n",
    "\n",
    "- Intended use:  General-purpose retrieval and semantic search across academic PDFs, sustainability reports, and mixed-domain long-form documents. Stronger semantic coherence than gte-base or MiniLM, but still lightweight enough for workshop hardware.\n",
    "\n",
    "- Throughput expectations:\n",
    "  - CPU only: workable for small corpora (<2k chunks) but slow for anything larger.  \n",
    "  - GPU (T4, L4, A10G, A100) with batch sizes around 64‚Äì128:  \n",
    "    - 20k‚Äì40k chunks/min on L4 or A10G  \n",
    "    - 10k‚Äì15k chunks/min on T4  \n",
    "    - 50k+ chunks/min on A100  \n",
    "      \n",
    "We will:\n",
    "\n",
    "1. Load the embedding model on GPU if available.\n",
    "2. Encode all chunks in batches.\n",
    "3. Store the resulting matrix as a `torch.Tensor` or `numpy.ndarray` along with the original `chunked_docs` list.\n",
    "\n",
    "Later, we‚Äôll implement a small retrieval helper that does cosine-similarity search over this matrix‚Äîno additional indexing library required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a545f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# We'll use a stronger embedding model now that we have a GPU.\n",
    "# This model has ~335M parameters and benefits from GPU acceleration,\n",
    "# but is still reasonable to run on a single 24 GB GPU.\n",
    "embedding_model_id = \"thenlper/gte-large\"\n",
    "\n",
    "use_cuda_for_embeddings = torch.cuda.is_available()\n",
    "print(\"CUDA available for embeddings:\", use_cuda_for_embeddings)\n",
    "\n",
    "# Single shared embedder object that we can pass around.\n",
    "embedder = SentenceTransformer(\n",
    "    embedding_model_id,\n",
    "    device=\"cuda\" if use_cuda_for_embeddings else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ade090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(embedder, docs, batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"Embed all chunk texts into a dense matrix of shape (N, D).\"\"\"\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    all_embeddings = []\n",
    "    start = time.time()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        emb = embedder.encode(\n",
    "            batch,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        all_embeddings.append(emb)\n",
    "    embeddings = np.vstack(all_embeddings) if all_embeddings else np.zeros((0, 768))\n",
    "    print(f\"Computed embeddings for {len(texts)} chunks in {time.time() - start:.1f}s\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf605e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_embeddings = embed_texts(embedder, chunked_docs)\n",
    "chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9aa65",
   "metadata": {},
   "source": [
    "### 6. Build a simple retrieval step (cosine similarity)\n",
    "\n",
    "We are **not** using a heavy vector database in this first episode.\n",
    "\n",
    "Instead, we:\n",
    "\n",
    "1. Embed each chunk with `thenlper/gte-large` (done above).\n",
    "2. Embed each question.\n",
    "3. Compute cosine similarity between the question embedding and all chunk embeddings.\n",
    "4. Take the top‚Äìk most similar chunks as our retrieved context.\n",
    "\n",
    "This keeps the retrieval logic completely transparent for teaching, while still matching the *spirit* of\n",
    "production systems that use FAISS, Chroma, Weaviate, etc.\n",
    "\n",
    "#### When might FAISS or a vector database be worth exploring?\n",
    "\n",
    "For small‚Äìto‚Äìmedium experiments (a few thousand to maybe tens of thousands of chunks), this \"plain NumPy + cosine\n",
    "similarity\" approach is usually enough. You might consider FAISS or a full vector DB when:\n",
    "\n",
    "- **Your corpus gets big**  \n",
    "  Once you‚Äôre in the hundreds of thousands to millions of chunks, brute-force similarity search can become slow\n",
    "  and memory-hungry. FAISS and friends provide *approximate nearest neighbor* search that scales much better.\n",
    "\n",
    "- **You need low-latency, repeated queries**  \n",
    "  If many users (or a web app) will hit your RAG system concurrently, you‚Äôll want:\n",
    "  - fast indexing,\n",
    "  - efficient caching, and\n",
    "  - sub-second query latency.  \n",
    "  Vector DBs are designed for this use case.\n",
    "\n",
    "- **You need rich filtering or metadata search**  \n",
    "  Vector DBs often support:\n",
    "  - filtering by metadata (e.g., `paper = \"chung2025\"`, `year > 2021`),\n",
    "  - combining keyword + vector search (‚Äúhybrid search‚Äù),\n",
    "  - role-based access control and multi-tenant setups.\n",
    "\n",
    "- **You want to share an index across services**  \n",
    "  If multiple notebooks, microservices, or teams need to reuse the **same embedding index**, a shared FAISS index or\n",
    "  hosted vector DB is much easier to manage than passing around `.npy` files.\n",
    "\n",
    "- **You need GPU-accelerated or distributed search**  \n",
    "  FAISS can use GPUs and sharding to speed up search on very large embedding collections. This is overkill for our\n",
    "  teaching demo (and the Wattbot project in general), but very relevant for production-scale systems.\n",
    "\n",
    "In this episode we deliberately stick with a simple in-memory index so the retrieval step is easy to inspect and\n",
    "debug. In later episodes (or your own projects), you can **swap out the retrieval layer** for FAISS or a vector DB\n",
    "without changing the overall RAG architecture: the model still sees ‚Äútop‚Äìk retrieved chunks‚Äù as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade795ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute cosine similarity between rows of a and rows of b.\"\"\"\n",
    "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
    "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
    "    return np.dot(a_norm, b_norm.T)\n",
    "\n",
    "def retrieve_top_k(\n",
    "    query_embedding: np.ndarray,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunked_docs: List[Dict[str, Any]],\n",
    "    k: int = 5,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return top-k most similar chunks for a query embedding.\"\"\"\n",
    "    if chunk_embeddings.shape[0] == 0:\n",
    "        return []\n",
    "\n",
    "    # query_embedding is 1D (D,)\n",
    "    sims = cosine_similarity_matrix(query_embedding.reshape(1, -1), chunk_embeddings)[0]\n",
    "    top_idx = np.argsort(-sims)[:k]\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for idx in top_idx:\n",
    "        doc = chunked_docs[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"score\": float(sims[idx]),\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"doc_id\": doc[\"doc_id\"],\n",
    "                \"page_num\": doc[\"page_num\"],\n",
    "                \"title\": doc[\"title\"],\n",
    "                \"url\": doc[\"url\"],\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check for `retrieve_top_k` on the first training question\n",
    "first_row = train_df.iloc[0]\n",
    "test_question = first_row[\"question\"]\n",
    "print(\"Sample question:\", test_question)\n",
    "\n",
    "test_q_emb = embedder.encode(\n",
    "    [test_question],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")[0]\n",
    "\n",
    "test_retrieved = retrieve_top_k(\n",
    "    query_embedding=test_q_emb,\n",
    "    chunk_embeddings=chunk_embeddings,\n",
    "    chunked_docs=chunked_docs,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "print(f\"Top {len(test_retrieved)} retrieved chunks:\")\n",
    "for r in test_retrieved:\n",
    "    snippet = r[\"text\"].replace(\"\\n\", \" \")\n",
    "    if len(snippet) > 160:\n",
    "        snippet = snippet[:160] + \"...\"\n",
    "    print(f\"- score={r['score']:.3f} | doc_id={r['doc_id']} | page={r['page_num']} | snippet={snippet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e41080",
   "metadata": {},
   "source": [
    "### 7. Load the Qwen model for answer generation\n",
    "\n",
    "For this episode we use **Qwen2.5-7B-Instruct** via the Hugging Face `transformers` library.\n",
    "\n",
    "- Parameter count: ~7 billion.\n",
    "- VRAM needs: ~14‚Äì16 GB in bfloat16 / 4-bit; fine for `ml.g5.xlarge` or a similar single-GPU instance.\n",
    "- Intended use here: short, grounded answers plus a normalized `answer_value`.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Call Qwen once to propose an answer and supporting evidence.\n",
    "2. Call Qwen a **second time** with a smaller prompt to generate a short explanation (<= 100 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00192b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "qwen_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "use_cuda_for_llm = torch.cuda.is_available()\n",
    "print(\"CUDA available for LLM:\", use_cuda_for_llm)\n",
    "\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(qwen_model_id)\n",
    "\n",
    "if use_cuda_for_llm:\n",
    "    llm_dtype = torch.bfloat16\n",
    "    model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "        qwen_model_id,\n",
    "        dtype=llm_dtype,\n",
    "        device_map=None,  # load on a single GPU\n",
    "    ).to(\"cuda\")\n",
    "    generation_device = 0\n",
    "else:\n",
    "    llm_dtype = torch.float32\n",
    "    model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "        qwen_model_id,\n",
    "        dtype=llm_dtype,\n",
    "        device_map=None,\n",
    "    )\n",
    "    generation_device = -1  # CPU\n",
    "\n",
    "qwen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    device=generation_device,\n",
    "    max_new_tokens=384,\n",
    ")\n",
    "\n",
    "def call_qwen_chat(system_prompt: str, user_prompt: str, max_new_tokens: int = 384) -> str:\n",
    "    \"\"\"Use Qwen chat template and return only the newly generated text.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    prompt_text = tokenizer_qwen.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = qwen_pipe(\n",
    "        prompt_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    full = outputs[0][\"generated_text\"]\n",
    "    generated = full[len(prompt_text):]\n",
    "    return generated.strip()\n",
    "\n",
    "print(\"Generator model and helper loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3dc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check for `call_qwen_chat`\n",
    "test_system_prompt = \"You are a concise assistant who answers simple questions clearly.\"\n",
    "test_user_prompt = \"What is 2 + 2? Answer in one short sentence.\"\n",
    "\n",
    "test_response = call_qwen_chat(\n",
    "    system_prompt=test_system_prompt,\n",
    "    user_prompt=test_user_prompt,\n",
    "    max_new_tokens=32,\n",
    ")\n",
    "print(f\"Generator ({qwen_model_id}) test response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615e6dc",
   "metadata": {},
   "source": [
    "### 8. Build prompts for answers and explanations\n",
    "\n",
    "We keep the prompts **very explicit**:\n",
    "\n",
    "- The first call asks Qwen to return JSON with:\n",
    "  - `answer` (short text),\n",
    "  - `answer_value` (normalized scalar or category),\n",
    "  - `ref_id` (comma‚Äëseparated doc ids, e.g. `\"jegham2025\"`),\n",
    "  - `supporting_material` (short quote or paraphrase).\n",
    "\n",
    "- The second call asks Qwen to generate a **single sentence explanation** (<= 100 characters).\n",
    "  We will prepend an evidence type tag (e.g. `[text]` or `[text+table]`) in code rather than\n",
    "  asking the model to output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b27c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_prompt(retrieved_chunks):\n",
    "    \"\"\"Format retrieved chunks so the LLM can see where text came from.\"\"\"\n",
    "    blocks = []\n",
    "    for r in retrieved_chunks:\n",
    "        header = f\"[DOC {r['doc_id']} | page {r['page_num']} | score {r['score']:.3f}]\"\n",
    "        blocks.append(header + \"\\n\" + r[\"text\"])\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "explanation_system_prompt = (\n",
    "    \"You are helping annotate how an answer is supported by a research paper. \"\n",
    "    \"You will see a question, an answer, and the supporting text used. \"\n",
    "    \"Your job is to (1) choose the MAIN type of evidence and \"\n",
    "    \"(2) give a VERY short explanation (<= 100 characters). \"\n",
    "    \"Valid evidence types are: text, figure, table, text+figure, table+figure, etc. \"\n",
    "    \"Respond in the strict format: evidence_type: explanation\"\n",
    ")\n",
    "\n",
    "def build_explanation_prompt(question, answer, supporting_materials, ref_id_list):\n",
    "    ref_str = \", \".join(ref_id_list) if ref_id_list else \"unknown\"\n",
    "    return f\"\"\"Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Supporting materials:\n",
    "{supporting_materials}\n",
    "\n",
    "Cited document ids: {ref_str}\n",
    "\n",
    "Remember:\n",
    "- evidence_type in [text, figure, table, text+figure, table+figure, etc.]\n",
    "- explanation <= 100 characters\n",
    "- Format: evidence_type: explanation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927b46a",
   "metadata": {},
   "source": [
    "### 9. Run over the full WattBot training set\n",
    "\n",
    "Now we:\n",
    "\n",
    "1. Iterate over **all** questions in `train_QA.csv`.\n",
    "2. Retrieve the top-\\(k\\) chunks for each question.\n",
    "3. Ask Qwen for an answer proposal (JSON).\n",
    "4. Derive:\n",
    "   - `answer` and `answer_value` from the JSON,\n",
    "   - `answer_unit` **copied directly from the ground truth** (never guessed),\n",
    "   - `ref_id` from the JSON,\n",
    "   - `ref_url` by mapping `ref_id` to `metadata.csv`,\n",
    "   - `supporting_material` from the JSON,\n",
    "   - `evidence_type` from the supporting text,\n",
    "   - `explanation` via a second Qwen call, prefixed with `[evidence_type]`.\n",
    "5. Save `wattbot_solutions.csv` in the project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from decimal import Decimal\n",
    "\n",
    "def normalize_answer_value(raw_answer_value, answer_text, answer_unit, is_blank):\n",
    "    \"\"\"\n",
    "    Normalize answer_value into the conventions used by train_QA:\n",
    "      - 'is_blank' for unanswerable questions\n",
    "      - plain numeric strings without units, commas, or scientific notation\n",
    "      - booleans as 1/0\n",
    "      - categorical strings (e.g., 'ML.ENERGY Benchmark') unchanged\n",
    "      - ranges like '[0.02,0.1]' preserved as-is\n",
    "    \"\"\"\n",
    "    s = str(raw_answer_value).strip()\n",
    "    if is_blank:\n",
    "        return \"is_blank\"\n",
    "    if not s or s.lower() == \"is_blank\":\n",
    "        return \"is_blank\"\n",
    "\n",
    "    # Preserve ranges like [0.02,0.1]\n",
    "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "        return s\n",
    "\n",
    "    lower = s.lower()\n",
    "\n",
    "    # Booleans -> 1/0\n",
    "    if lower in {\"true\", \"false\"}:\n",
    "        return \"1\" if lower == \"true\" else \"0\"\n",
    "\n",
    "    # Pure categorical (no digits) -> leave as-is\n",
    "    if not any(ch.isdigit() for ch in s):\n",
    "        return s\n",
    "\n",
    "    # Try to extract the first numeric token from either the raw string or the answer text\n",
    "    txt_candidates = [s, str(answer_text)]\n",
    "    match = None\n",
    "    for txt in txt_candidates:\n",
    "        if not txt:\n",
    "            continue\n",
    "        match = re.search(r\"[-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?\", str(txt).replace(\",\", \"\"))\n",
    "        if match:\n",
    "            break\n",
    "\n",
    "    if not match:\n",
    "        # Fallback: strip obvious formatting characters\n",
    "        cleaned = s.replace(\",\", \"\").replace(\"%\", \"\").strip()\n",
    "        return cleaned or \"is_blank\"\n",
    "\n",
    "    num_str = match.group(0)\n",
    "\n",
    "    # Format without scientific notation, trim trailing zeros\n",
    "    try:\n",
    "        d = Decimal(num_str)\n",
    "        normalized = format(d.normalize(), \"f\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            f = float(num_str)\n",
    "            normalized = (\"%.15f\" % f).rstrip(\"0\").rstrip(\".\")\n",
    "        except Exception:\n",
    "            normalized = num_str\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd45265",
   "metadata": {},
   "source": [
    "### Running the full RAG pipeline for one question\n",
    "\n",
    "At this point we have all the building blocks we need:\n",
    "\n",
    "- an **embedder** to turn questions into vectors  \n",
    "- a **retriever** (`retrieve_top_k`) to grab the most relevant text chunks  \n",
    "- a **chat helper** (`call_qwen_chat`) to talk to Qwen and get JSON back  \n",
    "- a small post-processing helper (`normalize_answer_value`) to clean numbers\n",
    "\n",
    "In the next few cells we tie these pieces together. We keep the code split into\n",
    "small helper functions so learners can follow each step:\n",
    "\n",
    "1. Retrieve context for a question.  \n",
    "2. Ask the LLM for an answer, references, and a quote.  \n",
    "3. Clean and normalize the structured fields (answer_value, ref_id, is_blank).  \n",
    "4. Ask a second LLM call for a short explanation and evidence type.\n",
    "\n",
    "\n",
    "### üîç Retrieving Relevant Context\n",
    "This function embeds the question and fetches the top‚ÄêK most relevant text chunks. It‚Äôs the first step of the RAG pipeline and determines what evidence the LLM can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lookup from document id -> URL using metadata\n",
    "docid_to_url = {\n",
    "    str(row[\"id\"]).strip(): row[\"url\"]\n",
    "    for _, row in metadata_df.iterrows()\n",
    "    if isinstance(row.get(\"url\", None), str)\n",
    "}\n",
    "\n",
    "def retrieve_context_for_question(question, embedder, chunk_embeddings, chunked_docs, top_k: int = 8):\n",
    "    \"\"\"Embed the question and retrieve the top-k most similar chunks.\"\"\"\n",
    "    q_emb = embedder.encode(\n",
    "        [question],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "    )[0]\n",
    "    retrieved = retrieve_top_k(q_emb, chunk_embeddings, chunked_docs, k=top_k)\n",
    "    context = format_context_for_prompt(retrieved)\n",
    "    return retrieved, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b6bc7",
   "metadata": {},
   "source": [
    "### First LLM Step: Producing an Answer\n",
    "Here we prompt the model to:\n",
    "- Decide if the question is answerable\n",
    "- Extract a numeric/categorical answer\n",
    "- Identify supporting evidence\n",
    "- Select relevant document IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_phase_for_question(qid, question, answer_unit, context):\n",
    "    \"\"\"\n",
    "    First LLM call: ask Qwen for an answer, answer_value, is_blank, ref_ids,\n",
    "    and a short supporting quote. Then normalize these fields.\n",
    "    \"\"\"\n",
    "    # System prompt: what role Qwen should play\n",
    "    system_prompt_answer = (\n",
    "        \"You answer questions about AI energy, carbon, and water from academic papers.\\n\"\n",
    "        \"You are given:\\n\"\n",
    "        \"- a question\\n\"\n",
    "        \"- retrieved text chunks from the relevant paper(s)\\n\\n\"\n",
    "        \"You must:\\n\"\n",
    "        \"1. Decide if the question can be answered from the provided context.\\n\"\n",
    "        \"2. If answerable, extract a concise numeric or short-text answer_value.\\n\"\n",
    "        \"3. Use the provided answer_unit EXACTLY as given (do NOT invent units).\\n\"\n",
    "        \"4. Select one or more document ids as ref_id from the supplied chunks.\\n\"\n",
    "        \"5. Copy a short supporting quote (<= 300 chars) into supporting_materials.\\n\"\n",
    "        \"6. If the context is insufficient, mark is_blank = true and set all\\n\"\n",
    "        \"   other fields to 'is_blank' except answer_unit (keep it as provided).\\n\"\n",
    "        \"Return a JSON object with fields:\\n\"\n",
    "        \"  answer (string)\\n\"\n",
    "        \"  answer_value (string)\\n\"\n",
    "        \"  is_blank (true or false)\\n\"\n",
    "        \"  ref_id (list of doc_id strings)\\n\"\n",
    "        \"  supporting_materials (string)\\n\"\n",
    "    )\n",
    "\n",
    "    context_block = context if context.strip() else \"[NO CONTEXT FOUND]\"\n",
    "\n",
    "    # User prompt: question + unit hint + retrieved context\n",
    "    user_prompt_answer = f\"\"\"Question: {question}\n",
    "Expected answer unit: {answer_unit}\n",
    "\n",
    "Retrieved context:\n",
    "{context_block}\n",
    "\n",
    "Return JSON ONLY with keys:\n",
    "  answer (string)\n",
    "  answer_value (string)\n",
    "  is_blank (true or false)\n",
    "  ref_id (list of doc_id strings)\n",
    "  supporting_materials (string)\n",
    "\"\"\"\n",
    "\n",
    "    raw_answer = call_qwen_chat(system_prompt_answer, user_prompt_answer, max_new_tokens=384)\n",
    "\n",
    "    # Try to parse JSON from the model output\n",
    "    parsed = {\n",
    "        \"answer\": \"\",\n",
    "        \"answer_value\": \"is_blank\",\n",
    "        \"is_blank\": True,\n",
    "        \"ref_id\": [],\n",
    "        \"supporting_materials\": \"is_blank\",\n",
    "    }\n",
    "    try:\n",
    "        first_brace = raw_answer.find(\"{\")\n",
    "        last_brace = raw_answer.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            json_str = raw_answer[first_brace : last_brace + 1]\n",
    "        else:\n",
    "            json_str = raw_answer\n",
    "        candidate = json.loads(json_str)\n",
    "        if isinstance(candidate, dict):\n",
    "            parsed.update(candidate)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parse error for question {qid}: {e}\")\n",
    "        # fall back to defaults in `parsed`\n",
    "\n",
    "    # Normalize parsed fields\n",
    "    is_blank = bool(parsed.get(\"is_blank\", False))\n",
    "    ref_ids = parsed.get(\"ref_id\") or []\n",
    "    if isinstance(ref_ids, str):\n",
    "        ref_ids = [ref_ids]\n",
    "    ref_ids = [str(r).strip() for r in ref_ids if str(r).strip()]\n",
    "\n",
    "    answer = str(parsed.get(\"answer\", \"\")).strip()\n",
    "    answer_value = str(parsed.get(\"answer_value\", \"\")).strip() or \"is_blank\"\n",
    "    answer_value = normalize_answer_value(\n",
    "        raw_answer_value=answer_value,\n",
    "        answer_text=answer,\n",
    "        answer_unit=answer_unit,\n",
    "        is_blank=is_blank,\n",
    "    )\n",
    "    supporting_materials = str(parsed.get(\"supporting_materials\", \"\")).strip()\n",
    "\n",
    "    # If context is empty or model says blank, force is_blank behaviour\n",
    "    if not context.strip() or is_blank:\n",
    "        is_blank = True\n",
    "        answer = \"\"\n",
    "        answer_value = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        supporting_materials = \"is_blank\"\n",
    "\n",
    "    # String formatting for ref_id / ref_url to match training style\n",
    "    if not ref_ids:\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "    else:\n",
    "        ref_id_str = str(ref_ids)\n",
    "\n",
    "        # Resolve ref_url via metadata\n",
    "        ref_url = \"is_blank\"\n",
    "        for rid in ref_ids:\n",
    "            if rid in docid_to_url:\n",
    "                ref_url = docid_to_url[rid]\n",
    "                break\n",
    "        if not ref_url:\n",
    "            ref_url = \"is_blank\"\n",
    "        ref_url_str = str([ref_url])\n",
    "\n",
    "    return answer, answer_value, is_blank, ref_ids, supporting_materials, ref_id_str, ref_url_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fbdb0",
   "metadata": {},
   "source": [
    "### Second LLM Step: Explanation and Evidence Type\n",
    "Now that we have an answer, we produce a short explanation and classify the evidence type. This step matches WattBot‚Äôs expected metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_phase_for_question(question, answer, supporting_materials, ref_ids, is_blank):\n",
    "    \"\"\"\n",
    "    Second LLM call: ask Qwen to label an evidence_type and provide a short\n",
    "    explanation tying the answer back to the supporting materials.\n",
    "    \"\"\"\n",
    "    if is_blank:\n",
    "        # For unanswerable questions we just propagate a sentinel.\n",
    "        evidence_type = \"other\"\n",
    "        explanation = \"is_blank\"\n",
    "        return evidence_type, explanation\n",
    "\n",
    "    expl_user_prompt = build_explanation_prompt(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        supporting_materials=supporting_materials,\n",
    "        ref_id_list=ref_ids,\n",
    "    )\n",
    "    raw_expl = call_qwen_chat(\n",
    "        explanation_system_prompt,\n",
    "        expl_user_prompt,\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "\n",
    "    # Take the first non-empty line (we expect something like \"text: short reason\")\n",
    "    first_line = \"\"\n",
    "    for line in raw_expl.splitlines():\n",
    "        if line.strip():\n",
    "            first_line = line.strip()\n",
    "            break\n",
    "\n",
    "    if \":\" in first_line:\n",
    "        etype, expl = first_line.split(\":\", 1)\n",
    "        evidence_type = etype.strip().lower() or \"other\"\n",
    "        explanation = expl.strip()\n",
    "    else:\n",
    "        evidence_type = \"other\"\n",
    "        explanation = first_line.strip() or \"short justification\"\n",
    "\n",
    "    # Keep explanations short for the CSV\n",
    "    if len(explanation) > 100:\n",
    "        explanation = explanation[:100]\n",
    "\n",
    "    return evidence_type, explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040b9ee",
   "metadata": {},
   "source": [
    "###  Orchestration: `run_single_qa`\n",
    "This high‚Äêlevel function ties together retrieval, answering, normalization, and explanation into one full pass over a single question.\n",
    "\n",
    "\n",
    "\n",
    "### Handling unanswerable questions\n",
    "\n",
    "Some WattBot questions truly **cannot** be answered from the retrieved papers.  \n",
    "We use a simple hybrid rule to detect these cases:\n",
    "\n",
    "- We look at the **top retrieval score** (cosine similarity).  \n",
    "- We also use the LLM's own `is_blank` flag from the first JSON response.  \n",
    "\n",
    "If **either** of these says the evidence is too weak, we mark the question as unanswerable\n",
    "and set all relevant fields (`answer_value`, `ref_id`, `supporting_materials`) to `is_blank`.\n",
    "\n",
    "The `THRESHOLD` inside `run_single_qa` controls how strict this behaviour is:\n",
    "\n",
    "- lower values ‚Üí fewer questions marked unanswerable  \n",
    "- higher values ‚Üí more questions marked unanswerable  \n",
    "\n",
    "You can change `THRESHOLD` and then re-run the notebook and `Score.py` to see\n",
    "how this trade-off affects your final WattBot score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2877e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_qa(\n",
    "    row,\n",
    "    embedder,\n",
    "    chunk_embeddings,\n",
    "    chunked_docs,\n",
    "    top_k: int = 8,\n",
    "):\n",
    "    \"\"\"Run retrieval + Qwen for a single training QA row.\n",
    "\n",
    "    This is the high-level orchestration function that calls three helpers:\n",
    "\n",
    "    1. retrieve_context_for_question  -> get relevant text chunks\n",
    "    2. answer_phase_for_question      -> generate answer from context, returning citations and supporting materials\n",
    "    3. explanation_phase_for_question -> evidence type + short explanation\n",
    "    \"\"\"\n",
    "\n",
    "    # Confidence threshold for retrieval.\n",
    "    # If the top similarity score is below this value, we treat the question\n",
    "    # as unanswerable, even if the LLM tried to produce an answer.\n",
    "    THRESHOLD = 0.25\n",
    "\n",
    "    qid = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "    answer_unit = row.get(\"answer_unit\", \"\")\n",
    "\n",
    "    # 1. Retrieval step\n",
    "    retrieved, context = retrieve_context_for_question(\n",
    "        question=question,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    top_score = retrieved[0][\"score\"] if retrieved else 0.0\n",
    "\n",
    "    # 2. Answer + refs + supporting materials (LLM's view)\n",
    "    (\n",
    "        answer,\n",
    "        answer_value,\n",
    "        is_blank_llm,\n",
    "        ref_ids,\n",
    "        supporting_materials,\n",
    "        ref_id_str,\n",
    "        ref_url_str,\n",
    "    ) = answer_phase_for_question(\n",
    "        qid=qid,\n",
    "        question=question,\n",
    "        answer_unit=answer_unit,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    # Hybrid is_blank decision:\n",
    "    # - if retrieval is weak (top_score < THRESHOLD)\n",
    "    # - OR the LLM marks is_blank = true\n",
    "    # then we treat the question as unanswerable.\n",
    "    is_blank = bool(is_blank_llm) or (top_score < THRESHOLD)\n",
    "\n",
    "    if is_blank:\n",
    "        answer = \"\"\n",
    "        answer_value = \"is_blank\"\n",
    "        ref_ids = []\n",
    "        ref_id_str = \"is_blank\"\n",
    "        ref_url_str = \"is_blank\"\n",
    "        supporting_materials = \"is_blank\"\n",
    "\n",
    "    # Always copy answer_unit from train_QA.csv (do NOT let the LLM invent it)\n",
    "    answer_unit = row.get(\"answer_unit\", \"\")\n",
    "\n",
    "    # 3. Explanation + evidence_type\n",
    "    evidence_type, explanation = explanation_phase_for_question(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        supporting_materials=supporting_materials,\n",
    "        ref_ids=ref_ids,\n",
    "        is_blank=is_blank,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"answer_value\": answer_value,\n",
    "        \"answer_unit\": answer_unit,\n",
    "        \"is_blank\": \"true\" if is_blank else \"false\",\n",
    "        \"ref_id\": ref_id_str,\n",
    "        \"ref_url\": ref_url_str,\n",
    "        \"supporting_materials\": supporting_materials,\n",
    "        \"evidence_type\": evidence_type,\n",
    "        \"explanation\": explanation,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31901afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Run over max_N training questions (this can take a while!)\n",
    "# -------------------------------------------------------------------\n",
    "all_results = []\n",
    "error_log = []\n",
    "max_N = np.inf\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    if idx >= max_N:\n",
    "        break\n",
    "    question = row[\"question\"]\n",
    "    print(f\"########################################################################################################\\nQUESTION: {question}\")\n",
    "\n",
    "    res = run_single_qa(\n",
    "        row=row,\n",
    "        embedder=embedder,\n",
    "        chunk_embeddings=chunk_embeddings,\n",
    "        chunked_docs=chunked_docs,\n",
    "        top_k=8,\n",
    "    )\n",
    "    answer = res[\"answer\"]\n",
    "    ref_ids = res[\"ref_id\"]\n",
    "\n",
    "    explanation = res[\"explanation\"]\n",
    "    print(f\"ANSWER: {answer}\")\n",
    "    print(f\"ref_ids: {ref_ids}\")\n",
    "    print(f\"EXPLANATION: {explanation}\")\n",
    "    \n",
    "    all_results.append(res)\n",
    "\n",
    "solutions_df = pd.DataFrame(all_results)\n",
    "solutions_path = os.path.join(local_data_dir, \"train_solutions_qwen.csv\")\n",
    "solutions_df.to_csv(solutions_path, index=False)\n",
    "\n",
    "print(f\"Saved solutions for {len(solutions_df)} questions to: {solutions_path}\")\n",
    "print(f\"Number of questions with errors (filled as blank): {len(error_log)}\")\n",
    "\n",
    "solutions_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca08d56",
   "metadata": {},
   "source": [
    "### Compare answers to ground truth\n",
    "\n",
    "WattBot evaluates each prediction using a weighted score that combines three components. Most of the credit (0.75) comes from the `answer_value`, which must match the ground truth after normalization (numeric answers must be within ¬±0.1% relative tolerance; categorical values must match exactly). An additional 0.15 comes from `ref_id`, where partial credit is given based on the Jaccard overlap between your cited document IDs and the ground-truth set. The final 0.10 comes from correctly marking unanswerable questions: if a question is truly unanswerable, you must set `answer_value`, `ref_id`, and `supporting_materials` to `is_blank`. Any other combination scores zero for this component.\n",
    "\n",
    "| Component      | Weight | What counts as correct |\n",
    "|----------------|--------|------------------------|\n",
    "| answer_value   | 0.75   | Numeric within ¬±0.1% relative tolerance; categorical exact match; `is_blank` if unanswerable |\n",
    "| ref_id         | 0.15   | Jaccard overlap with the ground-truth reference set (case-insensitive) |\n",
    "| is_NA          | 0.10   | All required fields set to `is_blank` when the question is truly unanswerable |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"True\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"False\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",   # can also pass \"is_blank\" or None\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,    # can pass \"is_blank\", or leave None to auto\n",
    "    n_examples=10,          # how many incorrect examples to print\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "\n",
    "    NA logic:\n",
    "    - If an explicit NA column is found/used (e.g. is_NA), we use it via _to_bool_flag.\n",
    "    - If you pass gt_is_na_col=\"is_blank\" or pred_is_na_col=\"is_blank\",\n",
    "      we *derive* NA from answer_value == \"is_blank\" instead of expecting a real column.\n",
    "    - If no NA column is available at all, we derive from answer_value == \"is_blank\".\n",
    "\n",
    "    Also prints up to `n_examples` rows where the model is not perfect\n",
    "    (answer_score < 1, ref_id_score < 1, or is_NA_score < 1).\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "\n",
    "    # ----- ground truth NA flags -----\n",
    "    if gt_is_na_col is not None and gt_is_na_col in merged.columns:\n",
    "        # Use explicit column (e.g. \"is_NA\")\n",
    "        gt_is_na_series = merged[gt_is_na_col].map(_to_bool_flag)\n",
    "    elif gt_is_na_col is not None and gt_is_na_col.lower() == \"is_blank\":\n",
    "        # Special meaning: derive NA from answer_value_gt == \"is_blank\"\n",
    "        gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "    else:\n",
    "        # Fallback: if we have is_NA or is_blank col, use it; else derive\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "\n",
    "    # ----- prediction NA flags -----\n",
    "    if pred_is_na_col is not None and pred_is_na_col in merged.columns:\n",
    "        pred_is_na_series = merged[pred_is_na_col].map(_to_bool_flag)\n",
    "    elif pred_is_na_col is not None and pred_is_na_col.lower() == \"is_blank\":\n",
    "        # Same convention: derive from answer_value_pred\n",
    "        pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "    else:\n",
    "        # Auto-detect or derive if no NA column in preds\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "\n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = bool(gt_is_na_series.iloc[idx])\n",
    "        pred_is_na = bool(pred_is_na_series.iloc[idx])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    # ----- Show some incorrect examples -----\n",
    "    incorrect = merged[\n",
    "        (merged[\"answer_score\"] < 1.0)\n",
    "        | (merged[\"ref_id_score\"] < 1.0)\n",
    "        | (merged[\"is_NA_score\"] < 1.0)\n",
    "    ]\n",
    "    \n",
    "    if not incorrect.empty and n_examples > 0:\n",
    "        print(\"\\nExamples of incorrect / partially correct responses \"\n",
    "              f\"(up to {n_examples} rows):\\n\")\n",
    "        # Grab up to n_examples \"worst\" rows by wattbot_score\n",
    "        for _, row in incorrect.sort_values(\"wattbot_score\").head(n_examples).iterrows():\n",
    "            q = row[\"question_gt\"] if \"question_gt\" in row.index else None\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"id: {row[id_col]}\")\n",
    "            if q is not None:\n",
    "                print(f\"Question: {q}\")\n",
    "            print(f\"GT answer_value:   {row[f'{gt_answer_col}_gt']}\")\n",
    "            print(f\"Pred answer_value: {row[f'{pred_answer_col}_pred']}\")\n",
    "            print(f\"GT ref_id:         {row[f'{gt_ref_col}_gt']}\")\n",
    "            print(f\"Pred ref_id:       {row[f'{pred_ref_col}_pred']}\")\n",
    "            print(f\"answer_score: {row['answer_score']:.3f}, \"\n",
    "                  f\"ref_id_score: {row['ref_id_score']:.3f}, \"\n",
    "                  f\"is_NA_score: {row['is_NA_score']:.3f}, \"\n",
    "                  f\"wattbot_score: {row['wattbot_score']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fd7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=\"./data/train_solutions_qwen.csv\",\n",
    "    gt_is_na_col=\"is_blank\",   # or \"is_blank\" / None depending on how you mark NAs\n",
    "    n_examples=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9da22",
   "metadata": {},
   "source": [
    "## Recap and next steps\n",
    "\n",
    "In this episode, we:\n",
    "\n",
    "- Loaded a small corpus of AI / ML energy papers into our notebook environment.\n",
    "- Split long documents into manageable chunks and cached those chunks to disk so we don‚Äôt have to re-run the chunking step every time.\n",
    "- Created vector embeddings for each chunk and used similarity search to retrieve relevant context for a given question.\n",
    "- Used an LLM to generate answers from retrieved context and wrote results out to a CSV for later scoring and analysis.\n",
    "- Handled unanswerable questions with an `is_blank` flag so the system can explicitly say ‚ÄúI don‚Äôt know‚Äù when the evidence isn‚Äôt there.\n",
    "\n",
    "This is just a first pass at a RAG pipeline: it works, but there‚Äôs a lot of headroom to improve both accuracy and robustness. Some natural next steps:\n",
    "\n",
    "- **Increase the size/quality of models used for embedding and generation**: Try stronger embedding models (e.g., larger sentence-transformers or domain-tuned embeddings) and more capable LLMs for answer generation, especially if you have GPU budget.\n",
    "\n",
    "- **Add a reranking step**: Instead of sending the top-k raw nearest neighbors directly to the LLM, use a cross-encoder or reranker model to re-score those candidates and send only the best ones.\n",
    "\n",
    "- **Handle figures and tables more carefully**: Many key numbers live in tables, figure captions, or plots. Consider:\n",
    "  - OCR / table-parsing tools (e.g., `pytesseract`, table extractors, PDF parsers).\n",
    "  - Multimodal models that can embed or interpret figures and diagrams, not just text.\n",
    "  - Separate chunking strategies for captions, tables, and main text.\n",
    "\n",
    "- **Enrich chunks with metadata**: Attach metadata like section headings (e.g., *Methods*, *Results*), paper ID, year, or paragraph type. You can:\n",
    "  - Filter or boost chunks by metadata at retrieval time.\n",
    "  - Use metadata in the prompt so the LLM knows where evidence is coming from.\n",
    "\n",
    "- **Look for LLMs tuned for scientific literature**: Experiment with models that are explicitly trained or finetuned on scientific text (e.g., arXiv / PubMed) so they:\n",
    "  - Parse equations and technical language more reliably.\n",
    "  - Are less likely to hallucinate when reading dense scientific prose.\n",
    "\n",
    "As you iterate, the goal is to treat this notebook as a baseline RAG ‚Äúworkbench‚Äù: you can swap in better models, smarter retrieval strategies, and richer document preprocessing without changing the overall pipeline structure.\n",
    "\n",
    "In the next episodes, we will repeat largely the same exact RAG pipeline using slightly different approaches on AWS (processing jobs and Bedrock).\n",
    "\n",
    "\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- **Notebook setup**: Start by provisioning a GPU-backed notebook instance\n",
    "  (e.g., `ml.g5.xlarge`) so that both the embedding model and Qwen2.5-7B\n",
    "  can run comfortably.\n",
    "- **Local-first RAG**: For teaching (and small corpora), we avoid an external vector database\n",
    "  and instead perform cosine similarity search over in-memory embeddings.\n",
    "- **Ground-truth units**: The `answer_unit` column is always copied directly\n",
    "  from `train_QA.csv`, never guessed by the LLM.\n",
    "- **Two-stage LLM use**: One call focuses on *answering and citing*; a second,\n",
    "  lighter call produces a short explanation tagged with an evidence type.\n",
    "- **WattBot conventions**: We respect the Kaggle competition format,\n",
    "  using `is_blank` for unanswerable questions and for missing fields.\n",
    "- **Scalability path**: The same logic can later be swapped to FAISS/Chroma\n",
    "  and larger models, while preserving the interface used here.\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
