{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c72c78",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with Amazon Bedrock\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "- What is the goal of this RAG workflow?\n",
    "- How do we run this RAG variant?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "- Understand this RAG approach.\n",
    "- Run the workflow end-to-end.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "# RAG with Amazon Bedrock (Embeddings + Generation)\n",
    "This notebook demonstrates a Bedrock-based RAG pipeline using Titan embeddings and Claude 3 Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364eeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, boto3, numpy as np, requests\n",
    "region=\"us-east-1\"\n",
    "bedrock=boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "EMBED_MODEL_ID=\"amazon.titan-embed-text-v2:0\"\n",
    "GEN_MODEL_ID=\"anthropic.claude-3-haiku-20240307-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110c88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.gutenberg.org/cache/epub/1112/pg1112.txt\"\n",
    "raw_text=requests.get(url).text\n",
    "\n",
    "def simple_paragraph_chunks(text,max_chars=800):\n",
    "    paras=[p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    chunks=[]; buf=\"\"\n",
    "    for p in paras:\n",
    "        if buf and len(buf)+len(p)+2>max_chars:\n",
    "            chunks.append(buf.strip()); buf=p\n",
    "        else:\n",
    "            buf=(buf+\"\\n\\n\"+p).strip() if buf else p\n",
    "    if buf: chunks.append(buf.strip())\n",
    "    return chunks\n",
    "\n",
    "chunks=simple_paragraph_chunks(raw_text,800)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts_bedrock(texts):\n",
    "    vecs=[]\n",
    "    for t in texts:\n",
    "        body=json.dumps({\"inputText\":t})\n",
    "        resp=bedrock.invoke_model(modelId=EMBED_MODEL_ID, body=body)\n",
    "        vecs.append(json.loads(resp[\"body\"].read())[\"embedding\"])\n",
    "    return np.array(vecs,dtype=\"float32\")\n",
    "\n",
    "chunk_embeddings=embed_texts_bedrock(chunks)\n",
    "chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(q,mat):\n",
    "    q=q/(np.linalg.norm(q)+1e-9)\n",
    "    m=mat/(np.linalg.norm(mat,axis=1,keepdims=True)+1e-9)\n",
    "    return m@q\n",
    "\n",
    "def retrieve_top_k(query,k=5):\n",
    "    qv=embed_texts_bedrock([query])[0]\n",
    "    sims=cosine_sim(qv,chunk_embeddings)\n",
    "    idx=np.argsort(-sims)[:k]\n",
    "    return [{\"index\":int(i),\"similarity\":float(sims[i]),\"text\":chunks[i]} for i in idx]\n",
    "\n",
    "retrieve_top_k(\"Who kills Mercutio?\",3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a33cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_bedrock_claude(query,k=5,temperature=0.0):\n",
    "    retrieved=retrieve_top_k(query,k)\n",
    "    ctx=\"\\n\\n---\\n\\n\".join([f\"[Chunk {r['index']}]\\n{r['text']}\" for r in retrieved])\n",
    "    system=\"You answer ONLY using the provided Shakespeare context.\"\n",
    "    user=(f\"Context:\\n{ctx}\\n\\nQuestion: {query}\")\n",
    "    body={\n",
    "        \"modelId\":GEN_MODEL_ID,\n",
    "        \"messages\":[{\"role\":\"user\",\"content\":[{\"text\":user}]}],\n",
    "        \"system\":[{\"text\":system}],\n",
    "        \"inferenceConfig\":{\"maxTokens\":256,\"temperature\":temperature}\n",
    "    }\n",
    "    resp=bedrock.converse(modelId=GEN_MODEL_ID,\n",
    "                          messages=body[\"messages\"],\n",
    "                          system=body[\"system\"],\n",
    "                          inferenceConfig=body[\"inferenceConfig\"])\n",
    "    out=resp[\"output\"][\"message\"][\"content\"]\n",
    "    return \"\\n\".join([c[\"text\"] for c in out if \"text\" in c])\n",
    "\n",
    "print(rag_answer_bedrock_claude(\"Who kills Mercutio?\",5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1296546",
   "metadata": {},
   "source": [
    "## Notes\n",
    "This notebook compares Bedrock-based RAG vs SageMaker training workflows.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "- This episode demonstrates a complete RAG workflow.\n",
    "- Components include retrieval, embedding, and generation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
