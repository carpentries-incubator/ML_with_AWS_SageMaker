{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392e12e5-8756-4172-bb3f-55b859d29416",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Accessing and Managing Data in S3 with SageMaker Notebooks\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can I load data from S3 into a SageMaker notebook?\n",
    "- How do I monitor storage usage and costs for my S3 bucket?\n",
    "- What steps are involved in pushing new data back to S3 from a notebook?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Read data directly from an S3 bucket into memory in a SageMaker notebook.\n",
    "- Check storage usage and estimate costs for data in an S3 bucket.\n",
    "- Upload new files from the SageMaker environment back to the S3 bucket.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb82e8e-439b-4339-85f5-b548e8d8a88f",
   "metadata": {},
   "source": [
    "#### Directory setup\n",
    "Let's make sure we're starting in the root directory of this instance, so that we all have our AWS_helpers.py file located in the same path (/test_AWS/scripts/AWS_helpers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636ef6a9-d818-46dc-91c7-e0bb67694049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n",
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/SageMaker/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04814a0-86ef-4268-a779-cd6dc62c5cf7",
   "metadata": {},
   "source": [
    "## 1A. Read data from S3 into memory\n",
    "Our data is stored on an S3 bucket called 'titanic-dataset-test'. We can use the following code to read data directly from S3 into memory in the Jupyter notebook environment, without actually downloading a copy of train.csv as a local file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbbf0be-ae55-4020-88ce-35dd4455d6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "(712, 12)\n",
      "(179, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>693</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Lam, Mr. Ali</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1601</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>482</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Frost, Mr. Anthony Wood \"Archie\"</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239854</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>528</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Farthing, Mr. John</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17483</td>\n",
       "      <td>221.7792</td>\n",
       "      <td>C95</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>856</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Aks, Mrs. Sam (Leah Rosen)</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>392091</td>\n",
       "      <td>9.3500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>802</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Collyer, Mrs. Harvey (Charlotte Annie Tate)</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C.A. 31921</td>\n",
       "      <td>26.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass                                         Name  \\\n",
       "0          693         1       3                                 Lam, Mr. Ali   \n",
       "1          482         0       2             Frost, Mr. Anthony Wood \"Archie\"   \n",
       "2          528         0       1                           Farthing, Mr. John   \n",
       "3          856         1       3                   Aks, Mrs. Sam (Leah Rosen)   \n",
       "4          802         1       2  Collyer, Mrs. Harvey (Charlotte Annie Tate)   \n",
       "\n",
       "      Sex   Age  SibSp  Parch      Ticket      Fare Cabin Embarked  \n",
       "0    male   NaN      0      0        1601   56.4958   NaN        S  \n",
       "1    male   NaN      0      0      239854    0.0000   NaN        S  \n",
       "2    male   NaN      0      0    PC 17483  221.7792   C95        S  \n",
       "3  female  18.0      0      1      392091    9.3500   NaN        S  \n",
       "4  female  31.0      1      1  C.A. 31921   26.2500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize the SageMaker role and session\n",
    "# Define the SageMaker role and session\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket and object key\n",
    "bucket_name = 'titanic-dataset-test'  # replace with your S3 bucket name\n",
    "\n",
    "# Read the train data from S3\n",
    "key = 'data/titanic_train.csv'  # replace with your object key\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "train_data = pd.read_csv(response['Body'])\n",
    "\n",
    "# Read the test data from S3\n",
    "key = 'data/titanic_test.csv'  # replace with your object key\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "test_data = pd.read_csv(response['Body'])\n",
    "\n",
    "# check shape\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# Inspect the first few rows of the DataFrame\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ecb6e1-c8ad-417a-932f-281b9f6aafeb",
   "metadata": {},
   "source": [
    "## 1B. Download copy into notebook environment\n",
    "If you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local\" copy (i.e., one stored in your notebook's instance).\n",
    "\n",
    "Download data from S3 to notebook environment. You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\n",
    "\n",
    "* check that you have selected the appropriate policy for this notebook\n",
    "* check that your bucket has the appropriate policy permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c8e06f-50f1-4f12-9b3d-32f0bdc711da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: ./titanic_train.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the S3 bucket and file location\n",
    "file_key = \"data/titanic_train.csv\"  # Path to your file in the S3 bucket\n",
    "local_file_path = \"./titanic_train.csv\"  # Local path to save the file\n",
    "\n",
    "# Initialize the S3 client and download the file\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(bucket_name, file_key, local_file_path)\n",
    "print(\"File downloaded:\", local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6e7f1-19f9-458c-b7e1-f2702d491cd1",
   "metadata": {},
   "source": [
    "## 2. Check current size and storage costs of bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a71377-900f-4314-9017-d36cb83b2d58",
   "metadata": {},
   "source": [
    "It's a good idea to periodically check how much storage you have used in your bucket. You can do this from a Jupyter notebook in SageMaker by using the **Boto3** library, which is the AWS SDK for Python. This will allow you to calculate the total size of objects within a specified bucket. Here’s how you can do it...\n",
    "\n",
    "### Step 1: Set Up the S3 Client and Calculate Bucket Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c2044c-acb4-48f5-b755-570d1cc4ecbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of bucket 'titanic-dataset-test': 41.04 MB\n"
     ]
    }
   ],
   "source": [
    "# Initialize the total size counter\n",
    "total_size_bytes = 0\n",
    "\n",
    "# List and sum the size of all objects in the bucket\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "for page in paginator.paginate(Bucket=bucket_name):\n",
    "    for obj in page.get('Contents', []):\n",
    "        total_size_bytes += obj['Size']\n",
    "\n",
    "# Convert the total size to gigabytes for cost estimation\n",
    "total_size_gb = total_size_bytes / (1024 ** 3)\n",
    "# print(f\"Total size of bucket '{bucket_name}': {total_size_gb:.2f} GB\") # can uncomment this if you want GB reported\n",
    "\n",
    "# Convert the total size to megabytes for readability\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "print(f\"Total size of bucket '{bucket_name}': {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588180ab-e807-4e0f-99c7-bf36570da467",
   "metadata": {},
   "source": [
    "We have added this code to a helper called `get_s3_bucket_size(bucket_name)` for your convenience. You can call this function via the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bbb057-b8cb-4d04-8cf9-7bc91c7caed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size_mb': 41.043779373168945, 'size_gb': 0.0400818157941103}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import test_AWS.scripts.AWS_helpers as helpers # test_AWS.scripts.AWS_helpers reflects path leading up to AWS_helpers.py\n",
    "\n",
    "helpers.get_s3_bucket_size(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe444fcd-a4aa-4536-aac0-df1c56a79594",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. **Paginator**: Since S3 buckets can contain many objects, we use a paginator to handle large listings.\n",
    "2. **Size Calculation**: We sum the `Size` attribute of each object in the bucket.\n",
    "3. **Unit Conversion**: The size is given in bytes, so dividing by `1024 ** 2` converts it to megabytes (MB).\n",
    "\n",
    "> **Note**: If your bucket has very large objects or you want to check specific folders within a bucket, you may want to refine this code to only fetch certain objects or folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf5a8c-8f28-42a8-be40-a6f155db96e6",
   "metadata": {},
   "source": [
    "## 3: Check storage costs of bucket\n",
    "To estimate the storage cost of your Amazon S3 bucket directly from a Jupyter notebook in SageMaker, you can use the following approach. This method calculates the total size of the bucket and estimates the monthly storage cost based on AWS S3 pricing.\n",
    "\n",
    "**Note**: AWS S3 pricing varies by region and storage class. The example below uses the S3 Standard storage class pricing for the US East (N. Virginia) region as of November 1, 2024. Please verify the current pricing for your specific region and storage class on the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999a7e66-04e3-4577-b7ab-4ca9472e3ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated monthly storage cost: $0.0009\n"
     ]
    }
   ],
   "source": [
    "# AWS S3 Standard Storage pricing for US East (N. Virginia) region\n",
    "# Pricing tiers as of November 1, 2024\n",
    "first_50_tb_price_per_gb = 0.023  # per GB for the first 50 TB\n",
    "next_450_tb_price_per_gb = 0.022  # per GB for the next 450 TB\n",
    "over_500_tb_price_per_gb = 0.021  # per GB for storage over 500 TB\n",
    "\n",
    "# Calculate the cost based on the size\n",
    "if total_size_gb <= 50 * 1024:\n",
    "    # Total size is within the first 50 TB\n",
    "    cost = total_size_gb * first_50_tb_price_per_gb\n",
    "elif total_size_gb <= 500 * 1024:\n",
    "    # Total size is within the next 450 TB\n",
    "    cost = (50 * 1024 * first_50_tb_price_per_gb) + \\\n",
    "           ((total_size_gb - 50 * 1024) * next_450_tb_price_per_gb)\n",
    "else:\n",
    "    # Total size is over 500 TB\n",
    "    cost = (50 * 1024 * first_50_tb_price_per_gb) + \\\n",
    "           (450 * 1024 * next_450_tb_price_per_gb) + \\\n",
    "           ((total_size_gb - 500 * 1024) * over_500_tb_price_per_gb)\n",
    "\n",
    "print(f\"Estimated monthly storage cost: ${cost:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a872b2-5a61-4503-925a-24222f8b1ecc",
   "metadata": {},
   "source": [
    "For your convenience, we have also added this code to a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0ec56b9-8c55-4c71-b682-aa0fae54a961",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated monthly storage cost for bucket 'titanic-dataset-test': $0.0009\n"
     ]
    }
   ],
   "source": [
    "import test_AWS.scripts.AWS_helpers as helpers # test_AWS.scripts.AWS_helpers reflects path leading up to AWS_helpers.py\n",
    "\n",
    "cost = helpers.calculate_s3_storage_cost(bucket_name)\n",
    "\n",
    "print(f\"Estimated monthly storage cost for bucket '{bucket_name}': ${cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0198b8-3e30-4432-840d-093953a11dcd",
   "metadata": {},
   "source": [
    "**Important Considerations**:\n",
    "\n",
    "- **Pricing Tiers**: AWS S3 pricing is tiered. The first 50 TB per month is priced at `$0.023 per GB`, the next 450 TB at `$0.022 per GB`, and storage over 500 TB at `$0.021 per GB`. Ensure you apply the correct pricing tier based on your total storage size.\n",
    "- **Region and Storage Class**: Pricing varies by AWS region and storage class. The example above uses the S3 Standard storage class pricing for the US East (N. Virginia) region. Adjust the pricing variables if your bucket is in a different region or uses a different storage class.\n",
    "- **Additional Costs**: This estimation covers storage costs only. AWS S3 may have additional charges for requests, data retrievals, and data transfers. For a comprehensive cost analysis, consider these factors as well.\n",
    "\n",
    "For detailed and up-to-date information on AWS S3 pricing, please refer to the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b6951-6549-41de-9948-0ee9832df0c1",
   "metadata": {},
   "source": [
    "## 4. Pushing new files from notebook environment to bucket\n",
    "As your analysis generates new files, you can upload to your bucket as demonstrated below. For this demo, you can create a blank `results.txt` file to upload to your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2974d63f-cf39-4ff6-8544-ac8aa6be5e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the S3 bucket name and the file paths\n",
    "train_file_path = \"results.txt\"\n",
    "\n",
    "# Upload the training file\n",
    "s3.upload_file(train_file_path, bucket_name, \"results/results.txt\")\n",
    "\n",
    "print(\"Files uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e08c9-8b7c-46a4-b3c1-28ad14c7d3b7",
   "metadata": {},
   "source": [
    "# Developer use - produce md version of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa582344-6048-4efc-9bee-6cc1d5b3d973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n",
      "[NbConvertApp] Converting notebook test_AWS/Accessing-S3-via-SageMaker-notebooks.ipynb to markdown\n",
      "[NbConvertApp] Writing 21133 bytes to test_AWS/Accessing-S3-via-SageMaker-notebooks.md\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!jupyter nbconvert --to markdown test_AWS/Accessing-S3-via-SageMaker-notebooks.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a79385-1671-4d54-8864-190d36fd1b03",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::: keypoints \n",
    "\n",
    "- Load data from S3 into memory for efficient storage and processing.\n",
    "- Periodically check storage usage and costs to manage S3 budgets.\n",
    "- Use SageMaker to upload analysis results and maintain an organized workflow.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
