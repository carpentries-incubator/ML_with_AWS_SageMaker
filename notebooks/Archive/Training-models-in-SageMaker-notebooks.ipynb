{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2761ac0c-8f9e-4396-b222-f89b7cfd815d",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Initializing SageMaker Environment and Training Models in SageMaker\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can I initialize the SageMaker environment and set up data in S3?\n",
    "- What are the differences between local training and SageMaker-managed training?\n",
    "- How do Estimator classes in SageMaker streamline the training process for various frameworks?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Set up and initialize the SageMaker environment, including roles, sessions, and S3 data.\n",
    "- Understand the difference between training locally in a SageMaker notebook and using SageMaker's managed infrastructure.\n",
    "- Learn to configure and use SageMaker's Estimator classes for different frameworks (e.g., XGBoost, PyTorch, SKLearn).\n",
    "- Compare performance, cost, and setup between custom scripts and built-in images in SageMaker.\n",
    "- Conduct training with data stored in S3 and monitor training job status using the SageMaker console.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef72914-4f7a-41b8-a84f-15a0e438dd59",
   "metadata": {},
   "source": [
    "## Initialize SageMaker environment\n",
    "\n",
    "This code initializes the AWS SageMaker environment by defining the SageMaker role, session, and S3 client. It also specifies the S3 bucket and key for accessing the Titanic training dataset stored in an S3 bucket.\n",
    "\n",
    "#### Boto3 API\n",
    "> Boto3 is the official AWS SDK for Python, allowing developers to interact programmatically with AWS services like S3, EC2, and Lambda. It provides both high-level and low-level APIs, making it easy to manage AWS resources and automate tasks. With built-in support for paginators, waiters, and session management, Boto3 simplifies working with AWS credentials, regions, and IAM permissions. Itâ€™s ideal for automating cloud operations and integrating AWS services into Python applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054fc92d-acab-40c5-88cc-f590e322f053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "role = arn:aws:iam::183295408236:role/ml-sagemaker-use\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize the SageMaker role (will reflect notebook instance's policy)\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f'role = {role}')\n",
    "\n",
    "# Create a SageMaker session to manage interactions with Amazon SageMaker, such as training jobs, model deployments, and data input/output.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Initialize an S3 client to interact with Amazon S3, allowing operations like uploading, downloading, and managing objects and buckets.\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket that we will load from\n",
    "bucket = 'titanic-dataset-test'  # replace with your S3 bucket name\n",
    "\n",
    "# Define train/test filenames\n",
    "train_filename = 'titanic_train.csv'\n",
    "test_filename = 'titanic_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f241bb7-7ad8-4a3c-bf88-bc82e28c6923",
   "metadata": {},
   "source": [
    "### Download copy into notebook environment\n",
    "If you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local\" copy (i.e., one that you store in your notebook's instance).\n",
    "\n",
    "Download data from S3 to notebook environment. You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\n",
    "\n",
    "* check that you have selected the appropriate policy for this notebook\n",
    "* check that your bucket has the appropriate policy permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44edeebe-6af2-411d-a389-2b50fa648cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: ./titanic_train.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the S3 bucket and file location\n",
    "file_key = f\"data/{train_filename}\"  # Path to your file in the S3 bucket\n",
    "local_file_path = f\"./{train_filename}\"  # Local path to save the file\n",
    "\n",
    "# Download the file using the s3 client variable we initialized earlier\n",
    "s3.download_file(bucket, file_key, local_file_path)\n",
    "print(\"File downloaded:\", local_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd136750-a3be-409e-92d8-5430f70d68cf",
   "metadata": {},
   "source": [
    "We can do the same for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253f4930-ca36-49ea-a2eb-fbb29826a394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded: ./titanic_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the S3 bucket and file location\n",
    "file_key = f\"data/{test_filename}\"  # Path to your file in the S3 bucket. W\n",
    "local_file_path = f\"./{test_filename}\"  # Local path to save the file\n",
    "\n",
    "# Initialize the S3 client and download the file\n",
    "s3.download_file(bucket, file_key, local_file_path)\n",
    "print(\"File downloaded:\", local_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b982df2-e8a7-498c-9ba6-fbaa41ae8bef",
   "metadata": {},
   "source": [
    "### Get code (train and tune scripts) from git repo. \n",
    "We recommend you DO NOT put data inside your code repo, as version tracking for data files takes up unnecessary storage in this notebook instance. Instead, store your data in a separte S3 bucket. We have a data folder in our repo only as a means to initially hand you the data for this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701518c-49fc-4248-a4dc-fe6106e3d99e",
   "metadata": {},
   "source": [
    "Check to make sure we're in our EC2 root folder (`/home/ec2-user/SageMaker`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb1e5e9-fdaf-4aea-bf1f-c562923328f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/test_AWS\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f542e1-451e-46f0-9dfc-cad1a4ec434c",
   "metadata": {},
   "source": [
    "If not, change directory using `%cd `."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c0118b-a9ad-418a-9353-f7d61e0d401e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n",
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/SageMaker/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e0a5c3-aaf3-4662-81bf-f385f3328b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'test_AWS' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/UW-Madison-DataScience/test_AWS.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2dd11-0a87-4f34-bf1e-f473a4900d99",
   "metadata": {},
   "source": [
    "### Testing train.py on this notebook's instance\n",
    "Notebook instances in SageMaker allow us allocate more powerful instances (or many instances) to machine learning jobs that require extra power, GPUs, or benefit from parallelization. Before we try exploiting this extra power, it is essential that we test our code thoroughly. We don't want to waste unnecessary compute cycles and resources on jobs that produce bugs instead of insights. If you need to, you can use a subset of your data to run quicker tests. You can also select a slightly better instance resource if your current instance insn't meeting your needs. See the [Instances for ML spreadsheet](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing) for guidance. \n",
    "\n",
    "#### Logging runtime & instance info\n",
    "To compare our local runtime with future experiments, we'll need to know what instance was used, as this will greatly impact runtime in many cases. We can extract the instance name for this notebook using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600eae82-5122-4eeb-9f0f-2752e8623a48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Instance 'Titanic-ML-Notebook' status: InService\n",
      "Instance Type: ml.t3.medium\n"
     ]
    }
   ],
   "source": [
    "# Replace with your notebook instance name.\n",
    "# This does NOT refer to specific ipynb fils, but to the notebook instance opened from SageMaker.\n",
    "notebook_instance_name = 'Titanic-ML-Notebook'\n",
    "\n",
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Describe the notebook instance\n",
    "response = sagemaker_client.describe_notebook_instance(NotebookInstanceName=notebook_instance_name)\n",
    "\n",
    "# Display the status and instance type\n",
    "print(f\"Notebook Instance '{notebook_instance_name}' status: {response['NotebookInstanceStatus']}\")\n",
    "local_instance = response['InstanceType']\n",
    "print(f\"Instance Type: {local_instance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffeda5-a942-415c-ad74-a0deff0e7900",
   "metadata": {},
   "source": [
    "#### Helper:  `get_notebook_instance_info()` \n",
    "You can also use the `get_notebook_instance_info()` function found in `AWS_helpers.py` to retrieve this info for your own project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adafa93a-17d0-4dc5-9656-a8c9eeb65010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Status': 'InService', 'InstanceType': 'ml.t3.medium'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from test_AWS.scripts.AWS_helpers import get_notebook_instance_info\n",
    "get_notebook_instance_info(notebook_instance_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb7ac7-1dff-429e-9cc1-f82a82874ef5",
   "metadata": {},
   "source": [
    "Test train.py on this notebook's instance (or when possible, on your own machine) before doing anything more complicated (e.g., hyperparameter tuning on multiple instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0dfc29a-e095-464e-b173-c61c51f33f5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost # need to add this to environment to run train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443ede9-4e4a-4daf-9f55-93e843407c04",
   "metadata": {},
   "source": [
    "Hereâ€™s what each argument does in detail for the below call to train_xgboost.py:\n",
    "\n",
    "- `--max_depth 5`: Sets the maximum depth of each tree in the model to 5. Limiting tree depth helps control model complexity and can reduce overfitting, especially on small datasets.\n",
    "  \n",
    "- `--eta 0.1`: Sets the learning rate to 0.1, which scales the contribution of each tree to the final model. A smaller learning rate often requires more rounds to converge but can lead to better performance.\n",
    "\n",
    "- `--subsample 0.8`: Specifies that 80% of the training data will be randomly sampled to build each tree. Subsampling can help with model robustness by preventing overfitting and increasing variance.\n",
    "\n",
    "- `--colsample_bytree 0.8`: Specifies that 80% of the features will be randomly sampled for each tree, enhancing the model's ability to generalize by reducing feature reliance.\n",
    "\n",
    "- `--num_round 100`: Sets the number of boosting rounds (trees) to 100. More rounds typically allow for a more refined model, but too many rounds can lead to overfitting.\n",
    "\n",
    "- `--train ./train.csv`: Points to the location of the training data, `train.csv`, which will be used to train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666a5d25-4542-4c77-9280-bff280d0306c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (569, 8)\n",
      "Val size: (143, 8)\n",
      "Training time: 0.06 seconds\n",
      "Model saved to ./xgboost-model\n",
      "Total local runtime: 1.01 seconds, instance_type = ml.t3.medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time as t # we'll use the time package to measure runtime\n",
    "\n",
    "start_time = t.time()\n",
    "\n",
    "# Run the script and pass arguments directly\n",
    "%run test_AWS/scripts/train_xgboost.py --max_depth 5 --eta 0.1 --subsample 0.8 --colsample_bytree 0.8 --num_round 100 --train ./titanic_train.csv\n",
    "\n",
    "# Measure and print the time taken\n",
    "print(f\"Total local runtime: {t.time() - start_time:.2f} seconds, instance_type = {local_instance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f11e0c-660e-48f0-84eb-1ce15a8856d2",
   "metadata": {},
   "source": [
    "Training on this relatively small dataset should take less than a minute, but as we scale up with larger datasets and more complex models in SageMaker, tracking both training time and total runtime becomes essential for efficient debugging and resource management.\n",
    "\n",
    "**Note**: Our training script includes print statements to monitor dataset size and track time spent specifically on training, which provides insights into resource usage for model development. We recommend incorporating similar logging to track not only training time but also total runtime, which includes additional steps like data loading, evaluation, and saving results. Tracking both can help you pinpoint bottlenecks and optimize your workflow as projects grow in size and complexity, especially when scaling with SageMakerâ€™s distributed resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc041f-8b11-44e9-b310-e86291457c48",
   "metadata": {},
   "source": [
    "## Training via SageMaker (using notebook as controller) - custom train.py script\n",
    "Unlike \"local\" training (using this notebook), this next approach leverages SageMakerâ€™s managed infrastructure to handle resources, parallelism, and scalability. By specifying instance parameters, such as instance_count and instance_type, you can control the resources allocated for training.\n",
    "\n",
    "In this example, we start with one ml.m5.large instance, which is suitable for small- to medium-sized datasets and simpler models. Using a single instance is often cost-effective and sufficient for initial testing, allowing for straightforward scaling up to more powerful instance types or multiple instances if training takes too long. See here for further guidance on selecting an appropriate instance for your data/model: [EC2 Instances for ML](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing)\n",
    "\n",
    "\n",
    "### Overview of Estimator Classes in SageMaker\n",
    "In SageMaker, **Estimator** classes streamline the configuration and training of models on managed instances. Each Estimator can work with custom scripts and be enhanced with additional dependencies by specifying a `requirements.txt` file, which is automatically installed at the start of training. Hereâ€™s a breakdown of some commonly used Estimator classes in SageMaker:\n",
    "\n",
    "#### 1. **`Estimator` (Base Class)**\n",
    "   - **Purpose**: General-purpose for custom Docker containers or defining an image URI directly.\n",
    "   - **Configuration**: Requires specifying an `image_uri` and custom entry points.\n",
    "   - **Dependencies**: You can use `requirements.txt` to install Python packages or configure a custom Docker container with pre-baked dependencies.\n",
    "   - **Ideal Use Cases**: Custom algorithms or models that need tailored environments not covered by built-in containers.\n",
    "\n",
    "#### 2. **`XGBoost` Estimator**\n",
    "   - **Purpose**: Provides an optimized container specifically for XGBoost models.\n",
    "   - **Configuration**:\n",
    "      - `entry_point`: Path to a custom script, useful for additional preprocessing or unique training workflows.\n",
    "      - `framework_version`: Select XGBoost version, e.g., `\"1.5-1\"`.\n",
    "      - `dependencies`: Specify additional packages through `requirements.txt` to enhance preprocessing capabilities or incorporate auxiliary libraries.\n",
    "   - **Ideal Use Cases**: Tabular data modeling using gradient-boosted trees; cases requiring custom preprocessing or tuning logic.\n",
    "\n",
    "#### 3. **`PyTorch` Estimator**\n",
    "   - **Purpose**: Configures training jobs with PyTorch for deep learning tasks.\n",
    "   - **Configuration**:\n",
    "      - `entry_point`: Training script with model architecture and training loop.\n",
    "      - `instance_type`: e.g., `ml.p3.2xlarge` for GPU acceleration.\n",
    "      - `framework_version` and `py_version`: Define specific versions.\n",
    "      - `dependencies`: Install any required packages via `requirements.txt` to support advanced data processing, data augmentation, or custom layer implementations.\n",
    "   - **Ideal Use Cases**: Deep learning models, particularly complex networks requiring GPUs and custom layers.\n",
    "\n",
    "#### 4. **`SKLearn` Estimator**\n",
    "   - **Purpose**: Supports scikit-learn workflows for data preprocessing and classical machine learning.\n",
    "   - **Configuration**:\n",
    "      - `entry_point`: Python script to handle feature engineering, preprocessing, or training.\n",
    "      - `framework_version`: Version of scikit-learn, e.g., `\"1.0-1\"`.\n",
    "      - `dependencies`: Use `requirements.txt` to install any additional Python packages required by the training script.\n",
    "   - **Ideal Use Cases**: Classical ML workflows, extensive preprocessing, or cases where additional libraries (e.g., pandas, numpy) are essential.\n",
    "\n",
    "#### 5. **`TensorFlow` Estimator**\n",
    "   - **Purpose**: Designed for training and deploying TensorFlow models.\n",
    "   - **Configuration**:\n",
    "      - `entry_point`: Script for model definition and training process.\n",
    "      - `instance_type`: Select based on dataset size and computational needs.\n",
    "      - `dependencies`: Additional dependencies can be listed in `requirements.txt` to install TensorFlow add-ons, custom layers, or preprocessing libraries.\n",
    "   - **Ideal Use Cases**: NLP, computer vision, and transfer learning applications in TensorFlow.\n",
    "\n",
    "---\n",
    "\n",
    "#### Configuring Custom Environments with `requirements.txt`\n",
    "\n",
    "For all these Estimators, adding a `requirements.txt` file under `dependencies` ensures that additional packages are installed before training begins. This approach allows the use of specific libraries that may be critical for custom preprocessing, feature engineering, or model modifications. Hereâ€™s how to include it:\n",
    "\n",
    "```python\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"train_script.py\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=\"s3://your-bucket/output\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    dependencies=['requirements.txt'],  # Adding custom dependencies\n",
    "    hyperparameters={\n",
    "        \"max_depth\": 5,\n",
    "        \"eta\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"num_round\": 100\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "This setup simplifies training, allowing you to maintain custom environments directly within SageMakerâ€™s managed containers, without needing to build and manage your own Docker images.\n",
    "\n",
    "\n",
    "### More information on pre-built environments\n",
    "he [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) provides lists of pre-built container images for each framework and their standard libraries, including details on pre-installed packages.\n",
    "      \n",
    "For this deployment, we configure the \"XGBoost\" estimator with a custom training script, train_xgboost.py, and define hyperparameters directly within the SageMaker setup. Hereâ€™s the full code, with some additional explanation following the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c181a9e-912d-43d7-baea-bf2419e0432f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-03-21-10-03-577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 21:10:05 Starting - Starting the training job...\n",
      "2024-11-03 21:10:19 Starting - Preparing the instances for training...\n",
      "2024-11-03 21:10:47 Downloading - Downloading input data...\n",
      "2024-11-03 21:11:27 Downloading - Downloading the training image......\n",
      "2024-11-03 21:12:39 Training - Training image download completed. Training in progress.\n",
      "2024-11-03 21:12:39 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:12:28.932 ip-10-2-252-145.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:12:28.955 ip-10-2-252-145.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:29:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "  Building wheel for train-xgboost (setup.py): started\n",
      "  Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "  Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=18013 sha256=854595c3834d84cba646e5026a79c4bfac6876d38d7415f6974b1f62ac802774\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-gpgzd7f8/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:31:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:12:31:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"colsample_bytree\": 0.8,\n",
      "        \"eta\": 0.1,\n",
      "        \"max_depth\": 5,\n",
      "        \"num_round\": 100,\n",
      "        \"subsample\": 0.8,\n",
      "        \"train\": \"titanic_train.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2024-11-03-21-10-03-577\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-03-21-10-03-577/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_xgboost\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-11-03-21-10-03-577/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-11-03-21-10-03-577\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-03-21-10-03-577/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\",\"--train\",\"titanic_train.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN=titanic_train.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8 --train titanic_train.csv\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34mTrain size: (569, 8)\u001b[0m\n",
      "\u001b[34mVal size: (143, 8)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[34mTraining time: 0.23 seconds\u001b[0m\n",
      "\u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\n",
      "2024-11-03 21:12:52 Completed - Training job completed\n",
      "Training seconds: 125\n",
      "Billable seconds: 125\n",
      "Runtime for training on SageMaker: 197.66 seconds, instance_type: ml.m5.large, instance_count: 1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "# Define instance type/count we'll use for training\n",
    "instance_type=\"ml.m5.large\"\n",
    "instance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\n",
    "\n",
    "# Define S3 paths for input and output\n",
    "train_s3_path = f's3://{bucket}/data/{train_filename}'\n",
    "\n",
    "# we'll store all results in a subfolder called xgboost on our bucket. This folder will automatically be created if it doesn't exist already.\n",
    "output_folder = 'xgboost'\n",
    "output_path = f's3://{bucket}/{output_folder}/' \n",
    "\n",
    "# Set up the SageMaker XGBoost Estimator with custom script\n",
    "xgboost_estimator = XGBoost(\n",
    "    entry_point='train_xgboost.py',      # Custom script path\n",
    "    source_dir='test_AWS/scripts',               # Directory where your script is located\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    "    framework_version=\"1.5-1\",           # Use latest supported version for better compatibility\n",
    "    hyperparameters={\n",
    "        'train': 'titanic_train.csv',\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_round': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define input data\n",
    "train_input = TrainingInput(train_s3_path, content_type='csv')\n",
    "\n",
    "# Measure and start training time\n",
    "start = t.time()\n",
    "xgboost_estimator.fit({'train': train_input})\n",
    "end = t.time()\n",
    "\n",
    "print(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06471756-048e-488d-a2d0-0684febe10c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Hyperparameters\n",
    ">The `hyperparameters` section in this code defines key parameters for the XGBoost model, such as `max_depth`, `eta`, `subsample`, `colsample_bytree`, and `num_round`, which control aspects of the model like tree depth, learning rate, and data sampling, directly impacting model performance and training time. \n",
    "> \n",
    "> Additionally, we define a `train_file` hyperparameter to pass the datasetâ€™s S3 path to `train_xgboost.py`, allowing the script to access this path directly. When running the training job, SageMaker passes these values to `train_xgboost.py` as command-line arguments, making them accessible in the script via `argparse` or similar methods. This setup enables flexible tuning of model parameters and data paths directly from the training configuration, without needing modifications in the script itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f252346-7090-455a-a96f-8e9b0e63244c",
   "metadata": {},
   "source": [
    "#### Why do we need a train hyperparameter in addition to TrainingInput?\n",
    ">  The `TrainingInput` in SageMaker isn't just about providing the data path for your script. It actually sets up a **data channel** that allows SageMaker to manage, validate, and automatically transfer your data from S3 to the training instance. Hereâ€™s how it works:\n",
    "> 1. **Data Download**: SageMaker uses `TrainingInput` to download your dataset from S3 to a temporary location on the training instance. This location is mounted and managed by SageMaker and can be accessed by the training job if needed.\n",
    "> 2. **Environment Setup**: Using `TrainingInput` also configures the job environment. For example, the path specified in `TrainingInput` (e.g., under `'train'`) becomes an environment variable (`SM_CHANNEL_TRAIN`), which points to the downloaded data location on the training instance.\n",
    "> 3. **Data Management**: SageMaker can manage and track data inputs independently of your script, which is especially useful for distributed training or when using managed algorithms.\n",
    "> ##### Why Use Both?\n",
    "> If your script is designed to handle the data directly (e.g., by downloading it from an S3 path), the **data path you pass as a hyperparameter** can handle this. However, SageMaker still needs `TrainingInput` to manage and configure the environment and data resources properly.\n",
    "> - **`TrainingInput`**: Required by SageMaker for managing the data channel, downloading data to the instance, and setting up the training environment.\n",
    "> - **Hyperparameter with S3 Path**: Necessary for your custom script to handle the dataset directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1190ce6-3c45-442c-a39e-32a00b1d01a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Model results\n",
    "> With this code, the training results and model artifacts are saved in a subfolder called `xgboost` in your specified S3 bucket. This folder (`s3://{bucket}/xgboost/`) will be automatically created if it doesnâ€™t already exist, and will contain:\n",
    "> \n",
    "> 1. **Model Artifacts**: The trained model file (often a `.tar.gz` file) that SageMaker saves in the `output_path`.\n",
    "> 2. **Logs and Metrics**: Any metrics and logs related to the training job, stored in the same `xgboost` folder.\n",
    "> \n",
    "> This setup allows for convenient access to both the trained model and related output for later evaluation or deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15496aa3-2311-46e8-96f6-575822a392fd",
   "metadata": {},
   "source": [
    "### Extracting trained model from S3 for final evaluation\n",
    "To evaluate the model on a test set after training, weâ€™ll go through these steps:\n",
    "\n",
    "1. **Download the trained model from S3**.\n",
    "2. **Load and preprocess** the test dataset. \n",
    "3. **Evaluate** the model on the test data.\n",
    "\n",
    "Hereâ€™s how you can implement this in your SageMaker notebook. The following code will:\n",
    "\n",
    "- Download the `model.tar.gz` file containing the trained model from S3.\n",
    "- Load the `test.csv` data from S3 and preprocess it as needed.\n",
    "- Use the XGBoost model to make predictions on the test set and then compute accuracy or other metrics on the results. \n",
    "\n",
    "If additional metrics or custom evaluation steps are needed, you can add them in place of or alongside accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa4a272-e8ff-4eef-ba3e-cb29f1e88c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost/sagemaker-xgboost-2024-11-03-21-10-03-577/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Model results are saved in auto-generated folders. Use xgboost_estimator.latest_training_job.name to get the folder name\n",
    "model_s3_path = f'{output_folder}/{xgboost_estimator.latest_training_job.name}/output/model.tar.gz'\n",
    "print(model_s3_path)\n",
    "local_model_path = 'model.tar.gz'\n",
    "\n",
    "# Download the trained model from S3\n",
    "s3.download_file(bucket, model_s3_path, local_model_path)\n",
    "\n",
    "# Extract the model file\n",
    "import tarfile\n",
    "with tarfile.open(local_model_path) as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac08164-4383-4460-8181-ea096451dde2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the test set. We downloaded this earlier from our S3 bucket.\n",
    "test_data = pd.read_csv(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f46e27-8983-4446-997c-6a9774ff07d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>566</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Davies, Mr. Alfred J</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Cribb, Mr. John Hatfield</td>\n",
       "      <td>male</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>371362</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>554</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Leeni, Mr. Fahim (\"Philip Zenni\")</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2620</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>861</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Hansen, Mr. Claus Peter</td>\n",
       "      <td>male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>350026</td>\n",
       "      <td>14.1083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>242</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Murphy, Miss. Katherine \"Kate\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>367230</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass                               Name     Sex  \\\n",
       "0          566         0       3               Davies, Mr. Alfred J    male   \n",
       "1          161         0       3           Cribb, Mr. John Hatfield    male   \n",
       "2          554         1       3  Leeni, Mr. Fahim (\"Philip Zenni\")    male   \n",
       "3          861         0       3            Hansen, Mr. Claus Peter    male   \n",
       "4          242         1       3     Murphy, Miss. Katherine \"Kate\"  female   \n",
       "\n",
       "    Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
       "0  24.0      2      0  A/4 48871  24.1500   NaN        S  \n",
       "1  44.0      0      1     371362  16.1000   NaN        S  \n",
       "2  22.0      0      0       2620   7.2250   NaN        C  \n",
       "3  41.0      2      0     350026  14.1083   NaN        S  \n",
       "4   NaN      1      0     367230  15.5000   NaN        Q  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990f9eb3-e22b-4eae-9801-b7472d565fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the test set to match the training setup\n",
    "from test_AWS.scripts.train_xgboost import preprocess_data\n",
    "X_test, y_test = preprocess_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "add8e3e4-63d1-428c-aaa0-c5fd1542c591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [21:13:21] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [21:13:21] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the trained model using joblib\n",
    "model = joblib.load(\"xgboost-model\")\n",
    "\n",
    "# Assume X_test and y_test are defined\n",
    "# Create DMatrix for X_test for XGBoost prediction compatibility\n",
    "dmatrix_test = xgb.DMatrix(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = model.predict(dmatrix_test)\n",
    "predictions = np.round(preds)  # Round to 0 or 1 for classification\n",
    "\n",
    "# Calculate accuracy or any other relevant metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37708acf-7e2b-4dc4-8cb6-4bfecb1d68e5",
   "metadata": {},
   "source": [
    "Now that weâ€™ve covered training using a custom script with the `XGBoost` estimator, letâ€™s examine the built-in image-based approach. Using SageMakerâ€™s pre-configured XGBoost image streamlines the setup by eliminating the need to manage custom scripts for common workflows, and it can also provide optimization advantages. Below, weâ€™ll discuss both the code and pros and cons of the image-based setup compared to the custom script approach.\n",
    "\n",
    "\n",
    "### Training with SageMaker's Built-in XGBoost Image\n",
    "\n",
    "With the SageMaker-provided XGBoost container, you can bypass custom script configuration if your workflow aligns with standard XGBoost training. This setup is particularly useful when you need quick, default configurations without custom preprocessing or additional libraries.\n",
    "\n",
    "\n",
    "### Comparison: Custom Script vs. Built-in Image\n",
    "\n",
    "| Feature                | Custom Script (`XGBoost` with `entry_point`)      | Built-in XGBoost Image                       |\n",
    "|------------------------|--------------------------------------------------|----------------------------------------------|\n",
    "| **Flexibility**        | Allows for custom preprocessing, data transformation, or advanced parameterization. Requires a Python script and custom dependencies can be added through `requirements.txt`. | Limited to XGBoost's built-in functionality, no custom preprocessing steps without additional customization. |\n",
    "| **Simplicity**         | Requires setting up a script with `entry_point` and managing dependencies. Ideal for specific needs but requires configuration. | Streamlined for fast deployment without custom code. Simple setup and no need for custom scripts.  |\n",
    "| **Performance**        | Similar performance, though potential for overhead with additional preprocessing. | Optimized for typical XGBoost tasks with faster startup. May offer marginally faster time-to-first-train. |\n",
    "| **Use Cases**          | Ideal for complex workflows requiring unique preprocessing steps or when adding specific libraries or functionalities. | Best for quick experiments, standard workflows, or initial testing on datasets without complex preprocessing. |\n",
    "\n",
    "**When to Use Each Approach**:\n",
    "- **Custom Script**: Recommended if you need to implement custom data preprocessing, advanced feature engineering, or specific workflow steps that require more control over training.\n",
    "- **Built-in Image**: Ideal when running standard XGBoost training, especially for quick experiments or production deployments where default configurations suffice.\n",
    "\n",
    "Both methods offer powerful and flexible approaches to model training on SageMaker, allowing you to select the approach best suited to your needs. Below is an example of training using the built-in XGBoost Image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e53219-1e9f-4dd4-a8a9-150ac540876d",
   "metadata": {},
   "source": [
    "#### Setting up the data path\n",
    "In this approach, using `TrainingInput` directly with SageMakerâ€™s built-in XGBoost container contrasts with our previous method, where we specified a custom script with argument inputs (specified in hyperparameters) for data paths and settings. With `TrainingInput`, data paths and formats are managed as structured inputs (`{'train': train_input}`) rather than passed as arguments in a script. This setup simplifies and standardizes data handling in SageMakerâ€™s built-in algorithms, keeping the data configuration separate from hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6105a27f-099e-40f0-8039-4a983ea412cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://titanic-dataset-test/data/titanic_train.csv'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ae6e6e-d0a9-425b-bf95-3276c9a8dba9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-03-21-13-21-649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 21:13:23 Starting - Starting the training job...\n",
      "2024-11-03 21:13:37 Starting - Preparing the instances for training...\n",
      "2024-11-03 21:14:04 Downloading - Downloading input data...\n",
      "2024-11-03 21:14:50 Downloading - Downloading the training image......\n",
      "2024-11-03 21:15:51 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.314 ip-10-0-146-78.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.342 ip-10-0-146-78.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Train matrix has 713 rows and 11 columns\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.774 ip-10-0-146-78.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.774 ip-10-0-146-78.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.775 ip-10-0-146-78.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.775 ip-10-0-146-78.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-11-03:21:15:56:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:474.71030\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.789 ip-10-0-146-78.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:15:56.791 ip-10-0-146-78.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:441.03842\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:411.78134\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:385.35440\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:362.34192\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:342.72199\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:325.37424\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:310.20413\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:297.79462\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:287.85199\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:277.92941\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:270.85162\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:263.09851\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:257.25269\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:251.85989\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:247.19409\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:243.73045\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:240.81642\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:238.41530\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:235.56351\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:233.57898\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:231.39540\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:228.63503\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:226.69484\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:225.35779\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:223.92523\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:222.10831\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:219.23029\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:218.87340\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:216.75085\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:215.76749\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:214.97679\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:213.81511\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:212.42398\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:211.10745\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:209.56615\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:208.38251\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:207.96460\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:206.41853\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:205.13840\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:204.59671\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:203.43626\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:202.23776\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:201.98227\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:201.53015\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:200.83151\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:199.75769\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:197.73955\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:196.67972\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:195.99304\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:194.60979\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:193.87764\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:193.04419\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:191.84062\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:191.63332\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:191.06137\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:190.63503\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:190.23791\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:190.01700\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:189.62627\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:188.78932\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:187.87903\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:187.33061\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:186.93269\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:186.04112\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:185.29774\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:184.67114\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:183.74358\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:183.30225\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:182.09914\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:181.83897\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:181.03862\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:180.78651\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:179.64867\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:178.82935\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:178.21071\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:177.54585\u001b[0m\n",
      "\u001b[34m[77]#011train-rmse:177.00539\u001b[0m\n",
      "\u001b[34m[78]#011train-rmse:176.26054\u001b[0m\n",
      "\u001b[34m[79]#011train-rmse:175.64746\u001b[0m\n",
      "\u001b[34m[80]#011train-rmse:174.62911\u001b[0m\n",
      "\u001b[34m[81]#011train-rmse:174.01623\u001b[0m\n",
      "\u001b[34m[82]#011train-rmse:173.50301\u001b[0m\n",
      "\u001b[34m[83]#011train-rmse:172.43010\u001b[0m\n",
      "\u001b[34m[84]#011train-rmse:171.95624\u001b[0m\n",
      "\u001b[34m[85]#011train-rmse:171.48639\u001b[0m\n",
      "\u001b[34m[86]#011train-rmse:171.19154\u001b[0m\n",
      "\u001b[34m[87]#011train-rmse:169.97925\u001b[0m\n",
      "\u001b[34m[88]#011train-rmse:169.45494\u001b[0m\n",
      "\u001b[34m[89]#011train-rmse:168.90468\u001b[0m\n",
      "\u001b[34m[90]#011train-rmse:168.16402\u001b[0m\n",
      "\u001b[34m[91]#011train-rmse:167.30739\u001b[0m\n",
      "\u001b[34m[92]#011train-rmse:166.85228\u001b[0m\n",
      "\u001b[34m[93]#011train-rmse:165.98686\u001b[0m\n",
      "\u001b[34m[94]#011train-rmse:165.70697\u001b[0m\n",
      "\u001b[34m[95]#011train-rmse:165.43739\u001b[0m\n",
      "\u001b[34m[96]#011train-rmse:164.83107\u001b[0m\n",
      "\u001b[34m[97]#011train-rmse:164.22020\u001b[0m\n",
      "\u001b[34m[98]#011train-rmse:163.90085\u001b[0m\n",
      "\u001b[34m[99]#011train-rmse:163.45399\u001b[0m\n",
      "\n",
      "2024-11-03 21:16:19 Uploading - Uploading generated training model\n",
      "2024-11-03 21:16:19 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n",
      "Runtime for training on SageMaker: 197.50 seconds, instance_type: ml.m5.large, instance_count: 1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator # when using images, we use the general Estimator class\n",
    "\n",
    "# Define instance type/count we'll use for training\n",
    "instance_type=\"ml.m5.large\"\n",
    "instance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\n",
    "\n",
    "# Use Estimator directly for built-in container without specifying entry_point\n",
    "xgboost_estimator_builtin = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\"),\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    "    hyperparameters={\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'num_round': 100\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define input data\n",
    "train_input = TrainingInput(train_s3_path, content_type=\"csv\")\n",
    "\n",
    "# Measure and start training time\n",
    "start = t.time()\n",
    "xgboost_estimator_builtin.fit({'train': train_input})\n",
    "end = t.time()\n",
    "\n",
    "print(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f01345-7d1d-4c36-834e-6e543f867927",
   "metadata": {},
   "source": [
    "## Monitoring Training\n",
    "\n",
    "To view and monitor your SageMaker training job, follow these steps in the AWS Management Console. Since training jobs may be visible to multiple users in your account, it's essential to confirm that you're interacting with your own job before making any changes.\n",
    "\n",
    "1. **Navigate to the SageMaker Console**  \n",
    "   - Go to the AWS Management Console and open the **SageMaker** service (can search for it)\n",
    "\n",
    "2. **View Training Jobs**  \n",
    "   - In the left-hand navigation menu, select **Training jobs**. Youâ€™ll see a list of recent training jobs, which may include jobs from other users in the account.\n",
    "\n",
    "3. **Verify Your Training Job**  \n",
    "   - Identify your job by looking for the specific name format (e.g., `sagemaker-xgboost-YYYY-MM-DD-HH-MM-SS-XXX`) generated when you launched the job.  Click on its name to access detailed information. Cross-check the job details, such as the **Instance Type** and **Input data configuration**, with the parameters you set in your script. \n",
    "\n",
    "4. **Monitor the Job Status**  \n",
    "   - Once youâ€™ve verified the correct job, click on its name to access detailed information:\n",
    "     - **Status**: Confirms whether the job is `InProgress`, `Completed`, or `Failed`.\n",
    "     - **Logs**: Review CloudWatch Logs and Job Metrics for real-time updates.\n",
    "     - **Output Data**: Shows the S3 location with the trained model artifacts.\n",
    "\n",
    "5. **Use CloudWatch for In-Depth Monitoring**  \n",
    "   - If additional monitoring is needed, go to **CloudWatch Logs** to view output logs associated with your training job in real-time.\n",
    "\n",
    "6. **Stopping a Training Job**  \n",
    "   - Before stopping a job, ensure youâ€™ve selected the correct one by verifying job details as outlined above.\n",
    "   - If youâ€™re certain itâ€™s your job, go to **Training jobs** in the SageMaker Console, select the job, and choose **Stop** from the **Actions** menu. Confirm your selection, as this action will halt the job and release any associated resources.\n",
    "   - **Important**: Avoid stopping jobs you donâ€™t own, as this could disrupt other usersâ€™ work and may have unintended consequences.\n",
    "\n",
    "Following these steps helps ensure you only interact with and modify jobs you own, reducing the risk of impacting other users' training processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe5b09-d3be-490f-9712-b46e5be91c22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## When Training Takes Too Long\n",
    "\n",
    "When training time becomes excessive, two main options can improve efficiency in SageMaker: \n",
    "* **Option 1: Upgrading to a more powerful instance** \n",
    "* **Option 2: Using multiple instances for distributed training**. \n",
    "\n",
    "Generally, **Option 1 is the preferred approach** and should be explored first.\n",
    "\n",
    "### Option 1: Upgrade to a More Powerful Instance (Preferred Starting Point)\n",
    "\n",
    "Upgrading to a more capable instance, particularly one with GPU capabilities (e.g., for deep learning), is often the simplest and most cost-effective way to speed up training. Hereâ€™s a breakdown of instances to consider. Check the [Instances for ML spreadsheet](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing) for guidance on selecting a better instance.\n",
    "\n",
    "**When to Use a Single Instance Upgrade**  \n",
    "Upgrading a single instance works well if:\n",
    "   - **Dataset Size**: The dataset is small to moderate (e.g., <10 GB), fitting comfortably within the memory of a larger instance.\n",
    "   - **Model Complexity**: The model is not so large that it requires distribution across multiple devices.\n",
    "   - **Training Time**: Expected training time is within a few hours, but could benefit from additional power.\n",
    "\n",
    "Upgrading a single instance is typically the most efficient option in terms of both cost and setup complexity. It avoids the communication overhead associated with multi-instance setups (discussed below) and is well-suited for most small to medium-sized datasets.\n",
    "\n",
    "### Option 2: Use Multiple Instances for Distributed Training\n",
    "If upgrading a single instance doesnâ€™t sufficiently reduce training time, distributed training across multiple instances may be a viable alternative, particularly for larger datasets and complex models. SageMaker supports two primary distributed training techniques: **data parallelism** and **model parallelism**.\n",
    "\n",
    "#### Understanding Data Parallelism vs. Model Parallelism\n",
    "\n",
    "- **Data Parallelism**: This approach splits the dataset across multiple instances, allowing each instance to process a subset of the data independently. After each batch, gradients are synchronized across instances to ensure consistent updates to the model. Data parallelism is effective when the model itself fits within an instanceâ€™s memory, but the data size or desired training speed requires faster processing through multiple instances.\n",
    "\n",
    "- **Model Parallelism**: Model parallelism divides the model itself across multiple instances, making it ideal for very large models (e.g., deep learning models in NLP or image processing) that cannot fit in memory on a single instance. Each instance processes a segment of the model, and results are combined during training. This approach is suitable for memory-intensive models that exceed the capacity of a single instance.\n",
    "\n",
    "#### How SageMaker Chooses Between Data and Model Parallelism\n",
    "\n",
    "In SageMaker, the choice between data and model parallelism is not entirely automatic. Hereâ€™s how it typically works:\n",
    "\n",
    "- **Data Parallelism (Automatic)**: When you set `instance_count > 1`, SageMaker will automatically apply data parallelism. This splits the dataset across instances, allowing each instance to process a subset independently and synchronize gradients after each batch. Data parallelism works well when the model can fit in the memory of a single instance, but the data size or processing speed needs enhancement with multiple instances.\n",
    "\n",
    "- **Model Parallelism (Manual Setup)**: To enable model parallelism, you need to configure it explicitly using the **SageMaker Model Parallel Library**, suitable for deep learning models in frameworks like PyTorch or TensorFlow. Model parallelism splits the model itself across multiple instances, which is useful for memory-intensive models that exceed the capacity of a single instance. Configuring model parallelism requires setting up a distribution strategy in SageMakerâ€™s Python SDK.\n",
    "\n",
    "- **Hybrid Parallelism (Manual Setup)**: For extremely large datasets and models, SageMaker can support both data and model parallelism together, but this setup requires manual configuration. Hybrid parallelism is beneficial for workloads that are both data- and memory-intensive, where both the model and the data need distributed processing.\n",
    "\n",
    "\n",
    "**When to Use Distributed Training with Multiple Instances**  \n",
    "Consider multiple instances if:\n",
    "   - **Dataset Size**: The dataset is large (>10 GB) and doesnâ€™t fit comfortably within a single instance's memory.\n",
    "   - **Model Complexity**: The model is complex, requiring extensive computation that a single instance cannot handle in a reasonable time.\n",
    "   - **Expected Training Time**: Training on a single instance takes prohibitively long (e.g., >10 hours), and distributed computing overhead is manageable.\n",
    "\n",
    "### Cost of distributed computing \n",
    "**tl;dr** Use 1 instance unless you are finding that you're waiting hours for the training/tuning to complete.\n",
    "\n",
    "Letâ€™s break down some key points for deciding between **1 instance vs. multiple instances** from a cost perspective:\n",
    "\n",
    "1. **Instance Cost per Hour**:\n",
    "   - SageMaker charges per instance-hour. Running **multiple instances** in parallel can finish training faster, reducing wall-clock time, but the **cost per hour will increase** with each added instance.\n",
    "\n",
    "2. **Single Instance vs. Multiple Instance Wall-Clock Time**:\n",
    "   - When using a single instance, training will take significantly longer, especially if your data is large. However, the wall-clock time difference between 1 instance and 10 instances may not translate to a direct 10x speedup when using multiple instances due to **communication overheads**.\n",
    "   - For example, with data-parallel training, instances need to synchronize gradients between batches, which introduces **communication costs** and may slow down training on larger clusters.\n",
    "\n",
    "3. **Scaling Efficiency**:\n",
    "   - Parallelizing training does not scale perfectly due to those overheads. Adding instances generally provides **diminishing returns** on training time reduction.\n",
    "   - For example, doubling instances from 1 to 2 may reduce training time by close to 50%, but going from 8 to 16 instances may only reduce training time by around 20-30%, depending on the model and batch sizes.\n",
    "\n",
    "4. **Typical Recommendation**:\n",
    "   - For **small-to-moderate datasets** or cases where training time isnâ€™t a critical factor, a **single instance** may be more cost-effective, as it avoids parallel processing overheads.\n",
    "   - For **large datasets** or where training speed is a high priority (e.g., tuning complex deep learning models), using **multiple instances** can be beneficial despite the cost increase due to time savings.\n",
    "\n",
    "5. **Practical Cost Estimation**:\n",
    "   - Suppose a single instance takes `T` hours to train and costs `$C` per hour. For a 10-instance setup, the cost would be approximately:\n",
    "     - **Single instance:** `T * $C`\n",
    "     - **10 instances (parallel):** `(T / k) * (10 * $C)`, where `k` is the speedup factor (<10 due to overhead).\n",
    "   - If the speedup is only about 5x instead of 10x due to communication overhead, then the cost difference may be minimal, with a slight edge to a single instance on total cost but at a higher wall-clock time.\n",
    "\n",
    "\n",
    "\n",
    "> In summary:\n",
    "> - **Start by upgrading to a more powerful instance (Option 1)** for datasets up to 10 GB and moderately complex models. A single, more powerful, instance is usually more cost-effective for smaller workloads and where time isnâ€™t critical. Running initial tests with a single instance can also provide a benchmark. You can then experiment with small increases in instance count to find a balance between cost and time savings, particularly considering communication overheads that affect parallel efficiency.\n",
    "> - **Consider distributed training across multiple instances (Option 2)** only when dataset size, model complexity, or training time demand it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7efcb66-dc5b-4c32-abb0-da9e19ceeace",
   "metadata": {},
   "source": [
    "## XGBoost's Distributed Training Mechanism\n",
    "In the event that option 2 explained above really is better for your use-case (e.g., you have a very large dataset or model that takes a while to train even with high performance instances), the next example will demo setting this up. Before we do, though, we should ask what distributed computing really means for our specific model/setup. XGBoostâ€™s distributed training relies on a data-parallel approach that divides the dataset across multiple instances (or workers), enabling each instance to work on a portion of the data independently. This strategy enhances efficiency, especially for large datasets and computationally intensive tasks. \n",
    "\n",
    "> **What about a model parallelism approach?** Unlike deep learning models with vast neural network layers, XGBoostâ€™s decision trees are usually small enough to fit in memory on a single instance, even when the dataset is large. Thus, model parallelism is rarely necessary.\n",
    "XGBoost does not inherently support model parallelism out of the box in SageMaker because the model architecture doesnâ€™t typically exceed memory limits, unlike massive language or image models. Although model parallelism can be theoretically applied (e.g., splitting large tree structures across instances), it's generally not supported natively in SageMaker for XGBoost, as it would require a custom distribution framework to split the model itself.\n",
    "\n",
    "Hereâ€™s how distributed training in XGBoost works, particularly in the SageMaker environment:\n",
    "\n",
    "### Key Steps in Distributed Training with XGBoost\n",
    "\n",
    "#### 1. **Data Partitioning**\n",
    "   - The dataset is divided among multiple instances. For example, with two instances, each instance may receive half of the dataset.\n",
    "   - In SageMaker, data partitioning across instances is handled automatically via the input channels you specify during training, reducing manual setup.\n",
    "\n",
    "#### 2. **Parallel Gradient Boosting**\n",
    "   - XGBoost performs gradient boosting by constructing trees iteratively based on calculated gradients.\n",
    "   - Each instance calculates gradients (first-order derivatives) and Hessians (second-order derivatives of the loss function) independently on its subset of data.\n",
    "   - This parallel processing allows each instance to determine which features to split and which trees to add to the model based on its data portion.\n",
    "\n",
    "#### 3. **Communication Between Instances**\n",
    "   - After computing gradients and Hessians locally, instances synchronize to share and combine these values.\n",
    "   - Synchronization keeps the model parameters consistent across instances. Only computed gradients are communicated, not the raw dataset, minimizing data transfer overhead.\n",
    "   - The combined gradients guide global model updates, ensuring that the ensemble of trees reflects the entire dataset, despite its division across multiple instances.\n",
    "\n",
    "#### 4. **Final Model Aggregation**\n",
    "   - Once training completes, XGBoost aggregates the trained trees from each instance into a single final model.\n",
    "   - This aggregation enables the final model to perform as though it trained on the entire dataset, even if the dataset couldnâ€™t fit into a single instanceâ€™s memory.\n",
    "\n",
    "SageMaker simplifies these steps by automatically managing the partitioning, synchronization, and aggregation processes during distributed training with XGBoost.\n",
    "\n",
    "## Implementing Distributed Training with XGBoost in SageMaker\n",
    "\n",
    "In SageMaker, setting up distributed training for XGBoost can offer significant time savings as dataset sizes and computational requirements increase. Hereâ€™s how you can configure it:\n",
    "\n",
    "1. **Select Multiple Instances**: Specify `instance_count > 1` in the SageMaker `Estimator` to enable distributed training.\n",
    "2. **Optimize Instance Type**: Choose an instance type suitable for your dataset size and XGBoost requirements \n",
    "3. **Monitor for Speed Improvements**: With larger datasets, distributed training can yield time savings by scaling horizontally. However, gains may vary depending on the dataset and computation per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ff4f3bd-2305-4277-affa-bcb667c49b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-03-23-50-10-241\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f4abb2255d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Run with 1 instance\u001b[39;00m\n\u001b[1;32m     28\u001b[0m start1 \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mxgboost_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m end1 \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Now run with 2 instances to observe speedup\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1376\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     forward_to_mlflow_tracking_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forward_to_mlflow_tracking_server:\n\u001b[1;32m   1378\u001b[0m     log_sagemaker_job_to_mlflow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2750\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2750\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5945\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5925\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5926\u001b[0m \n\u001b[1;32m   5927\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5943\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5945\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:8443\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8441\u001b[0m sagemaker_client \u001b[38;5;241m=\u001b[39m sagemaker_session\u001b[38;5;241m.\u001b[39msagemaker_client\n\u001b[1;32m   8442\u001b[0m request_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 8443\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[43m_wait_until\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8444\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainingJobName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8446\u001b[0m \u001b[38;5;28mprint\u001b[39m(secondary_training_status_message(description, \u001b[38;5;28;01mNone\u001b[39;00m), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8448\u001b[0m instance_count, stream_names, positions, client, log_group, dot, color_wrap \u001b[38;5;241m=\u001b[39m _logs_init(\n\u001b[1;32m   8449\u001b[0m     sagemaker_session\u001b[38;5;241m.\u001b[39mboto_session, description, job\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   8450\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:8358\u001b[0m, in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   8356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   8357\u001b[0m     elapsed_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m poll\n\u001b[0;32m-> 8358\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8359\u001b[0m     result \u001b[38;5;241m=\u001b[39m callable_fn()\n\u001b[1;32m   8360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m botocore\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mClientError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   8361\u001b[0m     \u001b[38;5;66;03m# For initial 5 mins we accept/pass AccessDeniedException.\u001b[39;00m\n\u001b[1;32m   8362\u001b[0m     \u001b[38;5;66;03m# The reason is to await tag propagation to avoid false AccessDenied claims for an\u001b[39;00m\n\u001b[1;32m   8363\u001b[0m     \u001b[38;5;66;03m# access policy based on resource tags, The caveat here is for true AccessDenied\u001b[39;00m\n\u001b[1;32m   8364\u001b[0m     \u001b[38;5;66;03m# cases the routine will fail after 5 mins\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define instance type/count we'll use for training\n",
    "instance_type=\"ml.m5.large\"\n",
    "instance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\n",
    "\n",
    "# Define the XGBoost estimator for distributed training\n",
    "xgboost_estimator = Estimator(\n",
    "    image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\"),\n",
    "    role=role,\n",
    "    instance_count=instance_count,  # Start with 1 instance for baseline\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgboost_estimator.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    num_round=100,\n",
    ")\n",
    "\n",
    "# Specify input data from S3\n",
    "train_input = TrainingInput(train_s3_path, content_type=\"csv\")\n",
    "\n",
    "# Run with 1 instance\n",
    "start1 = t.time()\n",
    "xgboost_estimator.fit({\"train\": train_input})\n",
    "end1 = t.time()\n",
    "\n",
    "\n",
    "# Now run with 2 instances to observe speedup\n",
    "xgboost_estimator.instance_count = 2\n",
    "start2 = t.time()\n",
    "xgboost_estimator.fit({\"train\": train_input})\n",
    "end2 = t.time()\n",
    "\n",
    "print(f\"Runtime for training on SageMaker: {end1 - start1:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\n",
    "print(f\"Runtime for training on SageMaker: {end2 - start2:.2f} seconds, instance_type: {instance_type}, instance_count: {xgboost_estimator.instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc2b07-3b34-48ad-b8c7-025da28bbbc6",
   "metadata": {},
   "source": [
    "### Why Scaling Instances Might Not Show Speedup Here\n",
    "\n",
    "* Small Dataset: With only 892 rows, the dataset might be too small to benefit from distributed training. Distributing small datasets often adds overhead (like network communication between instances), which outweighs the parallel processing benefits.\n",
    "\n",
    "* Distributed Overhead: Distributed training introduces coordination steps that can add latency. For very short training jobs, this overhead can become a larger portion of the total training time, reducing the benefit of additional instances.\n",
    "\n",
    "* Tree-Based Models: Tree-based models, like those in XGBoost, donâ€™t benefit from distributed scaling as much as deep learning models when datasets are small. For large datasets, distributed XGBoost can still offer speedups, but this effect is generally less than with neural networks, where parallel gradient updates across multiple instances become efficient.\n",
    "\n",
    "### When Multi-Instance Training Helps\n",
    "* Larger Datasets: Multi-instance training shines with larger datasets, where splitting the data across instances and processing it in parallel can significantly reduce the training time.\n",
    "\n",
    "* Complex Models: For highly complex models with many parameters (like deep learning models or large XGBoost ensembles) and long training times, distributing the training can help speed up the process as each instance contributes to the gradient calculation and optimization steps.\n",
    "\n",
    "* Distributed Algorithms: XGBoost has a built-in distributed training capability, but models that perform gradient descent, like deep neural networks, gain more obvious benefits because each instance can compute gradients for a batch of data simultaneously, allowing faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab290ba4-c74f-4475-b634-77f5b7343de5",
   "metadata": {},
   "source": [
    "## Training a neural network with SageMaker\n",
    "Let's see how to do a similar experiment, but this time using PyTorch neural networks. We will again demonstrate how to test our custom model train script (train_nn.py) before deploying to SageMaker, and discuss some strategies (e.g., using a GPU) for improving train time when needed.\n",
    "\n",
    "### Preparing the data (compressed npz files)\n",
    "When deploying a PyTorch model on SageMaker, itâ€™s helpful to prepare the input data in a format thatâ€™s directly accessible and compatible with PyTorchâ€™s data handling methods. The next code cell will prep our npz files from the existing csv versions. Why are we using this format?\n",
    "\n",
    "1. **Optimized Data Loading**:  \n",
    "   The `.npz` format stores arrays in a compressed, binary format, making it efficient for both storage and loading. PyTorch can easily handle `.npz` files, especially in batch processing, without requiring complex data transformations during training.\n",
    "\n",
    "2. **Batch Compatibility**:  \n",
    "   When training neural networks in PyTorch, itâ€™s common to load data in batches. By storing data in an `.npz` file, we can quickly load the entire dataset or specific parts (e.g., `X_train`, `y_train`) into memory and feed it to the PyTorch `DataLoader`, enabling efficient batched data loading.\n",
    "\n",
    "3. **Reduced I/O Overhead in SageMaker**:  \n",
    "   Storing data in `.npz` files minimizes the I/O operations during training, reducing time spent on data handling. This is especially beneficial in cloud environments like SageMaker, where efficient data handling directly impacts training costs and performance.\n",
    "\n",
    "4. **Consistency and Compatibility**:  \n",
    "   Using `.npz` files allows us to ensure consistency between training and validation datasets. Each file (`train_data.npz` and `val_data.npz`) stores the arrays in a standardized way that can be easily accessed by keys (`X_train`, `y_train`, `X_val`, `y_val`). This structure is compatible with PyTorch's `Dataset` class, making it straightforward to design custom datasets for training.\n",
    "\n",
    "5. **Support for Multiple Data Types**:  \n",
    "   `.npz` files support storage of multiple arrays within a single file. This is helpful for organizing features and labels without additional code. Here, the `train_data.npz` file contains both `X_train` and `y_train`, keeping everything related to training data in one place. Similarly, `val_data.npz` organizes validation features and labels, simplifying file management.\n",
    "\n",
    "In summary, saving the data in `.npz` files ensures a smooth workflow from data loading to model training in PyTorch, leveraging SageMaker's infrastructure for a more efficient, structured training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae53fea-4f53-4d54-bb1d-247378a01957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the Titanic dataset\n",
    "df = pd.read_csv(train_filename)\n",
    "\n",
    "# Encode categorical variables and normalize numerical ones\n",
    "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
    "df['Embarked'] = df['Embarked'].fillna('S')  # Fill missing values in 'Embarked'\n",
    "df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
    "\n",
    "# Fill missing values for 'Age' and 'Fare' with median\n",
    "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "# Select features and target\n",
    "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values\n",
    "y = df['Survived'].values\n",
    "\n",
    "# Normalize features (helps avoid exploding/vanishing gradients)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the preprocessed data to our local jupyter environment\n",
    "np.savez('train_data.npz', X_train=X_train, y_train=y_train)\n",
    "np.savez('val_data.npz', X_val=X_val, y_val=y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a306aa-ab7d-4112-944b-a48e2f04031e",
   "metadata": {},
   "source": [
    "Next, we will upload our compressed files to our S3 bucket. Storage is farily cheap on AWS (around $0.023 per GB per month), but be mindful of uploading too much data. It may be convenient to store a preprocessed version of the data, just don't store too many versions that aren't being actively used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0fdeaca-d5ab-4443-9e84-1c2e1f8b7987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully uploaded to S3.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "train_file = \"train_data.npz\"  # Local file path in your notebook environment\n",
    "val_file = \"val_data.npz\"  # Local file path in your notebook environment\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the training and validation files to S3\n",
    "s3.upload_file(train_file, bucket, f\"{train_file}\")\n",
    "s3.upload_file(val_file, bucket, f\"{val_file}\")\n",
    "\n",
    "print(\"Files successfully uploaded to S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89892c6-c191-4ef2-bafb-dbcab4b5ab9d",
   "metadata": {},
   "source": [
    "#### Testing our train script on notebook instance\n",
    "You should always test code thoroughly before scaling up and using more resources. Here, we will test our script using a small number of epochs â€” just to verify our setup is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf95fe6b-1144-422f-93e2-cf2c1c982825",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.3845, Val Loss: 0.5011, Val Accuracy: 0.7832\n",
      "validation:accuracy = 0.7832\n",
      "Epoch [200/1000], Loss: 0.3482, Val Loss: 0.5042, Val Accuracy: 0.7972\n",
      "validation:accuracy = 0.7972\n",
      "Epoch [300/1000], Loss: 0.3191, Val Loss: 0.5192, Val Accuracy: 0.7972\n",
      "validation:accuracy = 0.7972\n",
      "Epoch [400/1000], Loss: 0.2879, Val Loss: 0.5561, Val Accuracy: 0.7762\n",
      "validation:accuracy = 0.7762\n",
      "Epoch [500/1000], Loss: 0.2602, Val Loss: 0.6074, Val Accuracy: 0.7832\n",
      "validation:accuracy = 0.7832\n",
      "Epoch [600/1000], Loss: 0.2389, Val Loss: 0.6597, Val Accuracy: 0.7902\n",
      "validation:accuracy = 0.7902\n",
      "Epoch [700/1000], Loss: 0.2214, Val Loss: 0.7336, Val Accuracy: 0.7902\n",
      "validation:accuracy = 0.7902\n",
      "Epoch [800/1000], Loss: 0.2074, Val Loss: 0.7903, Val Accuracy: 0.7902\n",
      "validation:accuracy = 0.7902\n",
      "Epoch [900/1000], Loss: 0.1961, Val Loss: 0.8504, Val Accuracy: 0.8042\n",
      "validation:accuracy = 0.8042\n",
      "Epoch [1000/1000], Loss: 0.1865, Val Loss: 0.9208, Val Accuracy: 0.8042\n",
      "validation:accuracy = 0.8042\n",
      "Model saved to nn_model.pth\n",
      "Local training time: 12.53 seconds, instance_type = ml.t3.medium\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Measure training time locally\n",
    "start_time = t.time()\n",
    "%run  test_AWS/scripts/train_nn.py --train train_data.npz --val val_data.npz --epochs 1000 --learning_rate 0.001\n",
    "print(f\"Local training time: {t.time() - start_time:.2f} seconds, instance_type = {local_instance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25515f2-8c71-4650-8007-62541c663f0b",
   "metadata": {},
   "source": [
    "### Deploying PyTorch Neural Network via SageMaker\n",
    "Now that we have tested things locally, we can try to train with a larger number of epochs and a better instance selected. We can do this easily by invoking the PyTorch estimator. Our notebook is currently configured to use ml.m5.large. We can upgrade this to `ml.m5.xlarge` with the below code (using our notebook as a controller). \n",
    "\n",
    "**Should we use a GPU?**: Since this dataset is farily small, we don't necessarily need a GPU for training. Considering costs, the m5.xlarge is `$0.17/hour`, while the cheapest GPU instance is `$0.75/hour`. However, for larger datasets (> 1 GB) and models, we may want to consider a GPU if training time becomes cumbersome (see [Instances for ML](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing). If that doesn't work, we can try distributed computing (setting instance > 1). More on this in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c19b92-e22e-4e9b-b219-ada9b2af0a3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-11-03-21-24-01-337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 21:24:03 Starting - Starting the training job...\n",
      "2024-11-03 21:24:17 Starting - Preparing the instances for training...\n",
      "2024-11-03 21:24:49 Downloading - Downloading input data...\n",
      "2024-11-03 21:25:34 Downloading - Downloading the training image......\n",
      "2024-11-03 21:26:20 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:20,990 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:20,992 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:21,002 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:21,005 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:21,225 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:21,235 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:21,246 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:21,254 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10000,\n",
      "        \"learning_rate\": 0.001,\n",
      "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
      "        \"val\": \"/opt/ml/input/data/val/val_data.npz\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-11-03-21-24-01-337\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-24-01-337/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_nn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_nn.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_nn.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_nn\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/pytorch-training-2024-11-03-21-24-01-337/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2024-11-03-21-24-01-337\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-24-01-337/source/sourcedir.tar.gz\",\"module_name\":\"train_nn\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_nn.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10000\",\"--learning_rate\",\"0.001\",\"--train\",\"/opt/ml/input/data/train/train_data.npz\",\"--val\",\"/opt/ml/input/data/val/val_data.npz\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10000\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN=/opt/ml/input/data/train/train_data.npz\u001b[0m\n",
      "\u001b[34mSM_HP_VAL=/opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_nn.py --epochs 10000 --learning_rate 0.001 --train /opt/ml/input/data/train/train_data.npz --val /opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:21.994 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.282 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.282 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.282 algo-1:27 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.283 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.283 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:591] name:fc1.weight count_params:448\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:591] name:fc1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:591] name:fc2.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:591] name:fc2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:591] name:fc3.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:591] name:fc3.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:593] Total Trainable Params: 2625\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.284 algo-1:27 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:26:22.288 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mEpoch [100/10000], Loss: 0.3741, Val Loss: 0.4981, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [200/10000], Loss: 0.3376, Val Loss: 0.5075, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [300/10000], Loss: 0.3050, Val Loss: 0.5398, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [400/10000], Loss: 0.2759, Val Loss: 0.5843, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [500/10000], Loss: 0.2480, Val Loss: 0.6403, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [600/10000], Loss: 0.2235, Val Loss: 0.7094, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [700/10000], Loss: 0.2032, Val Loss: 0.7828, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [800/10000], Loss: 0.1869, Val Loss: 0.8688, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [900/10000], Loss: 0.1749, Val Loss: 0.9626, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [1000/10000], Loss: 0.1652, Val Loss: 1.0632, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1100/10000], Loss: 0.1570, Val Loss: 1.1384, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1200/10000], Loss: 0.1503, Val Loss: 1.2159, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1300/10000], Loss: 0.1445, Val Loss: 1.2936, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1400/10000], Loss: 0.1392, Val Loss: 1.3667, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [1500/10000], Loss: 0.1347, Val Loss: 1.4578, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [1600/10000], Loss: 0.1307, Val Loss: 1.5430, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1700/10000], Loss: 0.1267, Val Loss: 1.6380, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1800/10000], Loss: 0.1233, Val Loss: 1.7343, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1900/10000], Loss: 0.1197, Val Loss: 1.8190, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [2000/10000], Loss: 0.1155, Val Loss: 1.9306, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [2100/10000], Loss: 0.1124, Val Loss: 1.9985, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [2200/10000], Loss: 0.1097, Val Loss: 2.0931, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [2300/10000], Loss: 0.1074, Val Loss: 2.7644, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [2400/10000], Loss: 0.1051, Val Loss: 2.8522, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [2500/10000], Loss: 0.1028, Val Loss: 2.9309, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [2600/10000], Loss: 0.1004, Val Loss: 3.0025, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [2700/10000], Loss: 0.0982, Val Loss: 3.1121, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [2800/10000], Loss: 0.0963, Val Loss: 3.2071, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2900/10000], Loss: 0.0945, Val Loss: 3.2916, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3000/10000], Loss: 0.0928, Val Loss: 3.9730, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3100/10000], Loss: 0.0911, Val Loss: 4.0554, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3200/10000], Loss: 0.0897, Val Loss: 4.7380, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [3300/10000], Loss: 0.0880, Val Loss: 4.8385, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [3400/10000], Loss: 0.0865, Val Loss: 5.5202, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [3500/10000], Loss: 0.0853, Val Loss: 5.5978, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [3600/10000], Loss: 0.0840, Val Loss: 5.7657, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [3700/10000], Loss: 0.0830, Val Loss: 5.8172, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3800/10000], Loss: 0.0817, Val Loss: 5.8835, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3900/10000], Loss: 0.0804, Val Loss: 5.9313, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4000/10000], Loss: 0.0794, Val Loss: 5.9806, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4100/10000], Loss: 0.0785, Val Loss: 6.0370, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4200/10000], Loss: 0.0776, Val Loss: 6.0840, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4300/10000], Loss: 0.0767, Val Loss: 6.1438, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [4400/10000], Loss: 0.0759, Val Loss: 6.1904, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4500/10000], Loss: 0.0753, Val Loss: 6.2499, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4600/10000], Loss: 0.0743, Val Loss: 6.2893, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4700/10000], Loss: 0.0736, Val Loss: 6.3358, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4800/10000], Loss: 0.0730, Val Loss: 6.3830, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [4900/10000], Loss: 0.0723, Val Loss: 6.4144, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5000/10000], Loss: 0.0716, Val Loss: 6.4545, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5100/10000], Loss: 0.0709, Val Loss: 6.5004, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5200/10000], Loss: 0.0704, Val Loss: 6.5440, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5300/10000], Loss: 0.0700, Val Loss: 6.5842, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5400/10000], Loss: 0.0694, Val Loss: 6.6290, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5500/10000], Loss: 0.0689, Val Loss: 6.6718, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [5600/10000], Loss: 0.0684, Val Loss: 6.7239, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [5700/10000], Loss: 0.0682, Val Loss: 6.7683, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [5800/10000], Loss: 0.0678, Val Loss: 6.8251, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [5900/10000], Loss: 0.0674, Val Loss: 6.8717, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [6000/10000], Loss: 0.0670, Val Loss: 6.9796, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [6100/10000], Loss: 0.0665, Val Loss: 7.0137, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [6200/10000], Loss: 0.0663, Val Loss: 7.0656, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [6300/10000], Loss: 0.0660, Val Loss: 7.1042, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [6400/10000], Loss: 0.0656, Val Loss: 7.1209, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [6500/10000], Loss: 0.0652, Val Loss: 7.1672, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [6600/10000], Loss: 0.0656, Val Loss: 7.2272, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [6700/10000], Loss: 0.0648, Val Loss: 7.2315, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6800/10000], Loss: 0.0644, Val Loss: 7.2770, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [6900/10000], Loss: 0.0641, Val Loss: 7.3160, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7000/10000], Loss: 0.0638, Val Loss: 7.3366, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7100/10000], Loss: 0.0636, Val Loss: 7.4385, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7200/10000], Loss: 0.0634, Val Loss: 7.4674, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7300/10000], Loss: 0.0631, Val Loss: 7.4980, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7400/10000], Loss: 0.0633, Val Loss: 7.5205, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7500/10000], Loss: 0.0627, Val Loss: 7.5470, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7600/10000], Loss: 0.0625, Val Loss: 7.5795, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7700/10000], Loss: 0.0623, Val Loss: 7.6047, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7800/10000], Loss: 0.0622, Val Loss: 7.6293, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [7900/10000], Loss: 0.0620, Val Loss: 7.6610, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [8000/10000], Loss: 0.0617, Val Loss: 7.6780, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [8100/10000], Loss: 0.0615, Val Loss: 7.7170, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [8200/10000], Loss: 0.0620, Val Loss: 7.7381, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [8300/10000], Loss: 0.0613, Val Loss: 7.7641, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [8400/10000], Loss: 0.0610, Val Loss: 7.7903, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8500/10000], Loss: 0.0608, Val Loss: 7.8122, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8600/10000], Loss: 0.0607, Val Loss: 7.8515, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8700/10000], Loss: 0.0605, Val Loss: 7.8830, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8800/10000], Loss: 0.0604, Val Loss: 7.9039, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8900/10000], Loss: 0.0604, Val Loss: 7.9522, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9000/10000], Loss: 0.0600, Val Loss: 7.9772, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9100/10000], Loss: 0.0598, Val Loss: 8.0083, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9200/10000], Loss: 0.0596, Val Loss: 8.0429, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9300/10000], Loss: 0.0595, Val Loss: 8.0704, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9400/10000], Loss: 0.0595, Val Loss: 8.1067, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9500/10000], Loss: 0.0596, Val Loss: 8.1110, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9600/10000], Loss: 0.0590, Val Loss: 8.1497, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9700/10000], Loss: 0.0589, Val Loss: 8.1738, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9800/10000], Loss: 0.0589, Val Loss: 8.2023, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9900/10000], Loss: 0.0587, Val Loss: 8.2236, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [10000/10000], Loss: 0.0589, Val Loss: 8.2539, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mModel saved to /opt/ml/model/nn_model.pth\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:43,071 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:43,071 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:26:43,072 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-03 21:27:03 Uploading - Uploading generated training model\n",
      "2024-11-03 21:27:03 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n",
      "Runtime for training on SageMaker: 197.62 seconds, instance_type: ml.m5.large, instance_count: 1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "epochs = 10000\n",
    "instance_count = 1\n",
    "instance_type=\"ml.m5.large\"\n",
    "output_path = f's3://{bucket}/output_nn/' # this folder will auto-generate if it doesn't exist already\n",
    "\n",
    "# Define the PyTorch estimator and pass hyperparameters as arguments\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"test_AWS/scripts/train_nn.py\",\n",
    "    role=role,\n",
    "    instance_type=instance_type, # with this small dataset, we don't recessarily need a GPU for fast training. \n",
    "    instance_count=instance_count,  # Distributed training with two instances\n",
    "    framework_version=\"1.9\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    "    hyperparameters={\n",
    "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",  # SageMaker will mount this path\n",
    "        \"val\": \"/opt/ml/input/data/val/val_data.npz\",        # SageMaker will mount this path\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": 0.001\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define input paths\n",
    "train_input = TrainingInput(f\"s3://{bucket}/train_data.npz\", content_type=\"application/x-npz\")\n",
    "val_input = TrainingInput(f\"s3://{bucket}/val_data.npz\", content_type=\"application/x-npz\")\n",
    "\n",
    "# Start the training job and time it\n",
    "start = t.time()\n",
    "pytorch_estimator.fit({\"train\": train_input, \"val\": val_input})\n",
    "end = t.time()\n",
    "\n",
    "print(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bbafd-6b7d-4d7a-92d1-1bef7d23f95c",
   "metadata": {},
   "source": [
    "### Deploying PyTorch Neural Network via SageMaker with a GPU Instance\n",
    "\n",
    "In this section, weâ€™ll implement the same procedure as above, but using a GPU-enabled instance for potentially faster training. While GPU instances are more expensive, they can be cost-effective for larger datasets or more complex models that require significant computational power.\n",
    "\n",
    "#### Selecting a GPU Instance\n",
    "For a small dataset like ours, we donâ€™t strictly need a GPU, but for larger datasets or more complex models, a GPU can reduce training time. Here, weâ€™ll select an `ml.g4dn.xlarge` instance, which provides a single GPU and costs approximately `$0.75/hour` (check [Instances for ML](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing) for detailed pricing).\n",
    "\n",
    "#### Code Modifications for GPU Use\n",
    "Using a GPU requires minor changes in your training script (`train_nn.py`). Specifically, youâ€™ll need to:\n",
    "1. Check for GPU availability in PyTorch.\n",
    "2. Move the model and tensors to the GPU device if available.\n",
    "\n",
    "#### Enabling PyTorch to use GPU in `train_nn.py`  \n",
    "\n",
    "The following code snippet to enables GPU support in `train_nn.py`:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa16917-c740-4751-b7e8-608213d64471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-11-03-21-27-19-006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 21:27:20 Starting - Starting the training job...\n",
      "2024-11-03 21:27:34 Starting - Preparing the instances for training...\n",
      "2024-11-03 21:28:06 Downloading - Downloading input data...\n",
      "2024-11-03 21:28:25 Downloading - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:10,249 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:10,280 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:10,284 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:10,561 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10000,\n",
      "        \"learning_rate\": 0.001,\n",
      "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
      "        \"val\": \"/opt/ml/input/data/val/val_data.npz\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-11-03-21-27-19-006\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-27-19-006/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_nn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_nn.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_nn.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_nn\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/pytorch-training-2024-11-03-21-27-19-006/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2024-11-03-21-27-19-006\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-27-19-006/source/sourcedir.tar.gz\",\"module_name\":\"train_nn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_nn.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10000\",\"--learning_rate\",\"0.001\",\"--train\",\"/opt/ml/input/data/train/train_data.npz\",\"--val\",\"/opt/ml/input/data/val/val_data.npz\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10000\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN=/opt/ml/input/data/train/train_data.npz\u001b[0m\n",
      "\u001b[34mSM_HP_VAL=/opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_nn.py --epochs 10000 --learning_rate 0.001 --train /opt/ml/input/data/train/train_data.npz --val /opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\n",
      "2024-11-03 21:32:58 Training - Training image download completed. Training in progress.\u001b[34m[2024-11-03 21:33:14.341 algo-1:34 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.406 algo-1:34 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.407 algo-1:34 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.407 algo-1:34 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.407 algo-1:34 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:591] name:fc1.weight count_params:448\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:591] name:fc1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:591] name:fc2.weight count_params:2048\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:591] name:fc2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:591] name:fc3.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:591] name:fc3.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:593] Total Trainable Params: 2625\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.408 algo-1:34 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:33:14.409 algo-1:34 INFO hook.py:488] Hook is writing from the hook with pid: 34\u001b[0m\n",
      "\u001b[34mEpoch [100/10000], Loss: 0.3813, Val Loss: 0.5014, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[34mEpoch [200/10000], Loss: 0.3457, Val Loss: 0.5234, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [300/10000], Loss: 0.3140, Val Loss: 0.5531, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [400/10000], Loss: 0.2826, Val Loss: 0.5989, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [500/10000], Loss: 0.2556, Val Loss: 0.6691, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [600/10000], Loss: 0.2324, Val Loss: 0.7395, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [700/10000], Loss: 0.2148, Val Loss: 0.8320, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [800/10000], Loss: 0.2011, Val Loss: 0.9215, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [900/10000], Loss: 0.1909, Val Loss: 1.0265, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1000/10000], Loss: 0.1820, Val Loss: 1.1313, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1100/10000], Loss: 0.1744, Val Loss: 1.2326, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1200/10000], Loss: 0.1673, Val Loss: 1.9166, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1300/10000], Loss: 0.1612, Val Loss: 2.0230, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1400/10000], Loss: 0.1563, Val Loss: 2.1253, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1500/10000], Loss: 0.1519, Val Loss: 2.2354, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1600/10000], Loss: 0.1476, Val Loss: 2.3488, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1700/10000], Loss: 0.1435, Val Loss: 2.4441, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [1800/10000], Loss: 0.1392, Val Loss: 2.5590, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [1900/10000], Loss: 0.1355, Val Loss: 2.6673, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [2000/10000], Loss: 0.1321, Val Loss: 2.7777, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [2100/10000], Loss: 0.1287, Val Loss: 2.8916, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [2200/10000], Loss: 0.1259, Val Loss: 3.0203, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [2300/10000], Loss: 0.1231, Val Loss: 3.1378, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [2400/10000], Loss: 0.1206, Val Loss: 3.2416, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [2500/10000], Loss: 0.1181, Val Loss: 3.3395, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [2600/10000], Loss: 0.1159, Val Loss: 3.4380, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [2700/10000], Loss: 0.1136, Val Loss: 3.5399, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [2800/10000], Loss: 0.1115, Val Loss: 3.6908, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [2900/10000], Loss: 0.1097, Val Loss: 3.7636, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3000/10000], Loss: 0.1079, Val Loss: 3.8164, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [3100/10000], Loss: 0.1060, Val Loss: 3.8802, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3200/10000], Loss: 0.1041, Val Loss: 3.9490, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3300/10000], Loss: 0.1028, Val Loss: 4.0062, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [3400/10000], Loss: 0.1013, Val Loss: 4.0833, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [3500/10000], Loss: 0.0998, Val Loss: 4.1561, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [3600/10000], Loss: 0.0984, Val Loss: 4.2385, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [3700/10000], Loss: 0.0976, Val Loss: 4.3264, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3800/10000], Loss: 0.0962, Val Loss: 4.4004, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [3900/10000], Loss: 0.0952, Val Loss: 4.4670, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4000/10000], Loss: 0.0939, Val Loss: 4.5496, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4100/10000], Loss: 0.0924, Val Loss: 4.6431, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4200/10000], Loss: 0.0918, Val Loss: 4.7273, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4300/10000], Loss: 0.0904, Val Loss: 4.8645, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [4400/10000], Loss: 0.0895, Val Loss: 4.9325, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4500/10000], Loss: 0.0886, Val Loss: 5.0728, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4600/10000], Loss: 0.0876, Val Loss: 5.1128, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4700/10000], Loss: 0.0869, Val Loss: 5.7405, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4800/10000], Loss: 0.0861, Val Loss: 5.7850, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4900/10000], Loss: 0.0854, Val Loss: 6.4138, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5000/10000], Loss: 0.0846, Val Loss: 7.0397, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5100/10000], Loss: 0.0839, Val Loss: 7.0785, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5200/10000], Loss: 0.0834, Val Loss: 7.1157, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5300/10000], Loss: 0.0826, Val Loss: 7.1430, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5400/10000], Loss: 0.0819, Val Loss: 7.1962, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5500/10000], Loss: 0.0813, Val Loss: 7.2418, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5600/10000], Loss: 0.0806, Val Loss: 7.2769, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5700/10000], Loss: 0.0800, Val Loss: 7.3093, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5800/10000], Loss: 0.0791, Val Loss: 7.3722, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5900/10000], Loss: 0.0783, Val Loss: 7.4240, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [6000/10000], Loss: 0.0778, Val Loss: 7.4590, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6100/10000], Loss: 0.0773, Val Loss: 7.5014, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [6200/10000], Loss: 0.0767, Val Loss: 7.5366, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6300/10000], Loss: 0.0764, Val Loss: 7.5775, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6400/10000], Loss: 0.0758, Val Loss: 7.6222, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6500/10000], Loss: 0.0751, Val Loss: 7.6452, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6600/10000], Loss: 0.0748, Val Loss: 7.6749, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6700/10000], Loss: 0.0742, Val Loss: 7.7080, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [6800/10000], Loss: 0.0738, Val Loss: 7.7359, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [6900/10000], Loss: 0.0731, Val Loss: 7.7627, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7000/10000], Loss: 0.0729, Val Loss: 7.7904, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7100/10000], Loss: 0.0721, Val Loss: 7.8255, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7200/10000], Loss: 0.0719, Val Loss: 7.8604, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7300/10000], Loss: 0.0713, Val Loss: 7.8853, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7400/10000], Loss: 0.0708, Val Loss: 7.9206, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7500/10000], Loss: 0.0704, Val Loss: 7.9327, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [7600/10000], Loss: 0.0702, Val Loss: 7.9589, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [7700/10000], Loss: 0.0695, Val Loss: 7.9735, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [7800/10000], Loss: 0.0693, Val Loss: 8.0148, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [7900/10000], Loss: 0.0688, Val Loss: 8.0328, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8000/10000], Loss: 0.0685, Val Loss: 8.0482, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8100/10000], Loss: 0.0681, Val Loss: 8.0759, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8200/10000], Loss: 0.0678, Val Loss: 8.1046, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8300/10000], Loss: 0.0676, Val Loss: 8.1099, Val Accuracy: 0.7483\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7483\u001b[0m\n",
      "\u001b[34mEpoch [8400/10000], Loss: 0.0674, Val Loss: 8.1428, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8500/10000], Loss: 0.0669, Val Loss: 8.1620, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8600/10000], Loss: 0.0664, Val Loss: 8.1781, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8700/10000], Loss: 0.0665, Val Loss: 8.2052, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [8800/10000], Loss: 0.0659, Val Loss: 8.2003, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8900/10000], Loss: 0.0656, Val Loss: 8.2300, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [9000/10000], Loss: 0.0659, Val Loss: 8.2336, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [9100/10000], Loss: 0.0651, Val Loss: 8.2635, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [9200/10000], Loss: 0.0650, Val Loss: 8.3558, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9300/10000], Loss: 0.0646, Val Loss: 8.3585, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9400/10000], Loss: 0.0644, Val Loss: 8.3727, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9500/10000], Loss: 0.0645, Val Loss: 8.3758, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9600/10000], Loss: 0.0640, Val Loss: 8.4007, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [9700/10000], Loss: 0.0636, Val Loss: 8.4025, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9800/10000], Loss: 0.0637, Val Loss: 8.4198, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [9900/10000], Loss: 0.0632, Val Loss: 8.4249, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mEpoch [10000/10000], Loss: 0.0631, Val Loss: 8.4355, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mModel saved to /opt/ml/model/nn_model.pth\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:36,683 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:36,683 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:33:36,683 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-03 21:33:56 Uploading - Uploading generated training model\n",
      "2024-11-03 21:33:56 Completed - Training job completed\n",
      "Training seconds: 350\n",
      "Billable seconds: 350\n",
      "Runtime for training on SageMaker: 409.68 seconds, instance_type: ml.g4dn.xlarge, instance_count: 1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import time as t\n",
    "\n",
    "epochs = 10000\n",
    "instance_count = 1\n",
    "instance_type=\"ml.g4dn.xlarge\"\n",
    "output_path = f's3://{bucket}/output_nn/'\n",
    "\n",
    "# Define the PyTorch estimator and pass hyperparameters as arguments\n",
    "pytorch_estimator_gpu = PyTorch(\n",
    "    entry_point=\"test_AWS/scripts/train_nn.py\",\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    framework_version=\"1.9\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    "    hyperparameters={\n",
    "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
    "        \"val\": \"/opt/ml/input/data/val/val_data.npz\",\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": 0.001\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define input paths\n",
    "train_input = TrainingInput(f\"s3://{bucket}/train_data.npz\", content_type=\"application/x-npz\")\n",
    "val_input = TrainingInput(f\"s3://{bucket}/val_data.npz\", content_type=\"application/x-npz\")\n",
    "\n",
    "# Start the training job and time it\n",
    "start = t.time()\n",
    "pytorch_estimator_gpu.fit({\"train\": train_input, \"val\": val_input})\n",
    "end = t.time()\n",
    "print(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4d25-434f-49dd-b757-75ccc13366ea",
   "metadata": {},
   "source": [
    "#### GPUs can be slow for small datasets/models\n",
    "> This performance discrepancy might be due to the following factors:\n",
    "> \n",
    "> 1. **Small Dataset/Model Size**: When datasets and models are small, the overhead of transferring data between the CPU and GPU, as well as managing the GPU, can actually slow things down. For very small models and datasets, CPUs are often faster since there's minimal data to process.\n",
    "> \n",
    "> 2. **GPU Initialization Overhead**: Every time a training job starts on a GPU, thereâ€™s a small overhead for initializing CUDA libraries. For short jobs, this setup time can make the GPU appear slower overall.\n",
    "> \n",
    "> 3. **Batch Size**: GPUs perform best with larger batch sizes since they can process many data points in parallel. If the batch size is too small, the GPU is underutilized, leading to suboptimal performance. You may want to try increasing the batch size to see if this reduces training time.\n",
    "> \n",
    "> 4. **Instance Type**: Some GPU instances, like the `ml.g4dn` series, have less computational power than the larger `p3` series. Theyâ€™re better suited for inference or lightweight tasks rather than intense training, so a more powerful instance (e.g., `ml.p3.2xlarge`) could help for larger tasks.\n",
    "> \n",
    "> If training time continues to be critical, sticking with a CPU instance may be the best approach for smaller datasets. For larger, more complex models and datasets, the GPU's advantages should become more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151dcbf-a8fc-4ef8-bd3b-e291a803ea28",
   "metadata": {},
   "source": [
    "### Distributed Training for Neural Networks in SageMaker\n",
    "In the event that you do need distributed computing to achieve reasonable train times (remember to try an upgraded instance first!), simply adjust the instance count to a number between 2 and 5. Beyond 5 instances, you'll see diminishing returns and may be needlessly spending extra money/compute-energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faa368ba-6648-4b3a-93f8-a8b0271ed199",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2024-11-03-21-34-08-710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 21:34:10 Starting - Starting the training job...\n",
      "2024-11-03 21:34:25 Starting - Preparing the instances for training...\n",
      "2024-11-03 21:35:13 Downloading - Downloading the training image......\n",
      "2024-11-03 21:35:59 Training - Training image download completed. Training in progress..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,542 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,545 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,560 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,563 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,744 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,754 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,765 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:07,774 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,547 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,549 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,564 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,566 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,725 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,736 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,745 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:09,754 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10000,\n",
      "        \"learning_rate\": 0.001,\n",
      "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
      "        \"val\": \"/opt/ml/input/data/val/val_data.npz\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2024-11-03-21-34-08-710\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-34-08-710/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_nn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_nn.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=train_nn.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10000,\n",
      "        \"learning_rate\": 0.001,\n",
      "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
      "        \"val\": \"/opt/ml/input/data/val/val_data.npz\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"ContentType\": \"application/x-npz\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2024-11-03-21-34-08-710\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-34-08-710/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_nn\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_nn.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_nn.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_nn\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=train_nn\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://titanic-dataset-test/pytorch-training-2024-11-03-21-34-08-710/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2024-11-03-21-34-08-710\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-34-08-710/source/sourcedir.tar.gz\",\"module_name\":\"train_nn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_nn.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--epochs\",\"10000\",\"--learning_rate\",\"0.001\",\"--train\",\"/opt/ml/input/data/train/train_data.npz\",\"--val\",\"/opt/ml/input/data/val/val_data.npz\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=10000\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN=/opt/ml/input/data/train/train_data.npz\u001b[0m\n",
      "\u001b[35mSM_HP_VAL=/opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 train_nn.py --epochs 10000 --learning_rate 0.001 --train /opt/ml/input/data/train/train_data.npz --val /opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/pytorch-training-2024-11-03-21-34-08-710/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2024-11-03-21-34-08-710\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/pytorch-training-2024-11-03-21-34-08-710/source/sourcedir.tar.gz\",\"module_name\":\"train_nn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_nn.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10000\",\"--learning_rate\",\"0.001\",\"--train\",\"/opt/ml/input/data/train/train_data.npz\",\"--val\",\"/opt/ml/input/data/val/val_data.npz\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10000\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN=/opt/ml/input/data/train/train_data.npz\u001b[0m\n",
      "\u001b[34mSM_HP_VAL=/opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train_nn.py --epochs 10000 --learning_rate 0.001 --train /opt/ml/input/data/train/train_data.npz --val /opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.404 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.649 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.650 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.650 algo-1:27 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:591] name:fc1.weight count_params:448\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:591] name:fc1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:591] name:fc2.weight count_params:2048\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.453 algo-2:28 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.702 algo-2:28 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.703 algo-2:28 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.703 algo-2:28 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.703 algo-2:28 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:591] name:fc1.weight count_params:448\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:591] name:fc1.bias count_params:64\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:591] name:fc2.weight count_params:2048\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:591] name:fc2.bias count_params:32\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:591] name:fc3.weight count_params:32\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:591] name:fc3.bias count_params:1\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:593] Total Trainable Params: 2625\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.704 algo-2:28 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[35m[2024-11-03 21:36:08.707 algo-2:28 INFO hook.py:488] Hook is writing from the hook with pid: 28\u001b[0m\n",
      "\u001b[35mEpoch [100/10000], Loss: 0.3813, Val Loss: 0.5016, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [200/10000], Loss: 0.3425, Val Loss: 0.5124, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [300/10000], Loss: 0.3058, Val Loss: 0.5471, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [400/10000], Loss: 0.2767, Val Loss: 0.5937, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[35mEpoch [500/10000], Loss: 0.2513, Val Loss: 0.6359, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [600/10000], Loss: 0.2282, Val Loss: 0.7127, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [700/10000], Loss: 0.2105, Val Loss: 0.7909, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [800/10000], Loss: 0.1971, Val Loss: 0.8720, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [900/10000], Loss: 0.1866, Val Loss: 0.9500, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [1000/10000], Loss: 0.1778, Val Loss: 1.0363, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [1100/10000], Loss: 0.1707, Val Loss: 1.1280, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [1200/10000], Loss: 0.1645, Val Loss: 1.2185, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [1300/10000], Loss: 0.1575, Val Loss: 1.3122, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [1400/10000], Loss: 0.1520, Val Loss: 1.4273, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [1500/10000], Loss: 0.1472, Val Loss: 1.5305, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [1600/10000], Loss: 0.1426, Val Loss: 1.6468, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:591] name:fc2.bias count_params:32\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:591] name:fc3.weight count_params:32\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:591] name:fc3.bias count_params:1\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:593] Total Trainable Params: 2625\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.651 algo-1:27 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2024-11-03 21:36:10.654 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mEpoch [100/10000], Loss: 0.3787, Val Loss: 0.4965, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[34mEpoch [200/10000], Loss: 0.3415, Val Loss: 0.4979, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[34mEpoch [300/10000], Loss: 0.3106, Val Loss: 0.5190, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [400/10000], Loss: 0.2837, Val Loss: 0.5512, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [1700/10000], Loss: 0.1381, Val Loss: 2.3400, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [1800/10000], Loss: 0.1342, Val Loss: 2.4467, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [1900/10000], Loss: 0.1305, Val Loss: 2.5424, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [2000/10000], Loss: 0.1267, Val Loss: 2.6410, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[34mEpoch [500/10000], Loss: 0.2580, Val Loss: 0.5925, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [600/10000], Loss: 0.2348, Val Loss: 0.6519, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [700/10000], Loss: 0.2143, Val Loss: 0.7234, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [800/10000], Loss: 0.1973, Val Loss: 0.8037, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [2100/10000], Loss: 0.1234, Val Loss: 3.3205, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [2200/10000], Loss: 0.1203, Val Loss: 3.4029, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [2300/10000], Loss: 0.1178, Val Loss: 4.0621, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [2400/10000], Loss: 0.1153, Val Loss: 4.1508, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [2500/10000], Loss: 0.1128, Val Loss: 4.2235, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [900/10000], Loss: 0.1840, Val Loss: 0.8820, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [1000/10000], Loss: 0.1726, Val Loss: 1.5560, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [1100/10000], Loss: 0.1632, Val Loss: 1.6514, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [1200/10000], Loss: 0.1545, Val Loss: 1.7228, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [1300/10000], Loss: 0.1485, Val Loss: 1.7979, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [1400/10000], Loss: 0.1436, Val Loss: 1.8677, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [2600/10000], Loss: 0.1105, Val Loss: 4.2826, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [2700/10000], Loss: 0.1083, Val Loss: 4.3597, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [2800/10000], Loss: 0.1064, Val Loss: 4.4244, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [2900/10000], Loss: 0.1047, Val Loss: 4.5025, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [3000/10000], Loss: 0.1030, Val Loss: 4.5844, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [1500/10000], Loss: 0.1394, Val Loss: 1.9328, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [1600/10000], Loss: 0.1361, Val Loss: 2.0064, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [1700/10000], Loss: 0.1325, Val Loss: 2.0810, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [1800/10000], Loss: 0.1298, Val Loss: 2.1491, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [1900/10000], Loss: 0.1272, Val Loss: 2.2177, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [3100/10000], Loss: 0.1013, Val Loss: 4.6655, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [3200/10000], Loss: 0.0999, Val Loss: 4.7329, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [3300/10000], Loss: 0.0985, Val Loss: 4.8093, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [3400/10000], Loss: 0.0972, Val Loss: 4.8836, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [3500/10000], Loss: 0.0960, Val Loss: 4.9617, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [2000/10000], Loss: 0.1249, Val Loss: 2.2859, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2100/10000], Loss: 0.1227, Val Loss: 2.3611, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2200/10000], Loss: 0.1206, Val Loss: 2.4378, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [2300/10000], Loss: 0.1187, Val Loss: 2.5197, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2400/10000], Loss: 0.1169, Val Loss: 2.5972, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2500/10000], Loss: 0.1151, Val Loss: 3.2663, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [3600/10000], Loss: 0.0947, Val Loss: 5.0306, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [3700/10000], Loss: 0.0936, Val Loss: 5.1060, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [3800/10000], Loss: 0.0924, Val Loss: 5.1771, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [3900/10000], Loss: 0.0914, Val Loss: 5.2474, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4000/10000], Loss: 0.0904, Val Loss: 5.3196, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4100/10000], Loss: 0.0896, Val Loss: 5.3996, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [2600/10000], Loss: 0.1133, Val Loss: 3.3516, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [2700/10000], Loss: 0.1118, Val Loss: 4.0207, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2800/10000], Loss: 0.1101, Val Loss: 4.0968, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [2900/10000], Loss: 0.1087, Val Loss: 4.1785, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3000/10000], Loss: 0.1072, Val Loss: 4.2749, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [4200/10000], Loss: 0.0885, Val Loss: 5.4697, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4300/10000], Loss: 0.0876, Val Loss: 5.5431, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4400/10000], Loss: 0.0867, Val Loss: 5.6121, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4500/10000], Loss: 0.0859, Val Loss: 5.6809, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4600/10000], Loss: 0.0851, Val Loss: 5.8302, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3100/10000], Loss: 0.1060, Val Loss: 4.3506, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3200/10000], Loss: 0.1041, Val Loss: 4.4481, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3300/10000], Loss: 0.1031, Val Loss: 4.5300, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3400/10000], Loss: 0.1019, Val Loss: 4.6020, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3500/10000], Loss: 0.1008, Val Loss: 4.6680, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [4700/10000], Loss: 0.0843, Val Loss: 5.8830, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4800/10000], Loss: 0.0836, Val Loss: 5.9462, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [4900/10000], Loss: 0.0832, Val Loss: 6.0080, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [5000/10000], Loss: 0.0823, Val Loss: 6.0620, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [5100/10000], Loss: 0.0816, Val Loss: 6.6967, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [3600/10000], Loss: 0.0995, Val Loss: 4.7474, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3700/10000], Loss: 0.0984, Val Loss: 4.8972, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3800/10000], Loss: 0.0975, Val Loss: 4.9556, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [3900/10000], Loss: 0.0971, Val Loss: 4.9949, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [4000/10000], Loss: 0.0959, Val Loss: 5.0668, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4100/10000], Loss: 0.0948, Val Loss: 5.6867, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [5200/10000], Loss: 0.0811, Val Loss: 6.7350, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [5300/10000], Loss: 0.0804, Val Loss: 6.7778, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [5400/10000], Loss: 0.0798, Val Loss: 6.8193, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [5500/10000], Loss: 0.0792, Val Loss: 6.8590, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [5600/10000], Loss: 0.0787, Val Loss: 6.8934, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [4200/10000], Loss: 0.0943, Val Loss: 5.7424, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4300/10000], Loss: 0.0930, Val Loss: 5.8160, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4400/10000], Loss: 0.0921, Val Loss: 5.8663, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4500/10000], Loss: 0.0914, Val Loss: 5.9264, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [4600/10000], Loss: 0.0905, Val Loss: 5.9781, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[35mEpoch [5700/10000], Loss: 0.0782, Val Loss: 6.9369, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [5800/10000], Loss: 0.0781, Val Loss: 6.9614, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [5900/10000], Loss: 0.0771, Val Loss: 7.0126, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [6000/10000], Loss: 0.0767, Val Loss: 7.0562, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [6100/10000], Loss: 0.0760, Val Loss: 7.0912, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [6200/10000], Loss: 0.0757, Val Loss: 7.1389, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [4700/10000], Loss: 0.0898, Val Loss: 6.0255, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4800/10000], Loss: 0.0892, Val Loss: 6.0851, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [4900/10000], Loss: 0.0886, Val Loss: 6.1309, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5000/10000], Loss: 0.0877, Val Loss: 6.1744, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5100/10000], Loss: 0.0872, Val Loss: 6.8164, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[35mEpoch [6300/10000], Loss: 0.0748, Val Loss: 7.1648, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [6400/10000], Loss: 0.0744, Val Loss: 7.2153, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [6500/10000], Loss: 0.0739, Val Loss: 7.2607, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [6600/10000], Loss: 0.0735, Val Loss: 7.2904, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [6700/10000], Loss: 0.0732, Val Loss: 7.3369, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [5200/10000], Loss: 0.0864, Val Loss: 6.8529, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5300/10000], Loss: 0.0859, Val Loss: 6.9001, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5400/10000], Loss: 0.0853, Val Loss: 6.9430, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5500/10000], Loss: 0.0851, Val Loss: 6.9834, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5600/10000], Loss: 0.0842, Val Loss: 7.0222, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [6800/10000], Loss: 0.0728, Val Loss: 7.3659, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [6900/10000], Loss: 0.0726, Val Loss: 7.4142, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [7000/10000], Loss: 0.0721, Val Loss: 7.5228, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [7100/10000], Loss: 0.0717, Val Loss: 7.5609, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [7200/10000], Loss: 0.0713, Val Loss: 7.5951, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [7300/10000], Loss: 0.0709, Val Loss: 7.6344, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [5700/10000], Loss: 0.0838, Val Loss: 7.0774, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [5800/10000], Loss: 0.0831, Val Loss: 7.1082, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [5900/10000], Loss: 0.0828, Val Loss: 7.1610, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [6000/10000], Loss: 0.0822, Val Loss: 7.1864, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6100/10000], Loss: 0.0817, Val Loss: 7.2305, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6200/10000], Loss: 0.0813, Val Loss: 7.2680, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[35mEpoch [7400/10000], Loss: 0.0706, Val Loss: 7.6636, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [7500/10000], Loss: 0.0702, Val Loss: 7.7645, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[35mEpoch [7600/10000], Loss: 0.0700, Val Loss: 7.7985, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [7700/10000], Loss: 0.0696, Val Loss: 7.8277, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[35mEpoch [7800/10000], Loss: 0.0693, Val Loss: 7.9368, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[34mEpoch [6300/10000], Loss: 0.0809, Val Loss: 7.3120, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [6400/10000], Loss: 0.0805, Val Loss: 7.3411, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6500/10000], Loss: 0.0800, Val Loss: 7.3729, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6600/10000], Loss: 0.0799, Val Loss: 7.4066, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6700/10000], Loss: 0.0794, Val Loss: 7.4368, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [7900/10000], Loss: 0.0692, Val Loss: 7.9556, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8000/10000], Loss: 0.0687, Val Loss: 7.9887, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8100/10000], Loss: 0.0685, Val Loss: 8.0236, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8200/10000], Loss: 0.0682, Val Loss: 8.0405, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8300/10000], Loss: 0.0682, Val Loss: 8.0736, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[34mEpoch [6800/10000], Loss: 0.0789, Val Loss: 7.4689, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [6900/10000], Loss: 0.0793, Val Loss: 7.4817, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [7000/10000], Loss: 0.0783, Val Loss: 7.5318, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7100/10000], Loss: 0.0780, Val Loss: 8.1370, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7200/10000], Loss: 0.0776, Val Loss: 7.5942, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [8400/10000], Loss: 0.0677, Val Loss: 8.0867, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8500/10000], Loss: 0.0674, Val Loss: 8.1114, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8600/10000], Loss: 0.0671, Val Loss: 8.1422, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8700/10000], Loss: 0.0669, Val Loss: 8.1736, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8800/10000], Loss: 0.0667, Val Loss: 8.2030, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[35mEpoch [8900/10000], Loss: 0.0664, Val Loss: 8.2183, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[34mEpoch [7300/10000], Loss: 0.0773, Val Loss: 8.2057, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7400/10000], Loss: 0.0771, Val Loss: 8.2218, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7500/10000], Loss: 0.0767, Val Loss: 8.2510, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7600/10000], Loss: 0.0765, Val Loss: 8.2606, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7700/10000], Loss: 0.0761, Val Loss: 8.2879, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [9000/10000], Loss: 0.0662, Val Loss: 8.2551, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [9100/10000], Loss: 0.0662, Val Loss: 8.2706, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mEpoch [9200/10000], Loss: 0.0657, Val Loss: 8.8886, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [9300/10000], Loss: 0.0657, Val Loss: 8.9247, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [9400/10000], Loss: 0.0653, Val Loss: 9.0481, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7800/10000], Loss: 0.0759, Val Loss: 8.2985, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [7900/10000], Loss: 0.0758, Val Loss: 8.3353, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8000/10000], Loss: 0.0756, Val Loss: 8.3614, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8100/10000], Loss: 0.0752, Val Loss: 8.3810, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8200/10000], Loss: 0.0748, Val Loss: 8.3930, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[35mEpoch [9500/10000], Loss: 0.0652, Val Loss: 9.1051, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [9600/10000], Loss: 0.0648, Val Loss: 9.1288, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [9700/10000], Loss: 0.0647, Val Loss: 9.1579, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [9800/10000], Loss: 0.0646, Val Loss: 9.1813, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[35mEpoch [9900/10000], Loss: 0.0644, Val Loss: 9.2081, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[34mEpoch [8300/10000], Loss: 0.0750, Val Loss: 8.4140, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [8400/10000], Loss: 0.0745, Val Loss: 8.4386, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8500/10000], Loss: 0.0742, Val Loss: 8.4695, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8600/10000], Loss: 0.0741, Val Loss: 8.4896, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8700/10000], Loss: 0.0735, Val Loss: 8.5076, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [8800/10000], Loss: 0.0741, Val Loss: 8.5529, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[35mEpoch [10000/10000], Loss: 0.0640, Val Loss: 9.2331, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[35mModel saved to /opt/ml/model/nn_model.pth\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:27,980 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:27,980 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2024-11-03 21:36:27,980 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mEpoch [8900/10000], Loss: 0.0732, Val Loss: 8.5524, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9000/10000], Loss: 0.0735, Val Loss: 8.5693, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9100/10000], Loss: 0.0731, Val Loss: 8.6070, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9200/10000], Loss: 0.0728, Val Loss: 8.6183, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9300/10000], Loss: 0.0725, Val Loss: 8.6277, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9400/10000], Loss: 0.0723, Val Loss: 8.6563, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9500/10000], Loss: 0.0721, Val Loss: 8.6672, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9600/10000], Loss: 0.0720, Val Loss: 8.6886, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9700/10000], Loss: 0.0732, Val Loss: 8.7433, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[34mEpoch [9800/10000], Loss: 0.0717, Val Loss: 8.7260, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [9900/10000], Loss: 0.0714, Val Loss: 8.7701, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[34mEpoch [10000/10000], Loss: 0.0728, Val Loss: 8.7538, Val Accuracy: 0.7552\u001b[0m\n",
      "\u001b[34mvalidation:accuracy = 0.7552\u001b[0m\n",
      "\u001b[34mModel saved to /opt/ml/model/nn_model.pth\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:30,182 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:30,182 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-03 21:36:30,182 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-11-03 21:36:35 Uploading - Uploading generated training model\n",
      "2024-11-03 21:36:47 Completed - Training job completed\n",
      "Training seconds: 228\n",
      "Billable seconds: 228\n",
      "Runtime for training on SageMaker: 198.36 seconds, instance_type: ml.m5.xlarge, instance_count: 2\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import time as t\n",
    "\n",
    "epochs = 10000\n",
    "instance_count = 2 # increasing to 2 to see if it has any benefit (likely won't see any with this small dataset)\n",
    "instance_type=\"ml.m5.xlarge\"\n",
    "output_path = f's3://{bucket}/output_nn/'\n",
    "\n",
    "# Define the PyTorch estimator and pass hyperparameters as arguments\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"test_AWS/scripts/train_nn.py\",\n",
    "    role=role,\n",
    "    instance_type=instance_type, # with this small dataset, we don't recessarily need a GPU for fast training. \n",
    "    instance_count=instance_count,  # Distributed training with two instances\n",
    "    framework_version=\"1.9\",\n",
    "    py_version=\"py38\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=session,\n",
    "    hyperparameters={\n",
    "        \"train\": \"/opt/ml/input/data/train/train_data.npz\",  # SageMaker will mount this path\n",
    "        \"val\": \"/opt/ml/input/data/val/val_data.npz\",        # SageMaker will mount this path\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": 0.001\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define input paths\n",
    "train_input = TrainingInput(f\"s3://{bucket}/train_data.npz\", content_type=\"application/x-npz\")\n",
    "val_input = TrainingInput(f\"s3://{bucket}/val_data.npz\", content_type=\"application/x-npz\")\n",
    "\n",
    "# Start the training job and time it\n",
    "start = t.time()\n",
    "pytorch_estimator.fit({\"train\": train_input, \"val\": val_input})\n",
    "end = t.time()\n",
    "\n",
    "print(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c96ea4-3660-4d02-bd40-1f59ad232191",
   "metadata": {},
   "source": [
    "### Distributed Training for Neural Networks in SageMaker: Understanding Training Strategies and How Epochs Are Managed\n",
    "Amazon SageMaker provides two main strategies for distributed training: **data parallelism** and **model parallelism**. Understanding which strategy will be used depends on the model size and the configuration of your SageMaker training job, as well as the default settings of the specific SageMaker Estimator you are using.\n",
    "\n",
    "#### 1. **Data Parallelism (Most Common for Mini-batch SGD)**\n",
    "   - **How it Works**: In data parallelism, each instance in the cluster (e.g., multiple `ml.m5.xlarge` instances) maintains a **complete copy of the model**. The **training dataset is split across instances**, and each instance processes a different subset of data simultaneously. This enables multiple instances to complete forward and backward passes on different data batches independently.\n",
    "   - **Epoch Distribution**: Even though each instance processes all the specified epochs, they only work on a portion of the dataset for each epoch. After each batch, instances synchronize their gradient updates across all instances using a method such as *all-reduce*. This ensures that while each instance is working with a unique data batch, the model weights remain consistent across instances.\n",
    "   - **Key Insight**: Because all instances process the specified number of epochs and synchronize weight updates between batches, each instanceâ€™s training contributes to a cohesive, shared model. The **effective epoch count across instances appears to be shared** because data parallelism allows each instance to handle a fraction of the data per epoch, not the epochs themselves. Data parallelism is well-suited for models that can fit into a single instanceâ€™s memory and benefit from increased data throughput.\n",
    "\n",
    "#### 2. **Model Parallelism (Best for Large Models)**\n",
    "   - **How it Works**: Model parallelism divides the model itself across multiple instances, not the data. This approach is best suited for very large models that cannot fit into a single GPU or instanceâ€™s memory (e.g., large language models).\n",
    "   - **Epoch Distribution**: The model is partitioned so that each instance is responsible for specific layers or components. Data flows sequentially through these partitions, where each instance processes a part of each batch and passes it to the next instance.\n",
    "   - **Key Insight**: This approach is more complex due to the dependency between model components, so **synchronization occurs across the model layers rather than across data batches**. Model parallelism generally suits scenarios with exceptionally large model architectures that exceed memory limits of typical instances.\n",
    "\n",
    "### Determining Which Distributed Training Strategy is Used\n",
    "SageMaker will select the distributed strategy based on:\n",
    "   - **Framework and Estimator Configuration**: Most deep learning frameworks in SageMaker default to data parallelism, especially when using PyTorch or TensorFlow with standard configurations.\n",
    "   - **Model and Data Size**: If you specify a model that exceeds a single instance's memory capacity, SageMaker may switch to model parallelism if configured for it.\n",
    "   - **Instance Count**: When you specify `instance_count > 1` in your Estimator with a deep learning model, SageMaker will use data parallelism by default unless explicitly configured for model parallelism.\n",
    "\n",
    "You observed that each instance ran all epochs with `instance_count=2` and 10,000 epochs, which aligns with data parallelism. Here, each instance processed the full set of epochs independently, but each batch of data was different, and the gradient updates were synchronized across instances.\n",
    "\n",
    "### Summary of Key Points\n",
    "- **Data Parallelism** is the default distributed training strategy and splits the dataset across instances, allowing each instance to work on different data batches.\n",
    "   - Each instance runs all specified epochs, but the weight updates are synchronized, so **epoch workload is shared across the data** rather than by reducing epoch count per instance.\n",
    "- **Model Parallelism** splits the model itself across instances, typically only needed for very large models that exceed the memory capacity of single instances.\n",
    "- **Choosing Between Distributed Strategies**: Data parallelism is suitable for most neural network models, especially those that fit in memory, while model parallelism is intended for exceptionally large models with memory constraints.\n",
    "\n",
    "For cost optimization:\n",
    "- **Single-instance training** is typically more cost-effective for small or moderately sized datasets, while **multi-instance setups** can reduce wall-clock time for larger datasets and complex models, at a higher instance cost.\n",
    "- For **initial testing**, start with data parallelism on a single instance, and increase instance count if training time becomes prohibitive, while being mindful of communication overhead and scaling efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4735b5ce-e976-40dd-8be1-4d0f7d03f224",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n",
      "[NbConvertApp] Converting notebook test_AWS/05_Intro-train-models.ipynb to markdown\n",
      "[NbConvertApp] Writing 199187 bytes to test_AWS/05_Intro-train-models.md\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!jupyter nbconvert --to markdown test_AWS/05_Intro-train-models.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c500c98-7759-4161-8d2e-cca445fc21f3",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- **Environment Initialization**: Setting up a SageMaker session, defining roles, and specifying the S3 bucket are essential for managing data and running jobs in SageMaker.\n",
    "- **Local vs. Managed Training**: Local training in SageMaker notebooks can be useful for quick tests but lacks the scalability and resource management available in SageMaker-managed training.\n",
    "- **Estimator Classes**: SageMaker provides framework-specific Estimator classes (e.g., XGBoost, PyTorch, SKLearn) to streamline training setups, each suited to different model types and workflows.\n",
    "- **Custom Scripts vs. Built-in Images**: Custom training scripts offer flexibility with preprocessing and custom logic, while built-in images are optimized for rapid deployment and simpler setups.\n",
    "- **Training Data Channels**: Using `TrainingInput` ensures SageMaker manages data efficiently, especially for distributed setups where data needs to be synchronized across multiple instances.\n",
    "- **Distributed Training Options**: Data parallelism (splitting data across instances) is common for many models, while model parallelism (splitting the model across instances) is useful for very large models that exceed instance memory.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
