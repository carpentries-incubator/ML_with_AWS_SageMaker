{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864bcdd-9bdd-4eb7-ac55-09059a10c4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "GitHub Username:  qualiaMachine\n"
     ]
    }
   ],
   "source": [
    "from scripts.AWS_helpers import update_repo\n",
    "update_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3af79ae-4e7a-4353-b7e9-3717b51ee467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!git config --global user.name \"Chris Endemann\"\n",
    "!git config --global user.email endeman@wisc.edu\n",
    "github_url = 'github.com/UW-Madison-DataScience/test_AWS.git' # found under Code -> Clone -> HTTPS (remote the https:// before the rest of the address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bf82450-c6b3-4ad6-9764-f0ed8a9d5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import getpass\n",
    "\n",
    "# # Prompt for GitHub username and PAT securely\n",
    "# username = input(\"GitHub Username: \")\n",
    "# token = getpass.getpass(\"GitHub Personal Access Token (PAT): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1876e11-3e1d-4fe4-a236-932542fe1775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/test_AWS\n",
      "/home/ec2-user/SageMaker/test_AWS\n",
      "01_Setting-up-S3-bucket.md\t\t      LICENSE\n",
      "02_Setting-up-notebook-environment.md\t      prep-train-test-sets.ipynb\n",
      "03_Data-storage-and-access-via-buckets.ipynb  README.md\n",
      "04_Interacting-with-code-repo.ipynb\t      scripts\n",
      "05_Intro-train-models.ipynb\t\t      titanic_test.csv\n",
      "06_Hyperparameter-tuning.ipynb\t\t      titanic_train.csv\n",
      "Check_running_resources.ipynb\t\t      update-repo.ipynb\n",
      "create_large_data.ipynb\t\t\t      xgboost-model\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/SageMaker/test_AWS\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42fdf8b1-3c6d-422a-bb95-a38c70c16673",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbdiff /tmp/git-blob-1Xtnk6/05_Intro-train-models.ipynb 05_Intro-train-models.ipynb\n",
      "--- /tmp/git-blob-1Xtnk6/05_Intro-train-models.ipynb  2024-11-03 19:52:16.106938\n",
      "+++ 05_Intro-train-models.ipynb  2024-11-03 19:46:15.180899\n",
      "\u001b[34m\u001b[1m## replaced /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-  2\n",
      "\u001b[32m+  4\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/5/execution_count:\u001b[0m\n",
      "\u001b[31m-  3\n",
      "\u001b[32m+  5\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/5/outputs/0/text:\u001b[0m\n",
      "\u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[31m-File downloaded: ./titanic_train.csv\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mFile downloaded: ./titanic_test.csv\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/5/source:\u001b[0m\n",
      "\u001b[36m@@ -1,6 +1,6 @@\u001b[m\n",
      " # Define the S3 bucket and file location\u001b[m\n",
      "\u001b[31m-file_key = f\"data/{train_filename}\"  # Path to your file in the S3 bucket. W\u001b[m\n",
      "\u001b[31m-local_file_path = f\"./{train_filename}\"  # Local path to save the file\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mfile_key = f\"data/{test_filename}\"  # Path to your file in the S3 bucket. W\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mlocal_file_path = f\"./{test_filename}\"  # Local path to save the file\u001b[m\n",
      " \u001b[m\n",
      " # Initialize the S3 client and download the file\u001b[m\n",
      " s3 = boto3.client(\"s3\")\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/7:\u001b[0m\n",
      "\u001b[32m+  markdown cell:\n",
      "\u001b[32m+    id: f701518c-49fc-4248-a4dc-fe6106e3d99e\n",
      "\u001b[32m+    source:\n",
      "\u001b[32m+      Check to make sure we're in our EC2 root folder (`/home/ec2-user/SageMaker`).\n",
      "\u001b[32m+  code cell:\n",
      "\u001b[32m+    id: cdb1e5e9-fdaf-4aea-bf1f-c562923328f5\n",
      "\u001b[32m+    execution_count: 11\n",
      "\u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      tags:\n",
      "\u001b[32m+        []\n",
      "\u001b[32m+    source:\n",
      "\u001b[32m+      !pwd\n",
      "\u001b[32m+    outputs:\n",
      "\u001b[32m+      output 0:\n",
      "\u001b[32m+        output_type: stream\n",
      "\u001b[32m+        name: stdout\n",
      "\u001b[32m+        text:\n",
      "\u001b[32m+          /home/ec2-user/SageMaker/test_AWS\n",
      "\u001b[32m+  markdown cell:\n",
      "\u001b[32m+    id: 68f542e1-451e-46f0-9dfc-cad1a4ec434c\n",
      "\u001b[32m+    source:\n",
      "\u001b[32m+      If not, change directory using `%cd `.\n",
      "\u001b[32m+  code cell:\n",
      "\u001b[32m+    id: d4c0118b-a9ad-418a-9353-f7d61e0d401e\n",
      "\u001b[32m+    execution_count: 12\n",
      "\u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      tags:\n",
      "\u001b[32m+        []\n",
      "\u001b[32m+    source:\n",
      "\u001b[32m+      %cd /home/ec2-user/SageMaker/\n",
      "\u001b[32m+      !pwd\n",
      "\u001b[32m+    outputs:\n",
      "\u001b[32m+      output 0:\n",
      "\u001b[32m+        output_type: stream\n",
      "\u001b[32m+        name: stdout\n",
      "\u001b[32m+        text:\n",
      "\u001b[32m+          /home/ec2-user/SageMaker\n",
      "\u001b[32m+          /home/ec2-user/SageMaker\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/7/execution_count:\u001b[0m\n",
      "\u001b[31m-  4\n",
      "\u001b[32m+  13\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/8/source:\u001b[0m\n",
      "\u001b[36m@@ -1,4 +1,5 @@\u001b[m\n",
      " ### Testing train.py on this notebook's instance\u001b[m\n",
      " Notebook instances in SageMaker allow us allocate more powerful instances (or many instances) to machine learning jobs that require extra power, GPUs, or benefit from parallelization. Before we try exploiting this extra power, it is essential that we test our code thoroughly. We don't want to waste unnecessary compute cycles and resources on jobs that produce bugs instead of insights. If you need to, you can use a subset of your data to run quicker tests. You can also select a slightly better instance resource if your current instance insn't meeting your needs. See the [Instances for ML spreadsheet](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing) for guidance. \u001b[m\n",
      " \u001b[m\n",
      "\u001b[31m-Test train.py on this notebook's instance (or when possible, on your own machine) before doing anything more complicated (e.g., hyperparameter tuning on multiple instances)\u001b[m\n",
      "\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m#### Logging runtime & instance info\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mTo compare our local runtime with future experiments, we'll need to know what instance was used, as this will greatly impact runtime in many cases. We can extract the instance name for this notebook using...\u001b[m\n",
      "\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/9:\u001b[0m\n",
      "\u001b[32m+  code cell:\n",
      "\u001b[32m+    id: 600eae82-5122-4eeb-9f0f-2752e8623a48\n",
      "\u001b[32m+    execution_count: 14\n",
      "\u001b[32m+    metadata (known keys):\n",
      "\u001b[32m+      tags:\n",
      "\u001b[32m+        []\n",
      "\u001b[32m+    source:\n",
      "\u001b[32m+      # Replace with your notebook instance name.\n",
      "\u001b[32m+      # This does NOT refer to specific ipynb fils, but to the notebook instance opened from SageMaker.\n",
      "\u001b[32m+      notebook_instance_name = 'Titanic-ML-Notebook' \n",
      "\u001b[32m+      \n",
      "\u001b[32m+      # Initialize SageMaker client\n",
      "\u001b[32m+      import boto3\n",
      "\u001b[32m+      sagemaker_client = boto3.client('sagemaker')\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      # Describe the notebook instance\n",
      "\u001b[32m+      response = sagemaker_client.describe_notebook_instance(NotebookInstanceName=notebook_instance_name)\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      # Display the status and instance type\n",
      "\u001b[32m+      print(f\"Notebook Instance '{notebook_instance_name}' status: {response['NotebookInstanceStatus']}\")\n",
      "\u001b[32m+      local_instance = response['InstanceType']  \n",
      "\u001b[32m+      print(f\"Instance Type: {local_instance}\")\n",
      "\u001b[32m+    outputs:\n",
      "\u001b[32m+      output 0:\n",
      "\u001b[32m+        output_type: stream\n",
      "\u001b[32m+        name: stdout\n",
      "\u001b[32m+        text:\n",
      "\u001b[32m+          Notebook Instance 'Titanic-ML-Notebook' status: InService\n",
      "\u001b[32m+          Instance Type: ml.t3.medium\n",
      "\u001b[32m+  markdown cell:\n",
      "\u001b[32m+    id: decb7ac7-1dff-429e-9cc1-f82a82874ef5\n",
      "\u001b[32m+    source:\n",
      "\u001b[32m+      Test train.py on this notebook's instance (or when possible, on your own machine) before doing anything more complicated (e.g., hyperparameter tuning on multiple instances)\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/9/execution_count:\u001b[0m\n",
      "\u001b[31m-  5\n",
      "\u001b[32m+  15\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/11/execution_count:\u001b[0m\n",
      "\u001b[31m-  6\n",
      "\u001b[32m+  16\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/11/outputs/0/text:\u001b[0m\n",
      "\u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[31m-Training time: 0.07 seconds\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mTraining time: 0.05 seconds\u001b[m\n",
      " Model saved to ./xgboost-model\u001b[m\n",
      "\u001b[31m-Local training time: 1.01 seconds\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mLocal training time: 0.08 seconds, instance_type = ml.t3.medium\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/11/outputs/1:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stderr\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "\u001b[31m-      Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "\u001b[31m-        warnings.warn(\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/11/source:\u001b[0m\n",
      "\u001b[36m@@ -1,4 +1,4 @@\u001b[m\n",
      "\u001b[31m-import time as t\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mimport time as t # we'll use the time package to measure runtime\u001b[m\n",
      " \u001b[m\n",
      " start_time = t.time()\u001b[m\n",
      " \u001b[m\n",
      "\u001b[36m@@ -6,4 +6,4 @@\u001b[m \u001b[mstart_time = t.time()\u001b[m\n",
      " %run test_AWS/scripts/train_xgboost.py --max_depth 5 --eta 0.1 --subsample 0.8 --colsample_bytree 0.8 --num_round 100 --train ./titanic_train.csv\u001b[m\n",
      " \u001b[m\n",
      " # Measure and print the time taken\u001b[m\n",
      "\u001b[31m-print(f\"Local training time: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint(f\"Local training time: {t.time() - start_time:.2f} seconds, instance_type = {local_instance}\")\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/12-13:\u001b[0m\n",
      "\u001b[31m-  markdown cell:\n",
      "\u001b[31m-    id: 565c9155-45c8-4742-b30f-115cdc1f0047\n",
      "\u001b[31m-    source:\n",
      "\u001b[31m-      See which instances are available... (doesn't work yet???)\n",
      "\u001b[31m-  code cell:\n",
      "\u001b[31m-    id: 0e2820fa-8b0c-4eb9-8956-94e9395e3bf2\n",
      "\u001b[31m-    execution_count: 7\n",
      "\u001b[31m-    metadata (known keys):\n",
      "\u001b[31m-      tags:\n",
      "\u001b[31m-        []\n",
      "\u001b[31m-    source:\n",
      "\u001b[31m-      # !aws ec2 describe-instance-type-offerings --location-type us-east-1 \n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/39/execution_count:\u001b[0m\n",
      "\u001b[31m-  19\n",
      "\u001b[32m+  18\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/39/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      Epoch [100/1000], Loss: 0.3831, Val Loss: 0.4998, Val Accuracy: 0.8042\n",
      "\u001b[32m+      validation:accuracy = 0.8042\n",
      "\u001b[32m+      Epoch [200/1000], Loss: 0.3506, Val Loss: 0.5036, Val Accuracy: 0.7832\n",
      "\u001b[32m+      validation:accuracy = 0.7832\n",
      "\u001b[32m+      Epoch [300/1000], Loss: 0.3196, Val Loss: 0.5167, Val Accuracy: 0.7902\n",
      "\u001b[32m+      validation:accuracy = 0.7902\n",
      "\u001b[32m+      Epoch [400/1000], Loss: 0.2895, Val Loss: 0.5528, Val Accuracy: 0.7902\n",
      "\u001b[32m+      validation:accuracy = 0.7902\n",
      "\u001b[32m+      Epoch [500/1000], Loss: 0.2623, Val Loss: 0.5962, Val Accuracy: 0.7762\n",
      "\u001b[32m+      validation:accuracy = 0.7762\n",
      "\u001b[32m+      Epoch [600/1000], Loss: 0.2382, Val Loss: 0.6413, Val Accuracy: 0.7832\n",
      "\u001b[32m+      validation:accuracy = 0.7832\n",
      "\u001b[32m+      Epoch [700/1000], Loss: 0.2192, Val Loss: 0.6993, Val Accuracy: 0.7832\n",
      "\u001b[32m+      validation:accuracy = 0.7832\n",
      "\u001b[32m+      Epoch [800/1000], Loss: 0.2047, Val Loss: 0.7611, Val Accuracy: 0.7832\n",
      "\u001b[32m+      validation:accuracy = 0.7832\n",
      "\u001b[32m+      Epoch [900/1000], Loss: 0.1930, Val Loss: 0.8328, Val Accuracy: 0.7762\n",
      "\u001b[32m+      validation:accuracy = 0.7762\n",
      "\u001b[32m+      Epoch [1000/1000], Loss: 0.1828, Val Loss: 0.9100, Val Accuracy: 0.7762\n",
      "\u001b[32m+      validation:accuracy = 0.7762\n",
      "\u001b[32m+      Model saved to nn_model.pth\n",
      "\u001b[32m+      Local training time: 9.48 seconds, instance_type = ml.t3.medium\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/39/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      Epoch [100/1000], Loss: 0.3844, Val Loss: 0.5017, Val Accuracy: 0.8042\n",
      "\u001b[31m-      validation:accuracy = 0.8042\n",
      "\u001b[31m-      Epoch [200/1000], Loss: 0.3470, Val Loss: 0.4964, Val Accuracy: 0.8042\n",
      "\u001b[31m-      validation:accuracy = 0.8042\n",
      "\u001b[31m-      Epoch [300/1000], Loss: 0.3134, Val Loss: 0.5171, Val Accuracy: 0.7972\n",
      "\u001b[31m-      validation:accuracy = 0.7972\n",
      "\u001b[31m-      Epoch [400/1000], Loss: 0.2860, Val Loss: 0.5589, Val Accuracy: 0.7972\n",
      "\u001b[31m-      validation:accuracy = 0.7972\n",
      "\u001b[31m-      Epoch [500/1000], Loss: 0.2632, Val Loss: 0.5952, Val Accuracy: 0.7902\n",
      "\u001b[31m-      validation:accuracy = 0.7902\n",
      "\u001b[31m-      Epoch [600/1000], Loss: 0.2385, Val Loss: 0.6462, Val Accuracy: 0.7902\n",
      "\u001b[31m-      validation:accuracy = 0.7902\n",
      "\u001b[31m-      Epoch [700/1000], Loss: 0.2180, Val Loss: 0.7208, Val Accuracy: 0.7972\n",
      "\u001b[31m-      validation:accuracy = 0.7972\n",
      "\u001b[31m-      Epoch [800/1000], Loss: 0.2001, Val Loss: 0.8038, Val Accuracy: 0.7972\n",
      "\u001b[31m-      validation:accuracy = 0.7972\n",
      "\u001b[31m-      Epoch [900/1000], Loss: 0.1876, Val Loss: 0.9149, Val Accuracy: 0.7972\n",
      "\u001b[31m-      validation:accuracy = 0.7972\n",
      "\u001b[31m-      Epoch [1000/1000], Loss: 0.1774, Val Loss: 1.0214, Val Accuracy: 0.7832\n",
      "\u001b[31m-      validation:accuracy = 0.7832\n",
      "\u001b[31m-      Model saved to nn_model.pth\n",
      "\u001b[31m-      Local training time: 4.24 seconds\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/39/source:\u001b[0m\n",
      "\u001b[36m@@ -3,4 +3,4 @@\u001b[m \u001b[mimport torch\u001b[m\n",
      " # Measure training time locally\u001b[m\n",
      " start_time = t.time()\u001b[m\n",
      " %run  test_AWS/scripts/train_nn.py --train train_data.npz --val val_data.npz --epochs 1000 --learning_rate 0.001\u001b[m\n",
      "\u001b[31m-print(f\"Local training time: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mprint(f\"Local training time: {t.time() - start_time:.2f} seconds, instance_type = {local_instance}\")\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/41/source:\u001b[0m\n",
      "\u001b[36m@@ -4,7 +4,7 @@\u001b[m \u001b[mfrom sagemaker.inputs import TrainingInput\u001b[m\n",
      " epochs = 10000\u001b[m\n",
      " instance_count = 1\u001b[m\n",
      " instance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[31m-output_path = f's3://{bucket}/output_nn/'\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32moutput_path = f's3://{bucket}/output_nn/' # this folder will auto-generate if it doesn't exist already\u001b[m\n",
      " \u001b[m\n",
      " # Define the PyTorch estimator and pass hyperparameters as arguments\u001b[m\n",
      " pytorch_estimator = PyTorch(\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/46/execution_count:\u001b[0m\n",
      "\u001b[31m-  22\n",
      "\u001b[32m+  19\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/46/outputs/0/text:\u001b[0m\n",
      "\u001b[36m@@ -1 +1,2 @@\u001b[m\n",
      " INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mINFO:sagemaker:Creating training-job with name: pytorch-training-2024-11-03-19-26-20-935\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/46/outputs/1:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      2024-11-03 19:26:22 Starting - Starting the training job...\n",
      "\u001b[32m+      2024-11-03 19:26:36 Starting - Preparing the instances for training...\n",
      "\u001b[32m+      2024-11-03 19:27:07 Downloading - Downloading input data...\n",
      "\u001b[32m+      2024-11-03 19:27:27 Downloading - Downloading the training image...\n",
      "\u001b[32m+      2024-11-03 19:28:13 Training - Training image download completed. Training in progress..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32m+      \u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,425 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,427 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,437 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,439 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,747 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,758 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,769 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:23,778 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32m+      \u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[32m+      \u001b[35m{\n",
      "\u001b[32m+          \"additional_framework_parameters\": {},\n",
      "\u001b[32m+          \"channel_input_dirs\": {\n",
      "\u001b[32m+              \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[32m+              \"val\": \"/opt/ml/input/data/val\"\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"current_host\": \"algo-2\",\n",
      "\u001b[32m+          \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[32m+          \"hosts\": [\n",
      "\u001b[32m+              \"algo-1\",\n",
      "\u001b[32m+              \"algo-2\"\n",
      "\u001b[32m+          ],\n",
      "\u001b[32m+          \"hyperparameters\": {\n",
      "\u001b[32m+              \"epochs\": 10000,\n",
      "\u001b[32m+              \"learning_rate\": 0.001,\n",
      "\u001b[32m+              \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
      "\u001b[32m+              \"val\": \"/opt/ml/input/data/val/val_data.npz\"\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[32m+          \"input_data_config\": {\n",
      "\u001b[32m+              \"train\": {\n",
      "\u001b[32m+                  \"ContentType\": \"application/x-npz\",\n",
      "\u001b[32m+                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[32m+                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[32m+                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[32m+              },\n",
      "\u001b[32m+              \"val\": {\n",
      "\u001b[32m+                  \"ContentType\": \"application/x-npz\",\n",
      "\u001b[32m+                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[32m+                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[32m+                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[32m+              }\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[32m+          \"is_master\": false,\n",
      "\u001b[32m+          \"job_name\": \"pytorch-training-2024-11-03-19-26-20-935\",\n",
      "\u001b[32m+          \"log_level\": 20,\n",
      "\u001b[32m+          \"master_hostname\": \"algo-1\",\n",
      "\u001b[32m+          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[32m+          \"module_dir\": \"s3://titanic-dataset-test/pytorch-training-2024-11-03-19-26-20-935/source/sourcedir.tar.gz\",\n",
      "\u001b[32m+          \"module_name\": \"train_nn\",\n",
      "\u001b[32m+          \"network_interface_name\": \"eth0\",\n",
      "\u001b[32m+          \"num_cpus\": 4,\n",
      "\u001b[32m+          \"num_gpus\": 0,\n",
      "\u001b[32m+          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[32m+          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[32m+          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[32m+          \"resource_config\": {\n",
      "\u001b[32m+              \"current_host\": \"algo-2\",\n",
      "\u001b[32m+              \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "\u001b[32m+              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+              \"hosts\": [\n",
      "\u001b[32m+                  \"algo-1\",\n",
      "\u001b[32m+                  \"algo-2\"\n",
      "\u001b[32m+              ],\n",
      "\u001b[32m+              \"instance_groups\": [\n",
      "\u001b[32m+                  {\n",
      "\u001b[32m+                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+                      \"instance_type\": \"ml.m5.xlarge\",\n",
      "\u001b[32m+                      \"hosts\": [\n",
      "\u001b[32m+                          \"algo-1\",\n",
      "\u001b[32m+                          \"algo-2\"\n",
      "\u001b[32m+                      ]\n",
      "\u001b[32m+                  }\n",
      "\u001b[32m+              ],\n",
      "\u001b[32m+              \"network_interface_name\": \"eth0\"\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"user_entry_point\": \"train_nn.py\"\u001b[0m\n",
      "\u001b[32m+      \u001b[35m}\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_HPS={\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_USER_ENTRY_POINT=train_nn.py\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_MODULE_NAME=train_nn\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_MODULE_DIR=s3://titanic-dataset-test/pytorch-training-2024-11-03-19-26-20-935/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2024-11-03-19-26-20-935\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/pytorch-training-2024-11-03-19-26-20-935/source/sourcedir.tar.gz\",\"module_name\":\"train_nn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_nn.py\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_USER_ARGS=[\"--epochs\",\"10000\",\"--learning_rate\",\"0.001\",\"--train\",\"/opt/ml/input/data/train/train_data.npz\",\"--val\",\"/opt/ml/input/data/val/val_data.npz\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_HP_EPOCHS=10000\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_HP_TRAIN=/opt/ml/input/data/train/train_data.npz\u001b[0m\n",
      "\u001b[32m+      \u001b[35mSM_HP_VAL=/opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[32m+      \u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[32m+      \u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m+      \u001b[35m/opt/conda/bin/python3.8 train_nn.py --epochs 10000 --learning_rate 0.001 --train /opt/ml/input/data/train/train_data.npz --val /opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.481 algo-2:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.754 algo-2:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.754 algo-2:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.755 algo-2:27 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.755 algo-2:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.755 algo-2:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:591] name:fc1.weight count_params:448\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:591] name:fc1.bias count_params:64\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:591] name:fc2.weight count_params:2048\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:591] name:fc2.bias count_params:32\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:591] name:fc3.weight count_params:32\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:591] name:fc3.bias count_params:1\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:593] Total Trainable Params: 2625\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.756 algo-2:27 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[32m+      \u001b[35m[2024-11-03 19:28:24.758 algo-2:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [100/10000], Loss: 0.3817, Val Loss: 0.4938, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [200/10000], Loss: 0.3406, Val Loss: 0.4880, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [300/10000], Loss: 0.3084, Val Loss: 0.5172, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [400/10000], Loss: 0.2823, Val Loss: 0.5589, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [500/10000], Loss: 0.2578, Val Loss: 0.6041, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [600/10000], Loss: 0.2369, Val Loss: 0.6692, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32m+      \u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:22,916 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:22,917 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:22,926 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:22,928 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:23,102 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:23,112 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:23,122 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:23,130 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32m+      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[32m+      \u001b[34m{\n",
      "\u001b[32m+          \"additional_framework_parameters\": {},\n",
      "\u001b[32m+          \"channel_input_dirs\": {\n",
      "\u001b[32m+              \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[32m+              \"val\": \"/opt/ml/input/data/val\"\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"current_host\": \"algo-1\",\n",
      "\u001b[32m+          \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[32m+          \"hosts\": [\n",
      "\u001b[32m+              \"algo-1\",\n",
      "\u001b[32m+              \"algo-2\"\n",
      "\u001b[32m+          ],\n",
      "\u001b[32m+          \"hyperparameters\": {\n",
      "\u001b[32m+              \"epochs\": 10000,\n",
      "\u001b[32m+              \"learning_rate\": 0.001,\n",
      "\u001b[32m+              \"train\": \"/opt/ml/input/data/train/train_data.npz\",\n",
      "\u001b[32m+              \"val\": \"/opt/ml/input/data/val/val_data.npz\"\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[32m+          \"input_data_config\": {\n",
      "\u001b[32m+              \"train\": {\n",
      "\u001b[32m+                  \"ContentType\": \"application/x-npz\",\n",
      "\u001b[32m+                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[32m+                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[32m+                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[32m+              },\n",
      "\u001b[32m+              \"val\": {\n",
      "\u001b[32m+                  \"ContentType\": \"application/x-npz\",\n",
      "\u001b[32m+                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[32m+                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[32m+                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[32m+              }\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[32m+          \"is_master\": true,\n",
      "\u001b[32m+          \"job_name\": \"pytorch-training-2024-11-03-19-26-20-935\",\n",
      "\u001b[32m+          \"log_level\": 20,\n",
      "\u001b[32m+          \"master_hostname\": \"algo-1\",\n",
      "\u001b[32m+          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[32m+          \"module_dir\": \"s3://titanic-dataset-test/pytorch-training-2024-11-03-19-26-20-935/source/sourcedir.tar.gz\",\n",
      "\u001b[32m+          \"module_name\": \"train_nn\",\n",
      "\u001b[32m+          \"network_interface_name\": \"eth0\",\n",
      "\u001b[32m+          \"num_cpus\": 4,\n",
      "\u001b[32m+          \"num_gpus\": 0,\n",
      "\u001b[32m+          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[32m+          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[32m+          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[32m+          \"resource_config\": {\n",
      "\u001b[32m+              \"current_host\": \"algo-1\",\n",
      "\u001b[32m+              \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "\u001b[32m+              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+              \"hosts\": [\n",
      "\u001b[32m+                  \"algo-1\",\n",
      "\u001b[32m+                  \"algo-2\"\n",
      "\u001b[32m+              ],\n",
      "\u001b[32m+              \"instance_groups\": [\n",
      "\u001b[32m+                  {\n",
      "\u001b[32m+                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[32m+                      \"instance_type\": \"ml.m5.xlarge\",\n",
      "\u001b[32m+                      \"hosts\": [\n",
      "\u001b[32m+                          \"algo-1\",\n",
      "\u001b[32m+                          \"algo-2\"\n",
      "\u001b[32m+                      ]\n",
      "\u001b[32m+                  }\n",
      "\u001b[32m+              ],\n",
      "\u001b[32m+              \"network_interface_name\": \"eth0\"\n",
      "\u001b[32m+          },\n",
      "\u001b[32m+          \"user_entry_point\": \"train_nn.py\"\u001b[0m\n",
      "\u001b[32m+      \u001b[34m}\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_HPS={\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_USER_ENTRY_POINT=train_nn.py\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_MODULE_NAME=train_nn\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/pytorch-training-2024-11-03-19-26-20-935/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":10000,\"learning_rate\":0.001,\"train\":\"/opt/ml/input/data/train/train_data.npz\",\"val\":\"/opt/ml/input/data/val/val_data.npz\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"ContentType\":\"application/x-npz\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2024-11-03-19-26-20-935\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/pytorch-training-2024-11-03-19-26-20-935/source/sourcedir.tar.gz\",\"module_name\":\"train_nn\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_nn.py\"}\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_USER_ARGS=[\"--epochs\",\"10000\",\"--learning_rate\",\"0.001\",\"--train\",\"/opt/ml/input/data/train/train_data.npz\",\"--val\",\"/opt/ml/input/data/val/val_data.npz\"]\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_HP_EPOCHS=10000\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_HP_LEARNING_RATE=0.001\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_HP_TRAIN=/opt/ml/input/data/train/train_data.npz\u001b[0m\n",
      "\u001b[32m+      \u001b[34mSM_HP_VAL=/opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[32m+      \u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[32m+      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m+      \u001b[34m/opt/conda/bin/python3.8 train_nn.py --epochs 10000 --learning_rate 0.001 --train /opt/ml/input/data/train/train_data.npz --val /opt/ml/input/data/val/val_data.npz\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:23.781 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.026 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.026 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.026 algo-1:27 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.027 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.027 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.027 algo-1:27 INFO hook.py:591] name:fc1.weight count_params:448\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.027 algo-1:27 INFO hook.py:591] name:fc1.bias count_params:64\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.028 algo-1:27 INFO hook.py:591] name:fc2.weight count_params:2048\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.028 algo-1:27 INFO hook.py:591] name:fc2.bias count_params:32\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.028 algo-1:27 INFO hook.py:591] name:fc3.weight count_params:32\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.028 algo-1:27 INFO hook.py:591] name:fc3.bias count_params:1\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.028 algo-1:27 INFO hook.py:593] Total Trainable Params: 2625\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.028 algo-1:27 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[32m+      \u001b[34m[2024-11-03 19:28:24.030 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [100/10000], Loss: 0.3817, Val Loss: 0.4964, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [200/10000], Loss: 0.3483, Val Loss: 0.5007, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [300/10000], Loss: 0.3207, Val Loss: 0.5187, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [400/10000], Loss: 0.2887, Val Loss: 0.5505, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [500/10000], Loss: 0.2599, Val Loss: 0.6050, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [600/10000], Loss: 0.2370, Val Loss: 0.6641, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [700/10000], Loss: 0.2187, Val Loss: 0.7161, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [800/10000], Loss: 0.2041, Val Loss: 0.7852, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [900/10000], Loss: 0.1918, Val Loss: 0.8541, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1000/10000], Loss: 0.1813, Val Loss: 0.9329, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1100/10000], Loss: 0.1717, Val Loss: 1.0237, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1200/10000], Loss: 0.1636, Val Loss: 1.1262, Val Accuracy: 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [700/10000], Loss: 0.2177, Val Loss: 0.7132, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [800/10000], Loss: 0.2014, Val Loss: 0.7752, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [900/10000], Loss: 0.1882, Val Loss: 0.8502, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1000/10000], Loss: 0.1771, Val Loss: 0.9369, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1100/10000], Loss: 0.1693, Val Loss: 1.0047, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1200/10000], Loss: 0.1632, Val Loss: 1.0621, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1300/10000], Loss: 0.1572, Val Loss: 1.2208, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1400/10000], Loss: 0.1517, Val Loss: 1.3207, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1500/10000], Loss: 0.1469, Val Loss: 1.4089, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1600/10000], Loss: 0.1427, Val Loss: 2.0697, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1700/10000], Loss: 0.1389, Val Loss: 2.1479, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1300/10000], Loss: 0.1578, Val Loss: 1.1082, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1400/10000], Loss: 0.1527, Val Loss: 1.1449, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1500/10000], Loss: 0.1483, Val Loss: 1.1821, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1600/10000], Loss: 0.1444, Val Loss: 1.2240, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1700/10000], Loss: 0.1404, Val Loss: 1.2632, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1800/10000], Loss: 0.1352, Val Loss: 2.2225, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [1900/10000], Loss: 0.1319, Val Loss: 2.2846, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2000/10000], Loss: 0.1288, Val Loss: 2.3492, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2100/10000], Loss: 0.1259, Val Loss: 2.4118, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2200/10000], Loss: 0.1231, Val Loss: 2.4731, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2300/10000], Loss: 0.1203, Val Loss: 2.5413, Val Accuracy: 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1800/10000], Loss: 0.1363, Val Loss: 1.3063, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [1900/10000], Loss: 0.1332, Val Loss: 1.3442, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2000/10000], Loss: 0.1302, Val Loss: 1.3842, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2100/10000], Loss: 0.1274, Val Loss: 1.4188, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2200/10000], Loss: 0.1250, Val Loss: 1.4510, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2400/10000], Loss: 0.1178, Val Loss: 2.6070, Val Accuracy: 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2500/10000], Loss: 0.1156, Val Loss: 2.6837, Val Accuracy: 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2600/10000], Loss: 0.1133, Val Loss: 2.7545, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2700/10000], Loss: 0.1113, Val Loss: 2.8313, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2800/10000], Loss: 0.1092, Val Loss: 2.9076, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2300/10000], Loss: 0.1224, Val Loss: 1.4889, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2400/10000], Loss: 0.1197, Val Loss: 1.5353, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2500/10000], Loss: 0.1172, Val Loss: 1.5955, Val Accuracy: 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8252\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2600/10000], Loss: 0.1153, Val Loss: 1.6427, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [2900/10000], Loss: 0.1074, Val Loss: 2.9744, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3000/10000], Loss: 0.1054, Val Loss: 3.0698, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3100/10000], Loss: 0.1034, Val Loss: 3.1492, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3200/10000], Loss: 0.1011, Val Loss: 3.2379, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3300/10000], Loss: 0.0992, Val Loss: 3.3238, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2700/10000], Loss: 0.1128, Val Loss: 1.6760, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2800/10000], Loss: 0.1108, Val Loss: 1.7083, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [2900/10000], Loss: 0.1090, Val Loss: 1.7533, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3000/10000], Loss: 0.1074, Val Loss: 1.7878, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3400/10000], Loss: 0.0978, Val Loss: 3.4100, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3500/10000], Loss: 0.0960, Val Loss: 3.5138, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3600/10000], Loss: 0.0946, Val Loss: 3.5999, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3700/10000], Loss: 0.0933, Val Loss: 3.6751, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3800/10000], Loss: 0.0919, Val Loss: 3.8341, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [3900/10000], Loss: 0.0906, Val Loss: 3.9011, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3100/10000], Loss: 0.1058, Val Loss: 1.8364, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3200/10000], Loss: 0.1045, Val Loss: 1.8726, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3300/10000], Loss: 0.1031, Val Loss: 1.9103, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3400/10000], Loss: 0.1015, Val Loss: 2.5300, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3500/10000], Loss: 0.1002, Val Loss: 2.5589, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4000/10000], Loss: 0.0892, Val Loss: 3.9705, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4100/10000], Loss: 0.0880, Val Loss: 4.0273, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4200/10000], Loss: 0.0869, Val Loss: 4.1086, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4300/10000], Loss: 0.0859, Val Loss: 4.7465, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4400/10000], Loss: 0.0848, Val Loss: 4.8959, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3600/10000], Loss: 0.0991, Val Loss: 2.5962, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3700/10000], Loss: 0.0979, Val Loss: 2.6321, Val Accuracy: 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8182\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3800/10000], Loss: 0.0968, Val Loss: 2.6670, Val Accuracy: 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8112\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [3900/10000], Loss: 0.0957, Val Loss: 2.7171, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4000/10000], Loss: 0.0945, Val Loss: 2.7521, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4100/10000], Loss: 0.0943, Val Loss: 2.8044, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4500/10000], Loss: 0.0838, Val Loss: 4.9303, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4600/10000], Loss: 0.0828, Val Loss: 4.9825, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4700/10000], Loss: 0.0821, Val Loss: 5.0295, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4800/10000], Loss: 0.0810, Val Loss: 5.0845, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [4900/10000], Loss: 0.0803, Val Loss: 5.1317, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4200/10000], Loss: 0.0926, Val Loss: 2.8296, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4300/10000], Loss: 0.0918, Val Loss: 2.8715, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4400/10000], Loss: 0.0907, Val Loss: 2.9057, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4500/10000], Loss: 0.0899, Val Loss: 2.9417, Val Accuracy: 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.8042\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4600/10000], Loss: 0.0892, Val Loss: 2.9764, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5000/10000], Loss: 0.0794, Val Loss: 5.1779, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5100/10000], Loss: 0.0787, Val Loss: 5.2246, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5200/10000], Loss: 0.0779, Val Loss: 5.8573, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5300/10000], Loss: 0.0772, Val Loss: 5.9127, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5400/10000], Loss: 0.0765, Val Loss: 5.9509, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5500/10000], Loss: 0.0760, Val Loss: 5.4174, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5600/10000], Loss: 0.0752, Val Loss: 6.0417, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5700/10000], Loss: 0.0745, Val Loss: 6.0788, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5800/10000], Loss: 0.0742, Val Loss: 5.5440, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [5900/10000], Loss: 0.0737, Val Loss: 6.1636, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6000/10000], Loss: 0.0727, Val Loss: 5.6196, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4700/10000], Loss: 0.0883, Val Loss: 3.0146, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4800/10000], Loss: 0.0876, Val Loss: 3.0436, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [4900/10000], Loss: 0.0874, Val Loss: 3.6651, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5000/10000], Loss: 0.0862, Val Loss: 3.7035, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5100/10000], Loss: 0.0857, Val Loss: 3.7255, Val Accuracy: 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7972\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5200/10000], Loss: 0.0848, Val Loss: 3.7617, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5300/10000], Loss: 0.0841, Val Loss: 3.7893, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5400/10000], Loss: 0.0833, Val Loss: 4.4059, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5500/10000], Loss: 0.0827, Val Loss: 4.4443, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5600/10000], Loss: 0.0821, Val Loss: 4.4668, Val Accuracy: 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7902\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6100/10000], Loss: 0.0722, Val Loss: 5.6597, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6200/10000], Loss: 0.0720, Val Loss: 5.7019, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6300/10000], Loss: 0.0712, Val Loss: 5.7325, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6400/10000], Loss: 0.0706, Val Loss: 5.7721, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6500/10000], Loss: 0.0707, Val Loss: 5.8273, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5700/10000], Loss: 0.0815, Val Loss: 4.4949, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5800/10000], Loss: 0.0810, Val Loss: 4.6022, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [5900/10000], Loss: 0.0806, Val Loss: 4.6233, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6000/10000], Loss: 0.0801, Val Loss: 4.6322, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6100/10000], Loss: 0.0796, Val Loss: 4.6533, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6200/10000], Loss: 0.0792, Val Loss: 4.6714, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6600/10000], Loss: 0.0697, Val Loss: 5.8669, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6700/10000], Loss: 0.0693, Val Loss: 5.9007, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6800/10000], Loss: 0.0688, Val Loss: 5.9453, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [6900/10000], Loss: 0.0683, Val Loss: 5.9956, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7000/10000], Loss: 0.0681, Val Loss: 6.0157, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7100/10000], Loss: 0.0676, Val Loss: 6.0810, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6300/10000], Loss: 0.0787, Val Loss: 4.6912, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6400/10000], Loss: 0.0783, Val Loss: 4.7043, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6500/10000], Loss: 0.0777, Val Loss: 4.7053, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6600/10000], Loss: 0.0774, Val Loss: 4.7221, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6700/10000], Loss: 0.0770, Val Loss: 4.7515, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7200/10000], Loss: 0.0672, Val Loss: 6.1206, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7300/10000], Loss: 0.0669, Val Loss: 6.1590, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7400/10000], Loss: 0.0667, Val Loss: 6.1948, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7500/10000], Loss: 0.0662, Val Loss: 6.2573, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7600/10000], Loss: 0.0660, Val Loss: 6.2843, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6800/10000], Loss: 0.0768, Val Loss: 4.7510, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [6900/10000], Loss: 0.0765, Val Loss: 4.7717, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7000/10000], Loss: 0.0760, Val Loss: 4.7891, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7100/10000], Loss: 0.0758, Val Loss: 4.8042, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7200/10000], Loss: 0.0753, Val Loss: 4.8343, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7700/10000], Loss: 0.0658, Val Loss: 6.4063, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7800/10000], Loss: 0.0653, Val Loss: 6.4454, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [7900/10000], Loss: 0.0650, Val Loss: 6.4795, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8000/10000], Loss: 0.0650, Val Loss: 6.5165, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8100/10000], Loss: 0.0645, Val Loss: 6.5612, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7300/10000], Loss: 0.0750, Val Loss: 5.4208, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7400/10000], Loss: 0.0748, Val Loss: 5.4417, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7500/10000], Loss: 0.0745, Val Loss: 5.4604, Val Accuracy: 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7832\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7600/10000], Loss: 0.0740, Val Loss: 5.4635, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7700/10000], Loss: 0.0736, Val Loss: 5.4795, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8200/10000], Loss: 0.0643, Val Loss: 6.5963, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8300/10000], Loss: 0.0645, Val Loss: 6.6226, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8400/10000], Loss: 0.0639, Val Loss: 6.6634, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8500/10000], Loss: 0.0637, Val Loss: 6.7014, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8600/10000], Loss: 0.0633, Val Loss: 6.7258, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7800/10000], Loss: 0.0734, Val Loss: 5.4985, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [7900/10000], Loss: 0.0731, Val Loss: 5.5160, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8000/10000], Loss: 0.0729, Val Loss: 5.5369, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8100/10000], Loss: 0.0725, Val Loss: 5.5557, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8200/10000], Loss: 0.0724, Val Loss: 5.5668, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8700/10000], Loss: 0.0631, Val Loss: 6.7539, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8800/10000], Loss: 0.0628, Val Loss: 7.3786, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [8900/10000], Loss: 0.0630, Val Loss: 7.4002, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9000/10000], Loss: 0.0624, Val Loss: 7.4383, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9100/10000], Loss: 0.0626, Val Loss: 7.4461, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9200/10000], Loss: 0.0621, Val Loss: 7.4802, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8300/10000], Loss: 0.0721, Val Loss: 5.5789, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8400/10000], Loss: 0.0720, Val Loss: 5.5991, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8500/10000], Loss: 0.0717, Val Loss: 5.6040, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8600/10000], Loss: 0.0714, Val Loss: 5.6202, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8700/10000], Loss: 0.0717, Val Loss: 5.6290, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9300/10000], Loss: 0.0619, Val Loss: 7.5016, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9400/10000], Loss: 0.0616, Val Loss: 7.5316, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9500/10000], Loss: 0.0614, Val Loss: 7.5564, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9600/10000], Loss: 0.0612, Val Loss: 7.5835, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9700/10000], Loss: 0.0611, Val Loss: 7.6275, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8800/10000], Loss: 0.0715, Val Loss: 5.6322, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [8900/10000], Loss: 0.0712, Val Loss: 5.6510, Val Accuracy: 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7692\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9000/10000], Loss: 0.0707, Val Loss: 5.6692, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9100/10000], Loss: 0.0705, Val Loss: 5.6811, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9200/10000], Loss: 0.0703, Val Loss: 5.7019, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9800/10000], Loss: 0.0608, Val Loss: 7.6395, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [9900/10000], Loss: 0.0607, Val Loss: 7.7493, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mEpoch [10000/10000], Loss: 0.0607, Val Loss: 7.7741, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[34mModel saved to /opt/ml/model/nn_model.pth\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:43,306 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:43,306 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m+      \u001b[34m2024-11-03 19:28:43,306 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9300/10000], Loss: 0.0702, Val Loss: 5.7045, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9400/10000], Loss: 0.0700, Val Loss: 5.7266, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9500/10000], Loss: 0.0698, Val Loss: 5.7424, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9600/10000], Loss: 0.0701, Val Loss: 5.7717, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9700/10000], Loss: 0.0706, Val Loss: 5.7622, Val Accuracy: 0.7622\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7622\u001b[0m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      2024-11-03 19:28:48 Uploading - Uploading generated training model\u001b[35mEpoch [9800/10000], Loss: 0.0697, Val Loss: 5.7971, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [9900/10000], Loss: 0.0695, Val Loss: 6.3940, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mEpoch [10000/10000], Loss: 0.0691, Val Loss: 6.3955, Val Accuracy: 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mvalidation:accuracy = 0.7762\u001b[0m\n",
      "\u001b[32m+      \u001b[35mModel saved to /opt/ml/model/nn_model.pth\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:44,659 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:44,659 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m+      \u001b[35m2024-11-03 19:28:44,659 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      2024-11-03 19:29:01 Completed - Training job completed\n",
      "\u001b[32m+      Training seconds: 228\n",
      "\u001b[32m+      Billable seconds: 228\n",
      "\u001b[32m+      Runtime for training on SageMaker: 198.54 seconds, instance_type: ml.m5.xlarge, instance_count: 2\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/46/outputs/1:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: error\n",
      "\u001b[31m-    ename: FileNotFoundError\n",
      "\u001b[31m-    evalue: [Errno 2] No such file or directory: 'test_AWS/train_nn.py'\n",
      "\u001b[31m-    traceback:\n",
      "\u001b[31m-      item[0]: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m-      item[1]: \u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[31m-      item[2]:\n",
      "\u001b[31m-        Cell \u001b[0;32mIn[22], line 28\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Start the training job and time it\u001b[39;00m\n",
      "\u001b[31m-        \u001b[1;32m     27\u001b[0m start \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[31m-        \u001b[0;32m---> 28\u001b[0m \u001b[43mpytorch_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m     29\u001b[0m end \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[31m-        \u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime for training on SageMaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, instance_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, instance_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[31m-      item[3]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n",
      "\u001b[31m-        \u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[31m-        \u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-      item[4]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1366\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   1301\u001b[0m \u001b[38;5;129m@runnable_by_pipeline\u001b[39m\n",
      "\u001b[31m-        \u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n",
      "\u001b[31m-        \u001b[1;32m   1303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[31m-        \u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   1308\u001b[0m     experiment_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[31m-        \u001b[1;32m   1309\u001b[0m ):\n",
      "\u001b[31m-        \u001b[1;32m   1310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train a model using the input training dataset.\u001b[39;00m\n",
      "\u001b[31m-        \u001b[1;32m   1311\u001b[0m \n",
      "\u001b[31m-        \u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    The API calls the Amazon SageMaker CreateTrainingJob API to start\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   1364\u001b[0m \u001b[38;5;124;03m        :class:`~sagemaker.workflow.pipeline_context.PipelineSession`\u001b[39;00m\n",
      "\u001b[31m-        \u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1368\u001b[0m     experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n",
      "\u001b[31m-        \u001b[1;32m   1369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m _TrainingJob\u001b[38;5;241m.\u001b[39mstart_new(\u001b[38;5;28mself\u001b[39m, inputs, experiment_config)\n",
      "\u001b[31m-      item[5]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:3589\u001b[0m, in \u001b[0;36mFramework._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31m-        \u001b[1;32m   3582\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set hyperparameters needed for training. This method will also validate ``source_dir``.\u001b[39;00m\n",
      "\u001b[31m-        \u001b[1;32m   3583\u001b[0m \n",
      "\u001b[31m-        \u001b[1;32m   3584\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   3587\u001b[0m \u001b[38;5;124;03m            constructor if applicable.\u001b[39;00m\n",
      "\u001b[31m-        \u001b[1;32m   3588\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m-> 3589\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFramework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_for_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   3591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_set_debugger_configs()\n",
      "\u001b[31m-      item[6]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:964\u001b[0m, in \u001b[0;36mEstimatorBase._prepare_for_training\u001b[0;34m(self, job_name)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m    962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code\u001b[38;5;241m.\u001b[39ms3_prefix\n",
      "\u001b[31m-        \u001b[1;32m    963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m-        \u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stage_user_code_in_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m    965\u001b[0m     code_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code\u001b[38;5;241m.\u001b[39ms3_prefix\n",
      "\u001b[31m-        \u001b[1;32m    966\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muploaded_code\u001b[38;5;241m.\u001b[39mscript_name\n",
      "\u001b[31m-      item[7]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1046\u001b[0m, in \u001b[0;36mEstimatorBase._stage_user_code_in_s3\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   1043\u001b[0m             output_bucket, _ \u001b[38;5;241m=\u001b[39m parse_s3_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_path)\n",
      "\u001b[31m-        \u001b[1;32m   1044\u001b[0m             kms_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_kms_key \u001b[38;5;28;01mif\u001b[39;00m code_bucket \u001b[38;5;241m==\u001b[39m output_bucket \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtar_and_upload_dir\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_bucket\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_s3_prefix\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscript\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdependencies\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3_resource\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   1056\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-      item[8]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/fw_utils.py:456\u001b[0m, in \u001b[0;36mtar_and_upload_dir\u001b[0;34m(session, bucket, s3_key_prefix, script, directory, dependencies, kms_key, s3_resource, settings)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31m-        \u001b[1;32m    455\u001b[0m     source_files \u001b[38;5;241m=\u001b[39m _list_files_to_compress(script, directory) \u001b[38;5;241m+\u001b[39m dependencies\n",
      "\u001b[31m-        \u001b[0;32m--> 456\u001b[0m     tar_file \u001b[38;5;241m=\u001b[39m \u001b[43msagemaker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tar_file\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_TAR_SOURCE_FILENAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kms_key:\n",
      "\u001b[31m-        \u001b[1;32m    461\u001b[0m         extra_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServerSideEncryption\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws:kms\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSEKMSKeyId\u001b[39m\u001b[38;5;124m\"\u001b[39m: kms_key}\n",
      "\u001b[31m-      item[9]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/utils.py:470\u001b[0m, in \u001b[0;36mcreate_tar_file\u001b[0;34m(source_files, target)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tarfile\u001b[38;5;241m.\u001b[39mopen(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw:gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, dereference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m t:\n",
      "\u001b[31m-        \u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sf \u001b[38;5;129;01min\u001b[39;00m source_files:\n",
      "\u001b[31m-        \u001b[1;32m    469\u001b[0m         \u001b[38;5;66;03m# Add all files from the directory into the root of the directory structure of the tar\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m--> 470\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filename\n",
      "\u001b[31m-      item[10]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/tarfile.py:2164\u001b[0m, in \u001b[0;36mTarFile.add\u001b[0;34m(self, name, arcname, recursive, filter)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   2161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, name)\n",
      "\u001b[31m-        \u001b[1;32m   2163\u001b[0m \u001b[38;5;66;03m# Create a TarInfo object from the file.\u001b[39;00m\n",
      "\u001b[31m-        \u001b[0;32m-> 2164\u001b[0m tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgettarinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marcname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   2166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tarinfo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31m-        \u001b[1;32m   2167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dbg(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarfile: Unsupported type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "\u001b[31m-      item[11]:\n",
      "\u001b[31m-        File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/tarfile.py:2039\u001b[0m, in \u001b[0;36mTarFile.gettarinfo\u001b[0;34m(self, name, arcname, fileobj)\u001b[0m\n",
      "\u001b[31m-        \u001b[1;32m   2037\u001b[0m         statres \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlstat(name)\n",
      "\u001b[31m-        \u001b[1;32m   2038\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m-        \u001b[0;32m-> 2039\u001b[0m         statres \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31m-        \u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m-        \u001b[1;32m   2041\u001b[0m     statres \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfstat(fileobj\u001b[38;5;241m.\u001b[39mfileno())\n",
      "\u001b[31m-      item[12]: \u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_AWS/train_nn.py'\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/46/source:\u001b[0m\n",
      "\u001b[36m@@ -1,5 +1,11 @@\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mfrom sagemaker.pytorch import PyTorch\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mfrom sagemaker.inputs import TrainingInput\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mimport time as t\u001b[m\n",
      "\u001b[32m+\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32mepochs = 10000\u001b[m\n",
      " instance_count = 2 # increasing to 2 to see if it has any benefit (likely won't see any with this small dataset)\u001b[m\n",
      " instance_type=\"ml.m5.xlarge\"\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32moutput_path = f's3://{bucket}/output_nn/'\u001b[m\n",
      " \u001b[m\n",
      " # Define the PyTorch estimator and pass hyperparameters as arguments\u001b[m\n",
      " pytorch_estimator = PyTorch(\u001b[m\n",
      "\n",
      "\u001b[0mnbdiff /tmp/git-blob-axEjd3/update-repo.ipynb update-repo.ipynb\n",
      "--- /tmp/git-blob-axEjd3/update-repo.ipynb  2024-11-03 19:52:19.586920\n",
      "+++ update-repo.ipynb  2024-11-03 19:52:10.822966\n",
      "\u001b[34m\u001b[1m## replaced /cells/0/execution_count:\u001b[0m\n",
      "\u001b[31m-  1\n",
      "\u001b[32m+  8\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/1/execution_count:\u001b[0m\n",
      "\u001b[31m-  6\n",
      "\u001b[32m+  13\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/2/execution_count:\u001b[0m\n",
      "\u001b[31m-  7\n",
      "\u001b[32m+  14\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/2/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      /home/ec2-user/SageMaker\n",
      "\u001b[32m+      /home/ec2-user/SageMaker\n",
      "\u001b[32m+      lost+found    requirements.txt\ttitanic_model.pth  train_data.npz\n",
      "\u001b[32m+      model.joblib  results.txt\ttitanic_test.csv   train_large.csv\n",
      "\u001b[32m+      model.tar.gz  test_AWS\t\ttitanic_train.csv  val_data.npz\n",
      "\u001b[32m+      nn_model.pth  test.csv\t\ttrain.csv\t   xgboost-model\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/2/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      /home/ec2-user/SageMaker/test_AWS\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/2/source:\u001b[0m\n",
      "\u001b[36m@@ -1,2 +1,3 @@\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m%cd /home/ec2-user/SageMaker/test_AWS\u001b[m\n",
      " !pwd\u001b[m\n",
      "\u001b[31m-# !cd test_AWS\u001b[m\n",
      "\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m!ls\u001b[m\n",
      "\u001b[m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-  8\n",
      "\u001b[32m+  15\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/3/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      warning: Not a git repository. Use --no-index to compare two paths outside a working tree\n",
      "\u001b[32m+      usage: git diff --no-index [<options>] <path> <path>\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      Diff output format options\n",
      "\u001b[32m+          -p, --patch           generate patch\n",
      "\u001b[32m+          -s, --no-patch        suppress diff output\n",
      "\u001b[32m+          -u                    generate patch\n",
      "\u001b[32m+          -U, --unified[=<n>]   generate diffs with <n> lines context\n",
      "\u001b[32m+          -W, --function-context\n",
      "\u001b[32m+                                generate diffs with <n> lines context\n",
      "\u001b[32m+          --raw                 generate the diff in raw format\n",
      "\u001b[32m+          --patch-with-raw      synonym for '-p --raw'\n",
      "\u001b[32m+          --patch-with-stat     synonym for '-p --stat'\n",
      "\u001b[32m+          --numstat             machine friendly --stat\n",
      "\u001b[32m+          --shortstat           output only the last line of --stat\n",
      "\u001b[32m+          -X, --dirstat[=<param1,param2>...]\n",
      "\u001b[32m+                                output the distribution of relative amount of changes for each sub-directory\n",
      "\u001b[32m+          --cumulative          synonym for --dirstat=cumulative\n",
      "\u001b[32m+          --dirstat-by-file[=<param1,param2>...]\n",
      "\u001b[32m+                                synonym for --dirstat=files,param1,param2...\n",
      "\u001b[32m+          --check               warn if changes introduce conflict markers or whitespace errors\n",
      "\u001b[32m+          --summary             condensed summary such as creations, renames and mode changes\n",
      "\u001b[32m+          --name-only           show only names of changed files\n",
      "\u001b[32m+          --name-status         show only names and status of changed files\n",
      "\u001b[32m+          --stat[=<width>[,<name-width>[,<count>]]]\n",
      "\u001b[32m+                                generate diffstat\n",
      "\u001b[32m+          --stat-width <width>  generate diffstat with a given width\n",
      "\u001b[32m+          --stat-name-width <width>\n",
      "\u001b[32m+                                generate diffstat with a given name width\n",
      "\u001b[32m+          --stat-graph-width <width>\n",
      "\u001b[32m+                                generate diffstat with a given graph width\n",
      "\u001b[32m+          --stat-count <count>  generate diffstat with limited lines\n",
      "\u001b[32m+          --compact-summary     generate compact summary in diffstat\n",
      "\u001b[32m+          --binary              output a binary diff that can be applied\n",
      "\u001b[32m+          --full-index          show full pre- and post-image object names on the \"index\" lines\n",
      "\u001b[32m+          --color[=<when>]      show colored diff\n",
      "\u001b[32m+          --ws-error-highlight <kind>\n",
      "\u001b[32m+                                highlight whitespace errors in the 'context', 'old' or 'new' lines in the diff\n",
      "\u001b[32m+          -z                    do not munge pathnames and use NULs as output field terminators in --raw or --numstat\n",
      "\u001b[32m+          --abbrev[=<n>]        use <n> digits to display object names\n",
      "\u001b[32m+          --src-prefix <prefix>\n",
      "\u001b[32m+                                show the given source prefix instead of \"a/\"\n",
      "\u001b[32m+          --dst-prefix <prefix>\n",
      "\u001b[32m+                                show the given destination prefix instead of \"b/\"\n",
      "\u001b[32m+          --line-prefix <prefix>\n",
      "\u001b[32m+                                prepend an additional prefix to every line of output\n",
      "\u001b[32m+          --no-prefix           do not show any source or destination prefix\n",
      "\u001b[32m+          --inter-hunk-context <n>\n",
      "\u001b[32m+                                show context between diff hunks up to the specified number of lines\n",
      "\u001b[32m+          --output-indicator-new <char>\n",
      "\u001b[32m+                                specify the character to indicate a new line instead of '+'\n",
      "\u001b[32m+          --output-indicator-old <char>\n",
      "\u001b[32m+                                specify the character to indicate an old line instead of '-'\n",
      "\u001b[32m+          --output-indicator-context <char>\n",
      "\u001b[32m+                                specify the character to indicate a context instead of ' '\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      Diff rename options\n",
      "\u001b[32m+          -B, --break-rewrites[=<n>[/<m>]]\n",
      "\u001b[32m+                                break complete rewrite changes into pairs of delete and create\n",
      "\u001b[32m+          -M, --find-renames[=<n>]\n",
      "\u001b[32m+                                detect renames\n",
      "\u001b[32m+          -D, --irreversible-delete\n",
      "\u001b[32m+                                omit the preimage for deletes\n",
      "\u001b[32m+          -C, --find-copies[=<n>]\n",
      "\u001b[32m+                                detect copies\n",
      "\u001b[32m+          --find-copies-harder  use unmodified files as source to find copies\n",
      "\u001b[32m+          --no-renames          disable rename detection\n",
      "\u001b[32m+          --rename-empty        use empty blobs as rename source\n",
      "\u001b[32m+          --follow              continue listing the history of a file beyond renames\n",
      "\u001b[32m+          -l <n>                prevent rename/copy detection if the number of rename/copy targets exceeds given limit\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      Diff algorithm options\n",
      "\u001b[32m+          --minimal             produce the smallest possible diff\n",
      "\u001b[32m+          -w, --ignore-all-space\n",
      "\u001b[32m+                                ignore whitespace when comparing lines\n",
      "\u001b[32m+          -b, --ignore-space-change\n",
      "\u001b[32m+                                ignore changes in amount of whitespace\n",
      "\u001b[32m+          --ignore-space-at-eol\n",
      "\u001b[32m+                                ignore changes in whitespace at EOL\n",
      "\u001b[32m+          --ignore-cr-at-eol    ignore carrier-return at the end of line\n",
      "\u001b[32m+          --ignore-blank-lines  ignore changes whose lines are all blank\n",
      "\u001b[32m+          -I, --ignore-matching-lines <regex>\n",
      "\u001b[32m+                                ignore changes whose all lines match <regex>\n",
      "\u001b[32m+          --indent-heuristic    heuristic to shift diff hunk boundaries for easy reading\n",
      "\u001b[32m+          --patience            generate diff using the \"patience diff\" algorithm\n",
      "\u001b[32m+          --histogram           generate diff using the \"histogram diff\" algorithm\n",
      "\u001b[32m+          --diff-algorithm <algorithm>\n",
      "\u001b[32m+                                choose a diff algorithm\n",
      "\u001b[32m+          --anchored <text>     generate diff using the \"anchored diff\" algorithm\n",
      "\u001b[32m+          --word-diff[=<mode>]  show word diff, using <mode> to delimit changed words\n",
      "\u001b[32m+          --word-diff-regex <regex>\n",
      "\u001b[32m+                                use <regex> to decide what a word is\n",
      "\u001b[32m+          --color-words[=<regex>]\n",
      "\u001b[32m+                                equivalent to --word-diff=color --word-diff-regex=<regex>\n",
      "\u001b[32m+          --color-moved[=<mode>]\n",
      "\u001b[32m+                                moved lines of code are colored differently\n",
      "\u001b[32m+          --color-moved-ws <mode>\n",
      "\u001b[32m+                                how white spaces are ignored in --color-moved\n",
      "\u001b[32m+      \n",
      "\u001b[32m+      Other diff options\n",
      "\u001b[32m+          --relative[=<prefix>]\n",
      "\u001b[32m+                                when run from subdir, exclude changes outside and show relative paths\n",
      "\u001b[32m+          -a, --text            treat all files as text\n",
      "\u001b[32m+          -R                    swap two inputs, reverse the diff\n",
      "\u001b[32m+          --exit-code           exit with 1 if there were differences, 0 otherwise\n",
      "\u001b[32m+          --quiet               disable all output of the program\n",
      "\u001b[32m+          --ext-diff            allow an external diff helper to be executed\n",
      "\u001b[32m+          --textconv            run external text conversion filters when comparing binary files\n",
      "\u001b[32m+          --ignore-submodules[=<when>]\n",
      "\u001b[32m+                                ignore changes to submodules in the diff generation\n",
      "\u001b[32m+          --submodule[=<format>]\n",
      "\u001b[32m+                                specify how differences in submodules are shown\n",
      "\u001b[32m+          --ita-invisible-in-index\n",
      "\u001b[32m+                                hide 'git add -N' entries from the index\n",
      "\u001b[32m+          --ita-visible-in-index\n",
      "\u001b[32m+                                treat 'git add -N' entries as real in the index\n",
      "\u001b[32m+          -S <string>           look for differences that change the number of occurrences of the specified string\n",
      "\u001b[32m+          -G <regex>            look for differences that change the number of occurrences of the specified regex\n",
      "\u001b[32m+          --pickaxe-all         show all changes in the changeset with -S or -G\n",
      "\u001b[32m+          --pickaxe-regex       treat <string> in -S as extended POSIX regular expression\n",
      "\u001b[32m+          -O <file>             control the order in which files appear in the output\n",
      "\u001b[32m+          --rotate-to <path>    show the change in the specified path first\n",
      "\u001b[32m+          --skip-to <path>      skip the output to the specified path\n",
      "\u001b[32m+          --find-object <object-id>\n",
      "\u001b[32m+                                look for differences that change the number of occurrences of the specified object\n",
      "\u001b[32m+          --diff-filter [(A|C|D|M|R|T|U|X|B)...[*]]\n",
      "\u001b[32m+                                select files by diff type\n",
      "\u001b[32m+          --output <file>       output to a specific file\n",
      "\u001b[32m+      \n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/3/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      nbdiff /tmp/git-blob-7tPqMh/05_Intro-train-models.ipynb 05_Intro-train-models.ipynb\n",
      "\u001b[31m-      --- /tmp/git-blob-7tPqMh/05_Intro-train-models.ipynb  2024-11-01 23:01:32.936419\n",
      "\u001b[31m-      +++ 05_Intro-train-models.ipynb  2024-11-01 23:00:25.168139\n",
      "\u001b[31m-      \u001b[34m\u001b[1m## replaced /cells/1/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  25\n",
      "\u001b[31m-      \u001b[32m+  1\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/1/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "\u001b[31m-      \u001b[32m+      sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  26\n",
      "\u001b[31m-      \u001b[32m+  2\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/5/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  27\n",
      "\u001b[31m-      \u001b[32m+  3\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## modified /cells/5/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[31m-      \u001b[31m-File downloaded: ./titanic_test.csv\u001b[m\n",
      "\u001b[31m-      \u001b[32m+\u001b[m\u001b[32mFile downloaded: ./titanic_train.csv\u001b[m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/7/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  3\n",
      "\u001b[31m-      \u001b[32m+  4\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/9/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  28\n",
      "\u001b[31m-      \u001b[32m+  5\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/11/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  31\n",
      "\u001b[31m-      \u001b[32m+  6\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## modified /cells/11/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[31m-      \u001b[31m-Training time: 0.06 seconds\u001b[m\n",
      "\u001b[31m-      \u001b[32m+\u001b[m\u001b[32mTraining time: 0.07 seconds\u001b[m\n",
      "\u001b[31m-       Model saved to ./xgboost-model\u001b[m\n",
      "\u001b[31m-      \u001b[31m-Local training time: 0.10 seconds\u001b[m\n",
      "\u001b[31m-      \u001b[32m+\u001b[m\u001b[32mLocal training time: 1.10 seconds\u001b[m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/11/outputs/1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stderr\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "\u001b[31m-      \u001b[32m+      Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "\u001b[31m-      \u001b[32m+        warnings.warn(\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/13/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  34\n",
      "\u001b[31m-      \u001b[32m+  7\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/13/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-      An error occurred (UnauthorizedOperation) when calling the DescribeInstanceTypeOfferings operation: You are not authorized to perform this operation. User: arn:aws:sts::183295408236:assumed-role/ml-sagemaker-use/SageMaker is not authorized to perform: ec2:DescribeInstanceTypeOfferings because no identity-based policy allows the ec2:DescribeInstanceTypeOfferings action\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/15/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  37\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/15/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stderr\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-58-45-158\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      2024-11-01 22:58:46 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[32m+      2024-11-01 22:59:01 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[32m+      2024-11-01 22:59:25 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[32m+      2024-11-01 23:00:05 Downloading - Downloading the training image.\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/15/outputs/0-1:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.large.\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-25-59-504\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:26:01 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:26:17 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:26:41 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:27:21 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:28:32 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:28:32 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:28:22.062 ip-10-0-251-187.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:28:22.085 ip-10-0-251-187.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:22:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mProcessing /opt/ml/code\n",
      "\u001b[31m-      \u001b[31m-        Preparing metadata (setup.py): started\n",
      "\u001b[31m-      \u001b[31m-        Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "\u001b[31m-      \u001b[31m-        Building wheel for train-xgboost (setup.py): started\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m  Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "\u001b[31m-      \u001b[31m-        Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=10623 sha256=b31dfae0c3499467efbbc51c882ab217c3c21cd87f27d0dcd8ec2d51b58b77de\n",
      "\u001b[31m-      \u001b[31m-        Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-2l39uija/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:28:24:INFO] Invoking user script\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m{\n",
      "\u001b[31m-      \u001b[31m-          \"additional_framework_parameters\": {},\n",
      "\u001b[31m-      \u001b[31m-          \"channel_input_dirs\": {\n",
      "\u001b[31m-      \u001b[31m-              \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[31m-      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[31m-          \"current_host\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[31m-          \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "\u001b[31m-      \u001b[31m-          \"hosts\": [\n",
      "\u001b[31m-      \u001b[31m-              \"algo-1\"\n",
      "\u001b[31m-      \u001b[31m-          ],\n",
      "\u001b[31m-      \u001b[31m-          \"hyperparameters\": {\n",
      "\u001b[31m-      \u001b[31m-              \"colsample_bytree\": 0.8,\n",
      "\u001b[31m-      \u001b[31m-              \"eta\": 0.1,\n",
      "\u001b[31m-      \u001b[31m-              \"max_depth\": 5,\n",
      "\u001b[31m-      \u001b[31m-              \"num_round\": 100,\n",
      "\u001b[31m-      \u001b[31m-              \"subsample\": 0.8,\n",
      "\u001b[31m-      \u001b[31m-              \"train\": \"titanic_train.csv\"\n",
      "\u001b[31m-      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[31m-          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[31m-      \u001b[31m-          \"input_data_config\": {\n",
      "\u001b[31m-      \u001b[31m-              \"train\": {\n",
      "\u001b[31m-      \u001b[31m-                  \"ContentType\": \"csv\",\n",
      "\u001b[31m-      \u001b[31m-                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[31m-      \u001b[31m-                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[31m-      \u001b[31m-                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[31m-      \u001b[31m-              }\n",
      "\u001b[31m-      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[31m-          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[31m-      \u001b[31m-          \"is_master\": true,\n",
      "\u001b[31m-      \u001b[31m-          \"job_name\": \"sagemaker-xgboost-2024-11-01-22-25-59-504\",\n",
      "\u001b[31m-      \u001b[31m-          \"log_level\": 20,\n",
      "\u001b[31m-      \u001b[31m-          \"master_hostname\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[31m-          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[31m-      \u001b[31m-          \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\n",
      "\u001b[31m-      \u001b[31m-          \"module_name\": \"train_xgboost\",\n",
      "\u001b[31m-      \u001b[31m-          \"network_interface_name\": \"eth0\",\n",
      "\u001b[31m-      \u001b[31m-          \"num_cpus\": 2,\n",
      "\u001b[31m-      \u001b[31m-          \"num_gpus\": 0,\n",
      "\u001b[31m-      \u001b[31m-          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[31m-      \u001b[31m-          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[31m-      \u001b[31m-          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[31m-      \u001b[31m-          \"resource_config\": {\n",
      "\u001b[31m-      \u001b[31m-              \"current_host\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[31m-              \"current_instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-      \u001b[31m-              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-      \u001b[31m-              \"hosts\": [\n",
      "\u001b[31m-      \u001b[31m-                  \"algo-1\"\n",
      "\u001b[31m-      \u001b[31m-              ],\n",
      "\u001b[31m-      \u001b[31m-              \"instance_groups\": [\n",
      "\u001b[31m-      \u001b[31m-                  {\n",
      "\u001b[31m-      \u001b[31m-                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-      \u001b[31m-                      \"instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-      \u001b[31m-                      \"hosts\": [\n",
      "\u001b[31m-      \u001b[31m-                          \"algo-1\"\n",
      "\u001b[31m-      \u001b[31m-                      ]\n",
      "\u001b[31m-      \u001b[31m-                  }\n",
      "\u001b[31m-      \u001b[31m-              ],\n",
      "\u001b[31m-      \u001b[31m-              \"network_interface_name\": \"eth0\"\n",
      "\u001b[31m-      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[31m-          \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-11-01-22-25-59-504\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\",\"--train\",\"titanic_train.csv\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mSM_HP_TRAIN=titanic_train.csv\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8 --train titanic_train.csv\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mTraining time: 0.22 seconds\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:28:45 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[31m-      Training seconds: 125\n",
      "\u001b[31m-      \u001b[31m-      Billable seconds: 125\n",
      "\u001b[31m-      \u001b[31m-      Training time on SageMaker: 197.60 seconds\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/19/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  38\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/19/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      xgboost/sagemaker-xgboost-2024-11-01-22-25-59-504/output/model.tar.gz\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/20/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  40\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/21/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  41\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/21/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: execute_result\n",
      "\u001b[31m-      \u001b[31m-    execution_count: 41\n",
      "\u001b[31m-      \u001b[31m-    data:\n",
      "\u001b[31m-      \u001b[31m-      text/html:\n",
      "\u001b[31m-      \u001b[31m-        <div>\n",
      "\u001b[31m-      \u001b[31m-        <style scoped>\n",
      "\u001b[31m-      \u001b[31m-            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[31m-      \u001b[31m-                vertical-align: middle;\n",
      "\u001b[31m-      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[31m-            .dataframe tbody tr th {\n",
      "\u001b[31m-      \u001b[31m-                vertical-align: top;\n",
      "\u001b[31m-      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[31m-            .dataframe thead th {\n",
      "\u001b[31m-      \u001b[31m-                text-align: right;\n",
      "\u001b[31m-      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[31m-        </style>\n",
      "\u001b[31m-      \u001b[31m-        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[31m-      \u001b[31m-          <thead>\n",
      "\u001b[31m-      \u001b[31m-            <tr style=\"text-align: right;\">\n",
      "\u001b[31m-      \u001b[31m-              <th></th>\n",
      "\u001b[31m-      \u001b[31m-              <th>PassengerId</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Survived</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Pclass</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Name</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Sex</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Age</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>SibSp</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Parch</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Ticket</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Fare</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Cabin</th>\n",
      "\u001b[31m-      \u001b[31m-              <th>Embarked</th>\n",
      "\u001b[31m-      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[31m-          </thead>\n",
      "\u001b[31m-      \u001b[31m-          <tbody>\n",
      "\u001b[31m-      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[31m-              <th>0</th>\n",
      "\u001b[31m-      \u001b[31m-              <td>566</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>Davies, Mr. Alfred J</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>24.0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>2</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>A/4 48871</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>24.1500</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[31m-              <th>1</th>\n",
      "\u001b[31m-      \u001b[31m-              <td>161</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>Cribb, Mr. John Hatfield</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>44.0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>371362</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>16.1000</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[31m-              <th>2</th>\n",
      "\u001b[31m-      \u001b[31m-              <td>554</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>Leeni, Mr. Fahim (\"Philip Zenni\")</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>22.0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>2620</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>7.2250</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>C</td>\n",
      "\u001b[31m-      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[31m-              <th>3</th>\n",
      "\u001b[31m-      \u001b[31m-              <td>861</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>Hansen, Mr. Claus Peter</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>41.0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>2</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>350026</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>14.1083</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[31m-              <th>4</th>\n",
      "\u001b[31m-      \u001b[31m-              <td>242</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>Murphy, Miss. Katherine \"Kate\"</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>female</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>367230</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>15.5000</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[31m-              <td>Q</td>\n",
      "\u001b[31m-      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[31m-          </tbody>\n",
      "\u001b[31m-      \u001b[31m-        </table>\n",
      "\u001b[31m-      \u001b[31m-        </div>\n",
      "\u001b[31m-      \u001b[31m-      text/plain:\n",
      "\u001b[31m-      \u001b[31m-           PassengerId  Survived  Pclass                               Name     Sex  \\\n",
      "\u001b[31m-      \u001b[31m-        0          566         0       3               Davies, Mr. Alfred J    male   \n",
      "\u001b[31m-      \u001b[31m-        1          161         0       3           Cribb, Mr. John Hatfield    male   \n",
      "\u001b[31m-      \u001b[31m-        2          554         1       3  Leeni, Mr. Fahim (\"Philip Zenni\")    male   \n",
      "\u001b[31m-      \u001b[31m-        3          861         0       3            Hansen, Mr. Claus Peter    male   \n",
      "\u001b[31m-      \u001b[31m-        4          242         1       3     Murphy, Miss. Katherine \"Kate\"  female   \n",
      "\u001b[31m-      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[31m-            Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
      "\u001b[31m-      \u001b[31m-        0  24.0      2      0  A/4 48871  24.1500   NaN        S  \n",
      "\u001b[31m-      \u001b[31m-        1  44.0      0      1     371362  16.1000   NaN        S  \n",
      "\u001b[31m-      \u001b[31m-        2  22.0      0      0       2620   7.2250   NaN        C  \n",
      "\u001b[31m-      \u001b[31m-        3  41.0      2      0     350026  14.1083   NaN        S  \n",
      "\u001b[31m-      \u001b[31m-        4   NaN      1      0     367230  15.5000   NaN        Q  \n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/22/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  43\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/23/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  44\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/23/outputs/0-1:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      Test Set Accuracy: 0.7933\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "\u001b[31m-      \u001b[31m-      configuration generated by an older version of XGBoost, please export the model by calling\n",
      "\u001b[31m-      \u001b[31m-      `Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-          https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-      for more details about differences between saving model and serializing.\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-        warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m-      \u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "\u001b[31m-      \u001b[31m-        warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/26/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  48\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/26/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: execute_result\n",
      "\u001b[31m-      \u001b[31m-    execution_count: 48\n",
      "\u001b[31m-      \u001b[31m-    data:\n",
      "\u001b[31m-      \u001b[31m-      text/plain: 's3://titanic-dataset-test/data/titanic_train.csv'\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/27/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  49\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/27/outputs/0-2:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-48-56-105\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:48:57 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:49:11 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:49:42 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:50:22 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:51:29 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:51:29 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:23.778 ip-10-2-247-41.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:23.803 ip-10-2-247-41.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Single node training.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Train matrix has 713 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:24.213 ip-10-2-247-41.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:24.214 ip-10-2-247-41.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01:22:51:24:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[0]#011train-rmse:474.71030\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:24.227 ip-10-2-247-41.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-11-01 22:51:24.229 ip-10-2-247-41.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[1]#011train-rmse:441.03842\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2]#011train-rmse:411.78134\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[3]#011train-rmse:385.35440\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[4]#011train-rmse:362.34192\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[5]#011train-rmse:342.72199\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[6]#011train-rmse:325.37424\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[7]#011train-rmse:310.20413\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[8]#011train-rmse:297.79462\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[9]#011train-rmse:287.85199\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[10]#011train-rmse:277.92941\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[11]#011train-rmse:270.85162\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[12]#011train-rmse:263.09851\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[13]#011train-rmse:257.25269\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[14]#011train-rmse:251.85989\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[15]#011train-rmse:247.19409\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[16]#011train-rmse:243.73045\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[17]#011train-rmse:240.81642\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[18]#011train-rmse:238.41530\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[19]#011train-rmse:235.56351\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[20]#011train-rmse:233.57898\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[21]#011train-rmse:231.39540\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[22]#011train-rmse:228.63503\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[23]#011train-rmse:226.69484\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[24]#011train-rmse:225.35779\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[25]#011train-rmse:223.92523\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[26]#011train-rmse:222.10831\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[27]#011train-rmse:219.23029\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[28]#011train-rmse:218.87340\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[29]#011train-rmse:216.75085\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[30]#011train-rmse:215.76749\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[31]#011train-rmse:214.97679\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[32]#011train-rmse:213.81511\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[33]#011train-rmse:212.42398\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[34]#011train-rmse:211.10745\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[35]#011train-rmse:209.56615\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[36]#011train-rmse:208.38251\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[37]#011train-rmse:207.96460\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[38]#011train-rmse:206.41853\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[39]#011train-rmse:205.13840\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[40]#011train-rmse:204.59671\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[41]#011train-rmse:203.43626\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[42]#011train-rmse:202.23776\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[43]#011train-rmse:201.98227\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[44]#011train-rmse:201.53015\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[45]#011train-rmse:200.83151\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[46]#011train-rmse:199.75769\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[47]#011train-rmse:197.73955\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[48]#011train-rmse:196.67972\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[49]#011train-rmse:195.99304\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[50]#011train-rmse:194.60979\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[51]#011train-rmse:193.87764\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[52]#011train-rmse:193.04419\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[53]#011train-rmse:191.84062\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[54]#011train-rmse:191.63332\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[55]#011train-rmse:191.06137\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[56]#011train-rmse:190.63503\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[57]#011train-rmse:190.23791\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[58]#011train-rmse:190.01700\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[59]#011train-rmse:189.62627\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[60]#011train-rmse:188.78932\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[61]#011train-rmse:187.87903\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[62]#011train-rmse:187.33061\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[63]#011train-rmse:186.93269\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[64]#011train-rmse:186.04112\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[65]#011train-rmse:185.29774\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[66]#011train-rmse:184.67114\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[67]#011train-rmse:183.74358\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[68]#011train-rmse:183.30225\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[69]#011train-rmse:182.09914\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[70]#011train-rmse:181.83897\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[71]#011train-rmse:181.03862\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[72]#011train-rmse:180.78651\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[73]#011train-rmse:179.64867\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[74]#011train-rmse:178.82935\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[75]#011train-rmse:178.21071\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[76]#011train-rmse:177.54585\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[77]#011train-rmse:177.00539\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[78]#011train-rmse:176.26054\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[79]#011train-rmse:175.64746\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[80]#011train-rmse:174.62911\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[81]#011train-rmse:174.01623\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[82]#011train-rmse:173.50301\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[83]#011train-rmse:172.43010\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[84]#011train-rmse:171.95624\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[85]#011train-rmse:171.48639\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[86]#011train-rmse:171.19154\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[87]#011train-rmse:169.97925\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[88]#011train-rmse:169.45494\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[89]#011train-rmse:168.90468\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[90]#011train-rmse:168.16402\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[91]#011train-rmse:167.30739\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[92]#011train-rmse:166.85228\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[93]#011train-rmse:165.98686\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[94]#011train-rmse:165.70697\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[95]#011train-rmse:165.43739\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[96]#011train-rmse:164.83107\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[97]#011train-rmse:164.22020\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[98]#011train-rmse:163.90085\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[99]#011train-rmse:163.45399\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-      2024-11-01 22:51:43 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[31m-      Training seconds: 120\n",
      "\u001b[31m-      \u001b[31m-      Billable seconds: 120\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: error\n",
      "\u001b[31m-      \u001b[31m-    ename: NameError\n",
      "\u001b[31m-      \u001b[31m-    evalue: name 'instance_type' is not defined\n",
      "\u001b[31m-      \u001b[31m-    traceback:\n",
      "\u001b[31m-      \u001b[31m-      item[0]: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      item[1]: \u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[31m-      \u001b[31m-      item[2]:\n",
      "\u001b[31m-      \u001b[31m-        Cell \u001b[0;32mIn[49], line 28\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-        \u001b[1;32m     25\u001b[0m xgboost_estimator_builtin\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_input})\n",
      "\u001b[31m-      \u001b[31m-        \u001b[1;32m     26\u001b[0m end \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[31m-      \u001b[31m-        \u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime for training on SageMaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, instance_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43minstance_type\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, instance_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[31m-      \u001b[31m-      item[3]: \u001b[0;31mNameError\u001b[0m: name 'instance_type' is not defined\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/31/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  11\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/31/outputs/0-3:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-19-57-38-692\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 19:57:39 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 19:57:54 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 19:58:27 Downloading - Downloading input data......\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 19:59:18 Downloading - Downloading the training image...\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:00:09 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:14.589 ip-10-0-150-29.ec2.internal:8 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:14.614 ip-10-0-150-29.ec2.internal:8 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Single node training.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:15.170 ip-10-0-150-29.ec2.internal:8 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:15.171 ip-10-0-150-29.ec2.internal:8 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:15.172 ip-10-0-150-29.ec2.internal:8 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:15.172 ip-10-0-150-29.ec2.internal:8 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:00:15:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[0]#011train-rmse:475.71411\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:15.232 ip-10-0-150-29.ec2.internal:8 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:00:15.234 ip-10-0-150-29.ec2.internal:8 INFO hook.py:491] Hook is writing from the hook with pid: 8\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[1]#011train-rmse:441.49982\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2]#011train-rmse:411.72672\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[3]#011train-rmse:385.56036\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[4]#011train-rmse:363.56360\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[5]#011train-rmse:344.49588\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[6]#011train-rmse:327.67941\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[7]#011train-rmse:312.88376\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[8]#011train-rmse:299.41186\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[9]#011train-rmse:288.74734\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[10]#011train-rmse:280.12149\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[11]#011train-rmse:273.00226\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[12]#011train-rmse:266.27246\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[13]#011train-rmse:259.31256\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[14]#011train-rmse:253.75191\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[15]#011train-rmse:249.62512\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[16]#011train-rmse:245.67200\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[17]#011train-rmse:242.34294\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[18]#011train-rmse:238.86392\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[19]#011train-rmse:235.89893\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[20]#011train-rmse:233.08173\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[21]#011train-rmse:231.52962\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[22]#011train-rmse:229.66519\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[23]#011train-rmse:227.86470\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[24]#011train-rmse:226.72954\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[25]#011train-rmse:225.20438\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[26]#011train-rmse:223.76180\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[27]#011train-rmse:221.96107\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[28]#011train-rmse:221.09845\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[29]#011train-rmse:219.99710\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[30]#011train-rmse:219.37936\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[31]#011train-rmse:218.05364\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[32]#011train-rmse:217.43600\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[33]#011train-rmse:216.73910\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[34]#011train-rmse:216.43459\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[35]#011train-rmse:215.56277\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[36]#011train-rmse:214.80632\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[37]#011train-rmse:213.70375\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[38]#011train-rmse:213.17102\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[39]#011train-rmse:212.88145\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[40]#011train-rmse:211.96532\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[41]#011train-rmse:211.32878\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[42]#011train-rmse:210.17601\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[43]#011train-rmse:209.69156\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[44]#011train-rmse:208.88245\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[45]#011train-rmse:207.83882\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[46]#011train-rmse:206.71755\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[47]#011train-rmse:205.46107\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[48]#011train-rmse:204.46623\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[49]#011train-rmse:203.54263\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[50]#011train-rmse:202.88849\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[51]#011train-rmse:202.33600\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[52]#011train-rmse:201.32494\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[53]#011train-rmse:200.73129\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[54]#011train-rmse:200.46822\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[55]#011train-rmse:199.81789\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[56]#011train-rmse:199.07161\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[57]#011train-rmse:198.43318\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[58]#011train-rmse:198.14304\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[59]#011train-rmse:197.53601\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[60]#011train-rmse:197.10297\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[61]#011train-rmse:196.49066\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[62]#011train-rmse:196.28507\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[63]#011train-rmse:195.67941\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[64]#011train-rmse:195.51599\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[65]#011train-rmse:194.97147\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[66]#011train-rmse:193.90741\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[67]#011train-rmse:193.52390\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[68]#011train-rmse:193.01285\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[69]#011train-rmse:192.34790\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[70]#011train-rmse:191.98561\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[71]#011train-rmse:191.39389\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[72]#011train-rmse:190.95151\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[73]#011train-rmse:190.21582\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[74]#011train-rmse:189.08704\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[75]#011train-rmse:188.47955\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[76]#011train-rmse:188.12349\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[77]#011train-rmse:187.77058\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[78]#011train-rmse:187.10945\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[79]#011train-rmse:186.61465\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[80]#011train-rmse:185.80434\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[81]#011train-rmse:184.99844\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[82]#011train-rmse:184.62537\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[83]#011train-rmse:184.16344\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[84]#011train-rmse:183.58179\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[85]#011train-rmse:183.19162\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[86]#011train-rmse:182.80438\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[87]#011train-rmse:182.35306\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[88]#011train-rmse:181.88933\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[89]#011train-rmse:181.32742\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[90]#011train-rmse:180.80669\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[91]#011train-rmse:180.49007\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[92]#011train-rmse:179.89072\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[93]#011train-rmse:179.37184\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[94]#011train-rmse:179.06938\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[95]#011train-rmse:178.65883\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[96]#011train-rmse:177.99016\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[97]#011train-rmse:177.48030\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[98]#011train-rmse:177.15761\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[99]#011train-rmse:176.45931\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:00:32 Uploading - Uploading generated training model\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:00:32 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-20-00-56-211\n",
      "\u001b[31m-      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[31m-      Training seconds: 125\n",
      "\u001b[31m-      \u001b[31m-      Billable seconds: 125\n",
      "\u001b[31m-      \u001b[31m-      Training time with 1 instance: 197.52 seconds\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:00:56 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:01:24 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:01:57 Downloading - Downloading input data......\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:02:47 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:04:01 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:04:01 Uploading - Uploading generated training model\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:44.521 ip-10-2-73-243.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:44.558 ip-10-2-73-243.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Distributed node training with 2 hosts: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[20:03:45] task NULL got new rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Failed to connect to RabitTracker on attempt 0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:45:INFO] Sleeping for 3 sec before retrying\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:44.237 ip-10-2-108-166.ec2.internal:6 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:44.269 ip-10-2-108-166.ec2.internal:6 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Distributed node training with 2 hosts: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] start listen on algo-1:9099\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9099}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:44:INFO] No data received from connection ('10.2.108.166', 47654). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] No data received from connection ('10.2.73.243', 55330). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[20:03:45] task NULL got new rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Recieve start signal from 10.2.108.166; assign rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Recieve start signal from 10.2.73.243; assign rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] @tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] @tracker 0.003575563430786133 secs between node start and job finish\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] start listen on algo-1:9100\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Rabit slave environment: {'DMLC_TRACKER_URI': 'algo-1', 'DMLC_TRACKER_PORT': 9100}\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:45:INFO] No data received from connection ('10.2.108.166', 59764). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] No data received from connection ('10.2.73.243', 33634). Closing.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[20:03:48] task NULL got new rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Recieve start signal from 10.2.108.166; assign rank 0\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Recieve start signal from 10.2.73.243; assign rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] @tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:48.590 ip-10-2-108-166.ec2.internal:6 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:48.591 ip-10-2-108-166.ec2.internal:6 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:48.592 ip-10-2-108-166.ec2.internal:6 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:48.592 ip-10-2-108-166.ec2.internal:6 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:48:INFO] Connected to RabitTracker.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[20:03:48] task NULL got new rank 1\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:48:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:48.591 ip-10-2-73-243.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:48.591 ip-10-2-73-243.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:48.592 ip-10-2-73-243.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:48.592 ip-10-2-73-243.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29:20:03:48:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[35m[2024-10-29 20:03:48.651 ip-10-2-73-243.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29 20:03:48.650 ip-10-2-108-166.ec2.internal:6 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [0]#011train-rmse:475.06763\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [1]#011train-rmse:441.48178\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [2]#011train-rmse:411.14572\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [3]#011train-rmse:384.41522\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [4]#011train-rmse:362.00281\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [5]#011train-rmse:342.09195\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:48:INFO] [6]#011train-rmse:325.18576\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [7]#011train-rmse:310.38946\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [8]#011train-rmse:296.75525\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [9]#011train-rmse:285.56943\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [10]#011train-rmse:276.39905\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [11]#011train-rmse:267.94705\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [12]#011train-rmse:261.27862\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [13]#011train-rmse:255.99045\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [14]#011train-rmse:251.24669\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [15]#011train-rmse:246.39867\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [16]#011train-rmse:242.99580\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [17]#011train-rmse:239.83401\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [18]#011train-rmse:237.54298\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [19]#011train-rmse:234.17052\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [20]#011train-rmse:231.99132\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:49:INFO] [21]#011train-rmse:230.39975\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [22]#011train-rmse:228.85759\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [23]#011train-rmse:227.50473\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [24]#011train-rmse:225.93337\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [25]#011train-rmse:224.10906\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [26]#011train-rmse:222.46446\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [27]#011train-rmse:221.07739\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [28]#011train-rmse:219.62199\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [29]#011train-rmse:218.51106\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [30]#011train-rmse:217.05801\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [31]#011train-rmse:216.02483\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [32]#011train-rmse:215.17191\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [33]#011train-rmse:214.42673\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [34]#011train-rmse:213.53833\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [35]#011train-rmse:212.43672\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [36]#011train-rmse:211.95911\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [37]#011train-rmse:210.74970\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:50:INFO] [38]#011train-rmse:209.58318\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [39]#011train-rmse:208.81479\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [40]#011train-rmse:207.43625\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [41]#011train-rmse:206.39166\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [42]#011train-rmse:205.92508\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [43]#011train-rmse:204.72928\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [44]#011train-rmse:204.11678\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [45]#011train-rmse:203.84581\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [46]#011train-rmse:202.89059\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [47]#011train-rmse:202.05829\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [48]#011train-rmse:201.17131\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [49]#011train-rmse:200.90657\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [50]#011train-rmse:199.62822\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [51]#011train-rmse:199.26662\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [52]#011train-rmse:198.40724\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [53]#011train-rmse:197.83804\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [54]#011train-rmse:197.20424\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [55]#011train-rmse:196.22171\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:51:INFO] [56]#011train-rmse:195.52446\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [57]#011train-rmse:195.11131\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [58]#011train-rmse:194.62470\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [59]#011train-rmse:194.30669\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [60]#011train-rmse:193.58981\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [61]#011train-rmse:193.16104\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [62]#011train-rmse:192.49373\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [63]#011train-rmse:191.79701\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [64]#011train-rmse:191.18817\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [65]#011train-rmse:190.50592\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [66]#011train-rmse:190.10690\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [67]#011train-rmse:189.61542\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [68]#011train-rmse:188.87097\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [69]#011train-rmse:187.95456\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [70]#011train-rmse:187.44124\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [71]#011train-rmse:187.02232\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [72]#011train-rmse:186.86244\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [73]#011train-rmse:186.10358\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [74]#011train-rmse:185.34093\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:52:INFO] [75]#011train-rmse:185.01708\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [76]#011train-rmse:184.52373\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [77]#011train-rmse:183.98309\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [78]#011train-rmse:183.27400\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [79]#011train-rmse:182.74863\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [80]#011train-rmse:181.90372\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [81]#011train-rmse:181.33293\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [82]#011train-rmse:180.66556\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [83]#011train-rmse:180.19649\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [84]#011train-rmse:179.81682\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [85]#011train-rmse:178.86356\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [86]#011train-rmse:178.28250\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [87]#011train-rmse:177.91344\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [88]#011train-rmse:177.26918\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [89]#011train-rmse:176.40164\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [90]#011train-rmse:175.97617\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [91]#011train-rmse:175.74719\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [92]#011train-rmse:175.13661\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [93]#011train-rmse:174.35800\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:53:INFO] [94]#011train-rmse:173.84450\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [95]#011train-rmse:173.09387\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [96]#011train-rmse:172.90584\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [97]#011train-rmse:172.24793\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [98]#011train-rmse:171.71202\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] [99]#011train-rmse:170.94412\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \u001b[34m[2024-10-29:20:03:54:INFO] @tracker 5.666167259216309 secs between node start and job finish\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[31m-      2024-10-29 20:04:15 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[31m-      Training seconds: 274\n",
      "\u001b[31m-      \u001b[31m-      Billable seconds: 274\n",
      "\u001b[31m-      \u001b[31m-      Training time with 2 instances: 228.55 seconds\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from int to NoneType) /cells/33/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  51\n",
      "\u001b[31m-      \u001b[32m+  None\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0mnbdiff /tmp/git-blob-U6ecbm/update-repo.ipynb update-repo.ipynb\n",
      "\u001b[31m-      --- /tmp/git-blob-U6ecbm/update-repo.ipynb  2024-11-01 23:01:33.476421\n",
      "\u001b[31m-      +++ update-repo.ipynb  2024-11-01 23:00:41.224206\n",
      "\u001b[31m-      \u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/1/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  None\n",
      "\u001b[31m-      \u001b[32m+  2\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## added /cells/1/metadata/tags:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  []\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/1/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stdin\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      GitHub Username:  qualiaMachine\n",
      "\u001b[31m-      \u001b[32m+      GitHub Personal Access Token (PAT):  ········\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## modified /cells/1/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[36m@@ -1,6 +1,6 @@\u001b[m\n",
      "\u001b[31m-      \u001b[31m-import getpass\u001b[m\n",
      "\u001b[31m-      \u001b[32m+\u001b[m\u001b[32m# import getpass\u001b[m\n",
      "\u001b[31m-       \u001b[m\n",
      "\u001b[31m-      \u001b[31m-# Prompt for GitHub username and PAT securely\u001b[m\n",
      "\u001b[31m-      \u001b[32m+\u001b[m\u001b[32m# # Prompt for GitHub username and PAT securely\u001b[m\n",
      "\u001b[31m-       # github_url = 'github.com/UW-Madison-DataScience/test_AWS.git' # found under Code -> Clone -> HTTPS (remote the https:// before the rest of the address)\u001b[m\n",
      "\u001b[31m-       # username = input(\"GitHub Username: \")\u001b[m\n",
      "\u001b[31m-       # token = getpass.getpass(\"GitHub Personal Access Token (PAT): \")\u001b[m\n",
      "\u001b[31m-      \u001b[m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/2/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  None\n",
      "\u001b[31m-      \u001b[32m+  3\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## added /cells/2/metadata/tags:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  []\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/2/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      /home/ec2-user/SageMaker/test_AWS\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  None\n",
      "\u001b[31m-      \u001b[32m+  4\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## added /cells/3/metadata/scrolled:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  True\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## added /cells/3/metadata/tags:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  []\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/3/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      nbdiff /tmp/git-blob-3vTqzX/03_Data-storage-and-access-via-buckets.ipynb 03_Data-storage-and-access-via-buckets.ipynb\n",
      "\u001b[31m-      \u001b[32m+      --- /tmp/git-blob-3vTqzX/03_Data-storage-and-access-via-buckets.ipynb  2024-11-01 23:00:13.192088\n",
      "\u001b[31m-      \u001b[32m+      +++ 03_Data-storage-and-access-via-buckets.ipynb  2024-11-01 22:01:28.321055\n",
      "\u001b[31m-      \u001b[32m+      \u001b[34m\u001b[1m## modified /cells/2/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,5 +1,5 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ## 1B. Download copy into notebook environment\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-If you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local copy.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIf you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local\" copy (i.e., one stored in your notebook's instance).\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Download data from S3 to notebook environment. You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/4-5:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    id: 61165195-70f5-45c0-aaad-ffb975ebc6ef\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      ## 2. Pushing new files from notebook environment to bucket\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      As your analysis generates new files, you can upload to your bucket as demonstrated below. For this demo, you can create a blank `results.txt` file to upload to your bucket.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    id: 2974d63f-cf39-4ff6-8544-ac8aa6be5e24\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    execution_count: 14\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      import boto3\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      # Define the S3 bucket name and the file paths\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      bucket_name = \"titanic-dataset-test\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      train_file_path = \"results.txt\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      # Initialize the S3 client\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      s3 = boto3.client('s3')\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      # Upload the training file\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      s3.upload_file(train_file_path, bucket_name, \"results/results.txt\")\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      print(\"Files uploaded successfully.\")\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    outputs:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      output 0:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          Files uploaded successfully.\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/6/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  ## 3. Check current size and storage costs of bucket\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  ## 2. Check current size and storage costs of bucket\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/10/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,4 +1,4 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-## 4: Check storage costs of bucket\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m## 3: Check storage costs of bucket\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       To estimate the storage cost of your Amazon S3 bucket directly from a Jupyter notebook in SageMaker, you can use the following approach. This method calculates the total size of the bucket and estimates the monthly storage cost based on AWS S3 pricing.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       **Note**: AWS S3 pricing varies by region and storage class. The example below uses the S3 Standard storage class pricing for the US East (N. Virginia) region as of November 1, 2024. Please verify the current pricing for your specific region and storage class on the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/13:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: f60b6951-6549-41de-9948-0ee9832df0c1\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      ## 4. Pushing new files from notebook environment to bucket\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      As your analysis generates new files, you can upload to your bucket as demonstrated below. For this demo, you can create a blank `results.txt` file to upload to your bucket.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 2974d63f-cf39-4ff6-8544-ac8aa6be5e24\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    execution_count: 14\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      import boto3\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      # Define the S3 bucket name and the file paths\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      bucket_name = \"titanic-dataset-test\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      train_file_path = \"results.txt\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      # Initialize the S3 client\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      s3 = boto3.client('s3')\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      # Upload the training file\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      s3.upload_file(train_file_path, bucket_name, \"results/results.txt\")\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      print(\"Files uploaded successfully.\")\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    outputs:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      output 0:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          Files uploaded successfully.\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0mnbdiff /tmp/git-blob-d32cgw/04_Interacting-with-code-repo.ipynb 04_Interacting-with-code-repo.ipynb\n",
      "\u001b[31m-      \u001b[32m+      --- /tmp/git-blob-d32cgw/04_Interacting-with-code-repo.ipynb  2024-11-01 23:00:13.696090\n",
      "\u001b[31m-      \u001b[32m+      +++ 04_Interacting-with-code-repo.ipynb  2024-11-01 21:56:44.111790\n",
      "\u001b[31m-      \u001b[32m+      \u001b[34m\u001b[1m## replaced /cells/20/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  88\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  89\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/20/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      [main 8102f2c] Updates from Jupyter notebooks\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+       8 files changed, 622 insertions(+), 311 deletions(-)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+       create mode 100644 prep-train-test-sets.ipynb\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+       rename train_nn.py => scripts/train_nn.py (100%)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+       rename train_xgboost.py => scripts/train_xgboost.py (100%)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+       create mode 100644 update-repo.ipynb\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/20/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      [main c49ab7e] Updates from Jupyter notebooks\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-       1 file changed, 120 insertions(+), 569 deletions(-)\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/22/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  84\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  90\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/22/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,4 +1,4 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       From https://github.com/UW-Madison-DataScience/test_AWS\u001b[m\n",
      "\u001b[31m-      \u001b[32m+        * branch            main       -> FETCH_HEAD\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-   0363cc2..bc28ce1  main       -> origin/main\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m   bc28ce1..22d83d0  main       -> origin/main\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Already up to date.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/24/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  52\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  91\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/26/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  85\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  92\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/26/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,9 +1,9 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Enumerating objects: 5, done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Counting objects: 100% (5/5), done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mEnumerating objects: 17, done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mCounting objects: 100% (17/17), done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Delta compression using up to 2 threads\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Compressing objects: 100% (3/3), done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Writing objects: 100% (3/3), 702 bytes | 702.00 KiB/s, done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mCompressing objects: 100% (12/12), done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mWriting objects: 100% (12/12), 9.71 KiB | 2.43 MiB/s, done.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mTotal 12 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mremote: Resolving deltas: 100% (8/8), completed with 4 local objects.\u001b[K\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       To https://github.com/UW-Madison-DataScience/test_AWS.git\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-   bc28ce1..22d83d0  main -> main\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m   22d83d0..8102f2c  main -> main\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0mnbdiff /tmp/git-blob-dxpbq6/05_Intro-train-models.ipynb 05_Intro-train-models.ipynb\n",
      "\u001b[31m-      \u001b[32m+      --- /tmp/git-blob-dxpbq6/05_Intro-train-models.ipynb  2024-11-01 23:00:14.236093\n",
      "\u001b[31m-      \u001b[32m+      +++ 05_Intro-train-models.ipynb  2024-11-01 22:58:24.955638\n",
      "\u001b[31m-      \u001b[32m+      \u001b[34m\u001b[1m## inserted before /cells/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: bef72914-4f7a-41b8-a84f-15a0e438dd59\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      ## Initialize SageMaker environment\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      This code initializes the AWS SageMaker environment by defining the SageMaker role, session, and S3 client. It also specifies the S3 bucket and key for accessing the Titanic training dataset stored in an S3 bucket.\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    id: 46863ddc-86cf-4295-95bb-2c12e24a25a1\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      ### Read data from S3 into memory\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Our data is stored on an S3 bucket called 'titanic-dataset-test'. We can use the following code to read data directly from S3 into memory in the Jupyter notebook environment, without actually downloading a copy of train.csv as a local file.\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/1/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  21\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  25\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/1/outputs/0-1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      (712, 12)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      (179, 12)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: execute_result\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    execution_count: 21\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    data:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      text/html:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        <div>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        <style scoped>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                vertical-align: middle;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            .dataframe tbody tr th {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                vertical-align: top;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            .dataframe thead th {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                text-align: right;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        </style>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          <thead>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr style=\"text-align: right;\">\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th></th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>PassengerId</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Survived</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Pclass</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Name</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Sex</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Age</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>SibSp</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Parch</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Ticket</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Fare</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Cabin</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Embarked</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          </thead>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          <tbody>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>0</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>693</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Lam, Mr. Ali</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1601</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>56.4958</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>1</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>482</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>2</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Frost, Mr. Anthony Wood \"Archie\"</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>239854</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0.0000</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>2</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>528</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Farthing, Mr. John</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>PC 17483</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>221.7792</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>C95</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>3</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>856</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Aks, Mrs. Sam (Leah Rosen)</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>18.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>392091</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>9.3500</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>4</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>802</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>2</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Collyer, Mrs. Harvey (Charlotte Annie Tate)</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>31.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>C.A. 31921</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>26.2500</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          </tbody>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        </table>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        </div>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      text/plain:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-           PassengerId  Survived  Pclass                                         Name  \\\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        0          693         1       3                                 Lam, Mr. Ali   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        1          482         0       2             Frost, Mr. Anthony Wood \"Archie\"   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        2          528         0       1                           Farthing, Mr. John   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        3          856         1       3                   Aks, Mrs. Sam (Leah Rosen)   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        4          802         1       2  Collyer, Mrs. Harvey (Charlotte Annie Tate)   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              Sex   Age  SibSp  Parch      Ticket      Fare Cabin Embarked  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        0    male   NaN      0      0        1601   56.4958   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        1    male   NaN      0      0      239854    0.0000   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        2    male   NaN      0      0    PC 17483  221.7792   C95        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        3  female  18.0      0      1      392091    9.3500   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        4  female  31.0      1      1  C.A. 31921   26.2500   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/1/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -9,24 +9,9 @@\u001b[m \u001b[mrole = sagemaker.get_execution_role()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       session = sagemaker.Session()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       s3 = boto3.client('s3')\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# Define the S3 bucket and object key\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define the S3 bucket\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       bucket = 'titanic-dataset-test'  # replace with your S3 bucket name\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-key = 'data/titanic_train.csv'  # replace with your object key\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# Read the object from S3\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-response = s3.get_object(Bucket=bucket, Key=key)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# Load the data into a pandas DataFrame\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-train_data = pd.read_csv(response['Body'])\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-key = 'data/titanic_test.csv'  # replace with your object key\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-response = s3.get_object(Bucket=bucket, Key=key)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-test_data = pd.read_csv(response['Body'])\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# check shape\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-print(train_data.shape)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-print(test_data.shape)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# Inspect the first few rows of the DataFrame\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-train_data.head()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# train_data.shape\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define train/test filenames\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtrain_filename = 'titanic_train.csv'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtest_filename = 'titanic_test.csv'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/2/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,5 +1,5 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ### Download copy into notebook environment\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-If you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local copy.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIf you have larger dataset (> 1GB), you may want to skip this step and always read directly into memory. However, for smaller datasets, it can be convenient to have a \"local\" copy (i.e., one that you store in your notebook's instance).\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Download data from S3 to notebook environment. You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/3/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  22\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  26\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/3/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,6 +1,6 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Define the S3 bucket and file location\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-file_key = \"data/titanic_train.csv\"  # Path to your file in the S3 bucket\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-local_file_path = \"./titanic_train.csv\"  # Local path to save the file\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mfile_key = f\"data/{train_filename}\"  # Path to your file in the S3 bucket\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mlocal_file_path = f\"./{train_filename}\"  # Local path to save the file\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Initialize the S3 client and download the file\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       s3 = boto3.client(\"s3\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/4:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: cd136750-a3be-409e-92d8-5430f70d68cf\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      We can do the same for the test set.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 253f4930-ca36-49ea-a2eb-fbb29826a394\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    execution_count: 27\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        []\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      # Define the S3 bucket and file location\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      file_key = f\"data/{train_filename}\"  # Path to your file in the S3 bucket. W\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      local_file_path = f\"./{train_filename}\"  # Local path to save the file\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      # Initialize the S3 client and download the file\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      s3 = boto3.client(\"s3\")\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      s3.download_file(bucket, file_key, local_file_path)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      print(\"File downloaded:\", local_file_path)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    outputs:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      output 0:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          File downloaded: ./titanic_test.csv\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/4/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,2 +1,2 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ### Get code (train and tune scripts) from git repo. \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-DO NOT put data inside your code repo, as version tracking for data files takes up unnecessary storage in this notebook instance.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mWe recommend you DO NOT put data inside your code repo, as version tracking for data files takes up unnecessary storage in this notebook instance. Instead, store your data in a separte S3 bucket. We have a data folder in our repo only as a means to initially hand you the data for this tutorial.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/6/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,2 +1,4 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ### Testing train.py on this notebook's instance\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mNotebook instances in SageMaker allow us allocate more powerful instances (or many instances) to machine learning jobs that require extra power, GPUs, or benefit from parallelization. Before we try exploiting this extra power, it is essential that we test our code thoroughly. We don't want to waste unnecessary compute cycles and resources on jobs that produce bugs instead of insights. If you need to, you can use a subset of your data to run quicker tests. You can also select a slightly better instance resource if your current instance insn't meeting your needs. See the [Instances for ML spreadsheet](https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing) for guidance.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Test train.py on this notebook's instance (or when possible, on your own machine) before doing anything more complicated (e.g., hyperparameter tuning on multiple instances)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/7/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  4\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  28\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/7/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.1.2)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.14.1)\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/7/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Collecting xgboost\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Downloading xgboost-2.1.2-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from xgboost) (1.14.1)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Downloading xgboost-2.1.2-py3-none-manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[?25hInstalling collected packages: xgboost\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Successfully installed xgboost-2.1.2\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/9/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  5\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  31\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/9/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        warnings.warn(\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/9/outputs/1/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Training time: 0.08 seconds\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mTraining time: 0.06 seconds\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Model saved to ./xgboost-model\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Local training time: 8.43 seconds\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mLocal training time: 0.10 seconds\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/9/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -3,7 +3,7 @@\u001b[m \u001b[mimport time as t\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       start_time = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Run the script and pass arguments directly\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-%run test_AWS/train_xgboost.py --max_depth 5 --eta 0.1 --subsample 0.8 --colsample_bytree 0.8 --num_round 100 --train ./train.csv\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m%run test_AWS/scripts/train_xgboost.py --max_depth 5 --eta 0.1 --subsample 0.8 --colsample_bytree 0.8 --num_round 100 --train ./titanic_train.csv\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Measure and print the time taken\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       print(f\"Local training time: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/11/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  6\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  34\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/11/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  !aws ec2 describe-instance-type-offerings --location-type us-east-2 --filters Name=instance-type,Values=ml.t3.medium\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  # !aws ec2 describe-instance-type-offerings --location-type us-east-1 \n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/12/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -72,6 +72,8 @@\u001b[m \u001b[msklearn_estimator = SKLearn(\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       This setup simplifies training, allowing you to maintain custom environments directly within SageMaker’s managed containers, without needing to build and manage your own Docker images.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m---\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ### More information on pre-built environments\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       he [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) provides lists of pre-built container images for each framework and their standard libraries, including details on pre-installed packages.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+             \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/13/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  9\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  37\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/13/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    name: stderr\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.large.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-25-59-504\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:26:01 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:26:17 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:26:41 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:27:21 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:28:32 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:28:32 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:28:22.062 ip-10-0-251-187.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:28:22.085 ip-10-0-251-187.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:22:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mProcessing /opt/ml/code\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        Preparing metadata (setup.py): started\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        Building wheel for train-xgboost (setup.py): started\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m  Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=10623 sha256=b31dfae0c3499467efbbc51c882ab217c3c21cd87f27d0dcd8ec2d51b58b77de\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-2l39uija/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:28:24:INFO] Invoking user script\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m{\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"additional_framework_parameters\": {},\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"channel_input_dirs\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"current_host\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"hosts\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"algo-1\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          ],\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"hyperparameters\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"colsample_bytree\": 0.8,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"eta\": 0.1,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"max_depth\": 5,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"num_round\": 100,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"subsample\": 0.8,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"train\": \"titanic_train.csv\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"input_data_config\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"train\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  \"ContentType\": \"csv\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"is_master\": true,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"job_name\": \"sagemaker-xgboost-2024-11-01-22-25-59-504\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"log_level\": 20,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"master_hostname\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"module_name\": \"train_xgboost\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"network_interface_name\": \"eth0\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"num_cpus\": 2,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"num_gpus\": 0,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"resource_config\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"current_host\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"current_instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"hosts\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  \"algo-1\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              ],\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"instance_groups\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                      \"instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                      \"hosts\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                          \"algo-1\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                      ]\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                  }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              ],\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              \"network_interface_name\": \"eth0\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8,\"train\":\"titanic_train.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-11-01-22-25-59-504\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-11-01-22-25-59-504/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\",\"--train\",\"titanic_train.csv\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mSM_HP_TRAIN=titanic_train.csv\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8 --train titanic_train.csv\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mTraining time: 0.22 seconds\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:28:45 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Training seconds: 125\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Billable seconds: 125\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Training time on SageMaker: 197.60 seconds\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/13/outputs/0-1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-21-48-04-392\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:48:04 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:48:28 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:48:59 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:49:39 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:50:35 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 21:50:40.864 ip-10-2-116-141.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 21:50:40.894 ip-10-2-116-141.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Module train_xgboost does not provide a setup.py. \u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:41:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mProcessing /opt/ml/code\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Preparing metadata (setup.py): started\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mBuilding wheels for collected packages: train-xgboost\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Building wheel for train-xgboost (setup.py): started\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Building wheel for train-xgboost (setup.py): finished with status 'done'\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Created wheel for train-xgboost: filename=train_xgboost-1.0.0-py2.py3-none-any.whl size=37357 sha256=508327b0c137859acefb3ed0abd6a872018edba9b29b73a748efb4bcf8f7b21d\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-0glp7pi6/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSuccessfully built train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mInstalling collected packages: train-xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSuccessfully installed train-xgboost-1.0.0\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:43:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:21:50:43:INFO] Invoking user script\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m{\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"additional_framework_parameters\": {},\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"channel_input_dirs\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"train\": \"/opt/ml/input/data/train\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"current_host\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"hosts\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"algo-1\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          ],\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"hyperparameters\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"colsample_bytree\": 0.8,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"eta\": 0.1,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"max_depth\": 5,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"num_round\": 100,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"subsample\": 0.8\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"input_data_config\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"train\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  \"ContentType\": \"csv\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  \"TrainingInputMode\": \"File\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  \"S3DistributionType\": \"FullyReplicated\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  \"RecordWrapperType\": \"None\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"is_master\": true,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"job_name\": \"sagemaker-xgboost-2024-10-29-21-48-04-392\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"log_level\": 20,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"master_hostname\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"module_dir\": \"s3://titanic-dataset-test/sagemaker-xgboost-2024-10-29-21-48-04-392/source/sourcedir.tar.gz\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"module_name\": \"train_xgboost\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"network_interface_name\": \"eth0\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"num_cpus\": 2,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"num_gpus\": 0,\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"resource_config\": {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"current_host\": \"algo-1\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"current_instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"current_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"hosts\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  \"algo-1\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              ],\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"instance_groups\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                      \"instance_group_name\": \"homogeneousCluster\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                      \"instance_type\": \"ml.m5.large\",\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                      \"hosts\": [\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                          \"algo-1\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                      ]\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                  }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              ],\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              \"network_interface_name\": \"eth0\"\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          },\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          \"user_entry_point\": \"train_xgboost.py\"\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HPS={\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_USER_ENTRY_POINT=train_xgboost.py\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_MODULE_NAME=train_xgboost\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_MODULE_DIR=s3://titanic-dataset-test/sagemaker-xgboost-2024-10-29-21-48-04-392/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"colsample_bytree\":0.8,\"eta\":0.1,\"max_depth\":5,\"num_round\":100,\"subsample\":0.8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2024-10-29-21-48-04-392\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://titanic-dataset-test/sagemaker-xgboost-2024-10-29-21-48-04-392/source/sourcedir.tar.gz\",\"module_name\":\"train_xgboost\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_xgboost.py\"}\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_USER_ARGS=[\"--colsample_bytree\",\"0.8\",\"--eta\",\"0.1\",\"--max_depth\",\"5\",\"--num_round\",\"100\",\"--subsample\",\"0.8\"]\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_COLSAMPLE_BYTREE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_ETA=0.1\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mSM_HP_SUBSAMPLE=0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/bin/python3 -m train_xgboost --colsample_bytree 0.8 --eta 0.1 --max_depth 5 --num_round 100 --subsample 0.8\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mTraining time: 0.24 seconds\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34mModel saved to /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:51:04 Uploading - Uploading generated training model\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 21:51:04 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Training seconds: 126\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Billable seconds: 126\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Training time on SageMaker: 197.74 seconds\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/13/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,9 +1,12 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       from sagemaker.inputs import TrainingInput\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       from sagemaker.xgboost.estimator import XGBoost\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define instance type/count we'll use for training\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Define S3 paths for input and output\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-train_s3_path = f's3://{bucket}/train.csv'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtrain_s3_path = f's3://{bucket}/data/{train_filename}'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # we'll store all results in a subfolder called xgboost on our bucket. This folder will automatically be created if it doesn't exist already.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       output_folder = 'xgboost'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -12,14 +15,15 @@\u001b[m \u001b[moutput_path = f's3://{bucket}/{output_folder}/'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Set up the SageMaker XGBoost Estimator with custom script\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator = XGBoost(\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           entry_point='train_xgboost.py',      # Custom script path\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    source_dir='test_AWS',               # Directory where your script is located\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    source_dir='test_AWS/scripts',               # Directory where your script is located\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           role=role,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    instance_count=1,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    instance_type='ml.m5.large',\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_count=instance_count,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_type=instance_type,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           output_path=output_path,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           sagemaker_session=session,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           framework_version=\"1.5-1\",           # Use latest supported version for better compatibility\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           hyperparameters={\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m        'train': 'titanic_train.csv',\u001b[m\n",
      "\u001b[31m-      \u001b[32m+               'max_depth': 5,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+               'eta': 0.1,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+               'subsample': 0.8,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -36,4 +40,4 @@\u001b[m \u001b[mstart = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator.fit({'train': train_input})\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       end = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-print(f\"Training time on SageMaker: {end - start:.2f} seconds\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/14:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 06471756-048e-488d-a2d0-0684febe10c6\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        []\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      #### Hyperparameters\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      >The `hyperparameters` section in this code defines key parameters for the XGBoost model, such as `max_depth`, `eta`, `subsample`, `colsample_bytree`, and `num_round`. These parameters control aspects of the model, like tree depth, learning rate, and data sampling, which influence model performance and training time. \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > When running the training job, SageMaker passes these values to `train_xgboost.py` as command-line arguments, making them accessible in the script via `argparse` or similar methods. This setup enables dynamic tuning of model parameters directly from the training configuration, allowing for flexible experimentation without modifying the training script itself.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: d1190ce6-3c45-442c-a39e-32a00b1d01a9\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        []\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (unknown keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      jp-MarkdownHeadingCollapsed: True\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      #### Model results\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > With this code, the training results and model artifacts are saved in a subfolder called `xgboost` in your specified S3 bucket. This folder (`s3://{bucket}/xgboost/`) will be automatically created if it doesn’t already exist, and will contain:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > 1. **Model Artifacts**: The trained model file (often a `.tar.gz` file) that SageMaker saves in the `output_path`.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > 2. **Logs and Metrics**: Any metrics and logs related to the training job, stored in the same `xgboost` folder.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      > This setup allows for convenient access to both the trained model and related output for later evaluation or deployment.\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/14/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -2,7 +2,7 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       To evaluate the model on a test set after training, we’ll go through these steps:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       1. **Download the trained model from S3**.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-2. **Load and preprocess** the test dataset. We don't actually have ground truth test data available for this challenge (labels are missing). If you find yourself in this situation, you may want to exclude a random sample of the train data as your test.csv. In our example below, we get predictions on the train data, but you'll want to adjust this to use test data for your final assessment of model generalizeability.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m2. **Load and preprocess** the test dataset.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       3. **Evaluate** the model on the test data.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Here’s how you can implement this in your SageMaker notebook. The following code will:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/15/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  26\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  38\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/15/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-xgboost/sagemaker-xgboost-2024-10-29-21-48-04-392/output/model.tar.gz\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mxgboost/sagemaker-xgboost-2024-11-01-22-25-59-504/output/model.tar.gz\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/15/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -3,10 +3,6 @@\u001b[m \u001b[mmodel_s3_path = f'{output_folder}/{xgboost_estimator.latest_training_job.name}/o\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       print(model_s3_path)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       local_model_path = 'model.tar.gz'\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-# Load the test set from s3\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-s3.download_file(bucket, 'train.csv', 'train.csv') # only using train because we don't have test data in this example. Pretend this is test data for now.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-test_data = pd.read_csv('train.csv')\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Download the trained model from S3\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       s3.download_file(bucket, model_s3_path, local_model_path)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/16:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 1ac08164-4383-4460-8181-ea096451dde2\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    execution_count: 40\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        []\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      # Load the test set. We downloaded this earlier from our S3 bucket.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      test_data = pd.read_csv(test_filename)\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/16/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  27\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  41\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/16/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: execute_result\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    execution_count: 41\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    data:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      text/html:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        <div>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        <style scoped>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                vertical-align: middle;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            .dataframe tbody tr th {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                vertical-align: top;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            .dataframe thead th {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+                text-align: right;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        </style>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          <thead>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            <tr style=\"text-align: right;\">\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th></th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>PassengerId</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Survived</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Pclass</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Name</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Sex</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Age</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>SibSp</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Parch</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Ticket</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Fare</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Cabin</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>Embarked</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          </thead>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          <tbody>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>0</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>566</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>Davies, Mr. Alfred J</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>24.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>2</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>A/4 48871</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>24.1500</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>1</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>161</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>Cribb, Mr. John Hatfield</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>44.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>371362</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>16.1000</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>2</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>554</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>Leeni, Mr. Fahim (\"Philip Zenni\")</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>22.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>2620</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>7.2250</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>C</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>3</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>861</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>Hansen, Mr. Claus Peter</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>41.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>2</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>350026</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>14.1083</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <th>4</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>242</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>Murphy, Miss. Katherine \"Kate\"</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>female</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>367230</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>15.5000</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+              <td>Q</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          </tbody>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        </table>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        </div>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      text/plain:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+           PassengerId  Survived  Pclass                               Name     Sex  \\\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        0          566         0       3               Davies, Mr. Alfred J    male   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        1          161         0       3           Cribb, Mr. John Hatfield    male   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        2          554         1       3  Leeni, Mr. Fahim (\"Philip Zenni\")    male   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        3          861         0       3            Hansen, Mr. Claus Peter    male   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        4          242         1       3     Murphy, Miss. Katherine \"Kate\"  female   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+            Age  SibSp  Parch     Ticket     Fare Cabin Embarked  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        0  24.0      2      0  A/4 48871  24.1500   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        1  44.0      0      1     371362  16.1000   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        2  22.0      0      0       2620   7.2250   NaN        C  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        3  41.0      2      0     350026  14.1083   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        4   NaN      1      0     367230  15.5000   NaN        Q  \n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/16/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: execute_result\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    execution_count: 27\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    data:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      text/html:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        <div>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        <style scoped>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            .dataframe tbody tr th:only-of-type {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                vertical-align: middle;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            .dataframe tbody tr th {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                vertical-align: top;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            .dataframe thead th {\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                text-align: right;\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            }\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        </style>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        <table border=\"1\" class=\"dataframe\">\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          <thead>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr style=\"text-align: right;\">\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th></th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>PassengerId</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Survived</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Pclass</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Name</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Sex</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Age</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>SibSp</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Parch</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Ticket</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Fare</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Cabin</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>Embarked</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          </thead>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          <tbody>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>0</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Braund, Mr. Owen Harris</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>22.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>A/5 21171</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>7.2500</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>1</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>2</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>38.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>PC 17599</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>71.2833</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>C85</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>C</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>2</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Heikkinen, Miss. Laina</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>26.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>STON/O2. 3101282</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>7.9250</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>3</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>4</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>female</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>35.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>1</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>113803</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>53.1000</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>C123</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            <tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <th>4</th>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>5</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>3</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>Allen, Mr. William Henry</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>male</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>35.0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>0</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>373450</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>8.0500</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>NaN</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-              <td>S</td>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-            </tr>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-          </tbody>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        </table>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        </div>\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      text/plain:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-           PassengerId  Survived  Pclass  \\\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        0            1         0       3   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        1            2         1       1   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        2            3         1       3   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        3            4         1       1   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        4            5         0       3   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-                                                        Name     Sex   Age  SibSp  \\\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-           Parch            Ticket     Fare Cabin Embarked  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        0      0         A/5 21171   7.2500   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        1      0          PC 17599  71.2833   C85        C  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        3      0            113803  53.1000  C123        S  \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        4      0            373450   8.0500   NaN        S  \n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/17/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  28\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  43\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/17/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,3 +1,3 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Preprocess the test set to match the training setup\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-from test_AWS.train_xgboost import preprocess_data\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mfrom test_AWS.scripts.train_xgboost import preprocess_data\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       X_test, y_test = preprocess_data(test_data)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/18/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  29\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  44\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/18/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1 +1 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-Test Set Accuracy: 0.9125\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mTest Set Accuracy: 0.7933\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/18/outputs/1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    name: stderr\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      configuration generated by an older version of XGBoost, please export the model by calling\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      `Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      for more details about differences between saving model and serializing.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:33:42] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/18/outputs/1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stderr\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [22:09:15] WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/20:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  markdown cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 90e53219-1e9f-4dd4-a8a9-150ac540876d\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      #### Setting up the data path\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      In this approach, using `TrainingInput` directly with SageMaker’s built-in XGBoost container contrasts with our previous method, where we specified a custom script with argument inputs (specified in hyperparameters) for data paths and settings. With `TrainingInput`, data paths and formats are managed as structured inputs (`{'train': train_input}`) rather than passed as arguments in a script. This setup simplifies and standardizes data handling in SageMaker’s built-in algorithms, keeping the data configuration separate from hyperparameters.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 6105a27f-099e-40f0-8039-4a983ea412cb\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    execution_count: 48\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        []\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      train_s3_path\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    outputs:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      output 0:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        output_type: execute_result\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        execution_count: 48\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        data:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+          text/plain: 's3://titanic-dataset-test/data/titanic_train.csv'\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## replaced /cells/20/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  10\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  49\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/20/outputs/0/text:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,2 +1,2 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-10-29-19-54-20-817\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mINFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-11-01-22-48-56-105\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/20/outputs/1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:48:57 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:49:11 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:49:42 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:50:22 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:51:29 Training - Training image download completed. Training in progress.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:51:29 Uploading - Uploading generated training model\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:23.778 ip-10-2-247-41.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:23.803 ip-10-2-247-41.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Single node training.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Train matrix has 713 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.213 ip-10-2-247-41.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.214 ip-10-2-247-41.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.215 ip-10-2-247-41.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01:22:51:24:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[0]#011train-rmse:474.71030\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.227 ip-10-2-247-41.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2024-11-01 22:51:24.229 ip-10-2-247-41.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[1]#011train-rmse:441.03842\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[2]#011train-rmse:411.78134\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[3]#011train-rmse:385.35440\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[4]#011train-rmse:362.34192\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[5]#011train-rmse:342.72199\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[6]#011train-rmse:325.37424\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[7]#011train-rmse:310.20413\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[8]#011train-rmse:297.79462\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[9]#011train-rmse:287.85199\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[10]#011train-rmse:277.92941\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[11]#011train-rmse:270.85162\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[12]#011train-rmse:263.09851\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[13]#011train-rmse:257.25269\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[14]#011train-rmse:251.85989\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[15]#011train-rmse:247.19409\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[16]#011train-rmse:243.73045\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[17]#011train-rmse:240.81642\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[18]#011train-rmse:238.41530\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[19]#011train-rmse:235.56351\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[20]#011train-rmse:233.57898\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[21]#011train-rmse:231.39540\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[22]#011train-rmse:228.63503\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[23]#011train-rmse:226.69484\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[24]#011train-rmse:225.35779\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[25]#011train-rmse:223.92523\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[26]#011train-rmse:222.10831\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[27]#011train-rmse:219.23029\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[28]#011train-rmse:218.87340\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[29]#011train-rmse:216.75085\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[30]#011train-rmse:215.76749\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[31]#011train-rmse:214.97679\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[32]#011train-rmse:213.81511\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[33]#011train-rmse:212.42398\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[34]#011train-rmse:211.10745\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[35]#011train-rmse:209.56615\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[36]#011train-rmse:208.38251\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[37]#011train-rmse:207.96460\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[38]#011train-rmse:206.41853\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[39]#011train-rmse:205.13840\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[40]#011train-rmse:204.59671\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[41]#011train-rmse:203.43626\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[42]#011train-rmse:202.23776\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[43]#011train-rmse:201.98227\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[44]#011train-rmse:201.53015\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[45]#011train-rmse:200.83151\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[46]#011train-rmse:199.75769\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[47]#011train-rmse:197.73955\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[48]#011train-rmse:196.67972\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[49]#011train-rmse:195.99304\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[50]#011train-rmse:194.60979\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[51]#011train-rmse:193.87764\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[52]#011train-rmse:193.04419\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[53]#011train-rmse:191.84062\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[54]#011train-rmse:191.63332\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[55]#011train-rmse:191.06137\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[56]#011train-rmse:190.63503\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[57]#011train-rmse:190.23791\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[58]#011train-rmse:190.01700\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[59]#011train-rmse:189.62627\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[60]#011train-rmse:188.78932\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[61]#011train-rmse:187.87903\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[62]#011train-rmse:187.33061\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[63]#011train-rmse:186.93269\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[64]#011train-rmse:186.04112\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[65]#011train-rmse:185.29774\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[66]#011train-rmse:184.67114\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[67]#011train-rmse:183.74358\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[68]#011train-rmse:183.30225\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[69]#011train-rmse:182.09914\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[70]#011train-rmse:181.83897\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[71]#011train-rmse:181.03862\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[72]#011train-rmse:180.78651\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[73]#011train-rmse:179.64867\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[74]#011train-rmse:178.82935\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[75]#011train-rmse:178.21071\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[76]#011train-rmse:177.54585\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[77]#011train-rmse:177.00539\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[78]#011train-rmse:176.26054\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[79]#011train-rmse:175.64746\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[80]#011train-rmse:174.62911\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[81]#011train-rmse:174.01623\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[82]#011train-rmse:173.50301\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[83]#011train-rmse:172.43010\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[84]#011train-rmse:171.95624\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[85]#011train-rmse:171.48639\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[86]#011train-rmse:171.19154\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[87]#011train-rmse:169.97925\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[88]#011train-rmse:169.45494\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[89]#011train-rmse:168.90468\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[90]#011train-rmse:168.16402\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[91]#011train-rmse:167.30739\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[92]#011train-rmse:166.85228\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[93]#011train-rmse:165.98686\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[94]#011train-rmse:165.70697\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[95]#011train-rmse:165.43739\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[96]#011train-rmse:164.83107\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[97]#011train-rmse:164.22020\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[98]#011train-rmse:163.90085\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \u001b[34m[99]#011train-rmse:163.45399\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      2024-11-01 22:51:43 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Training seconds: 120\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      Billable seconds: 120\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    output_type: error\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    ename: NameError\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    evalue: name 'instance_type' is not defined\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    traceback:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      item[0]: \u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      item[1]: \u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      item[2]:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        Cell \u001b[0;32mIn[49], line 28\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        \u001b[1;32m     25\u001b[0m xgboost_estimator_builtin\u001b[38;5;241m.\u001b[39mfit({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train_input})\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        \u001b[1;32m     26\u001b[0m end \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        \u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime for training on SageMaker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds, instance_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43minstance_type\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, instance_count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstance_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      item[3]: \u001b[0;31mNameError\u001b[0m: name 'instance_type' is not defined\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## deleted /cells/20/outputs/1:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-  output:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    name: stdout\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    text:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:54:24 Starting - Starting the training job...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:54:38 Starting - Preparing the instances for training...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:55:10 Downloading - Downloading input data...\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:55:55 Downloading - Downloading the training image......\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:56:51 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-        from pandas import MultiIndex, Int64Index\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.068 ip-10-0-106-124.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.103 ip-10-0-106-124.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Single node training.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Train matrix has 892 rows and 11 columns\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.643 ip-10-0-106-124.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.644 ip-10-0-106-124.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.645 ip-10-0-106-124.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.645 ip-10-0-106-124.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29:19:56:57:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[0]#011train-rmse:475.71411\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.661 ip-10-0-106-124.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2024-10-29 19:56:57.663 ip-10-0-106-124.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[1]#011train-rmse:441.49982\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[2]#011train-rmse:411.72672\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[3]#011train-rmse:385.56036\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[4]#011train-rmse:363.56360\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[5]#011train-rmse:344.49588\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[6]#011train-rmse:327.67941\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[7]#011train-rmse:312.88376\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[8]#011train-rmse:299.41186\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[9]#011train-rmse:288.74734\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[10]#011train-rmse:280.12149\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[11]#011train-rmse:273.00226\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[12]#011train-rmse:266.27246\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[13]#011train-rmse:259.31256\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[14]#011train-rmse:253.75191\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[15]#011train-rmse:249.62512\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[16]#011train-rmse:245.67200\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[17]#011train-rmse:242.34294\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[18]#011train-rmse:238.86392\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[19]#011train-rmse:235.89893\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[20]#011train-rmse:233.08173\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[21]#011train-rmse:231.52962\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[22]#011train-rmse:229.66519\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[23]#011train-rmse:227.86470\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[24]#011train-rmse:226.72954\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[25]#011train-rmse:225.20438\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[26]#011train-rmse:223.76180\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[27]#011train-rmse:221.96107\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[28]#011train-rmse:221.09845\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[29]#011train-rmse:219.99710\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[30]#011train-rmse:219.37936\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[31]#011train-rmse:218.05364\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[32]#011train-rmse:217.43600\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[33]#011train-rmse:216.73910\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[34]#011train-rmse:216.43459\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[35]#011train-rmse:215.56277\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[36]#011train-rmse:214.80632\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[37]#011train-rmse:213.70375\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[38]#011train-rmse:213.17102\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[39]#011train-rmse:212.88145\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[40]#011train-rmse:211.96532\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[41]#011train-rmse:211.32878\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[42]#011train-rmse:210.17601\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[43]#011train-rmse:209.69156\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[44]#011train-rmse:208.88245\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[45]#011train-rmse:207.83882\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[46]#011train-rmse:206.71755\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[47]#011train-rmse:205.46107\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[48]#011train-rmse:204.46623\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[49]#011train-rmse:203.54263\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[50]#011train-rmse:202.88849\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[51]#011train-rmse:202.33600\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[52]#011train-rmse:201.32494\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[53]#011train-rmse:200.73129\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[54]#011train-rmse:200.46822\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[55]#011train-rmse:199.81789\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[56]#011train-rmse:199.07161\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[57]#011train-rmse:198.43318\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[58]#011train-rmse:198.14304\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[59]#011train-rmse:197.53601\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[60]#011train-rmse:197.10297\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[61]#011train-rmse:196.49066\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[62]#011train-rmse:196.28507\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[63]#011train-rmse:195.67941\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[64]#011train-rmse:195.51599\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[65]#011train-rmse:194.97147\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[66]#011train-rmse:193.90741\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[67]#011train-rmse:193.52390\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[68]#011train-rmse:193.01285\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[69]#011train-rmse:192.34790\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[70]#011train-rmse:191.98561\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[71]#011train-rmse:191.39389\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[72]#011train-rmse:190.95151\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[73]#011train-rmse:190.21582\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[74]#011train-rmse:189.08704\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[75]#011train-rmse:188.47955\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[76]#011train-rmse:188.12349\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[77]#011train-rmse:187.77058\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[78]#011train-rmse:187.10945\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[79]#011train-rmse:186.61465\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[80]#011train-rmse:185.80434\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[81]#011train-rmse:184.99844\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[82]#011train-rmse:184.62537\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[83]#011train-rmse:184.16344\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[84]#011train-rmse:183.58179\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[85]#011train-rmse:183.19162\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[86]#011train-rmse:182.80438\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[87]#011train-rmse:182.35306\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[88]#011train-rmse:181.88933\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[89]#011train-rmse:181.32742\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[90]#011train-rmse:180.80669\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[91]#011train-rmse:180.49007\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[92]#011train-rmse:179.89072\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[93]#011train-rmse:179.37184\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[94]#011train-rmse:179.06938\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[95]#011train-rmse:178.65883\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[96]#011train-rmse:177.99016\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[97]#011train-rmse:177.48030\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[98]#011train-rmse:177.15761\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \u001b[34m[99]#011train-rmse:176.45931\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:57:20 Uploading - Uploading generated training model\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      2024-10-29 19:57:20 Completed - Training job completed\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Training seconds: 130\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Billable seconds: 130\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-      Training time on SageMaker with built-in XGBoost image: 197.50 seconds\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/20/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,11 +1,15 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       from sagemaker.estimator import Estimator # when using images, we use the general Estimator class\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define instance type/count we'll use for training\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Use Estimator directly for built-in container without specifying entry_point\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator_builtin = Estimator(\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\"),\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           role=role,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    instance_count=1,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    instance_type=\"ml.m5.large\",\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_count=instance_count,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_type=instance_type,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           output_path=output_path,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           sagemaker_session=session,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           hyperparameters={\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -25,4 +29,4 @@\u001b[m \u001b[mstart = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator_builtin.fit({'train': train_input})\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       end = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-print(f\"Training time on SageMaker with built-in XGBoost image: {end - start:.2f} seconds\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {instance_count}\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/22/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -20,11 +20,24 @@\u001b[m \u001b[mUpgrading a single instance works well if:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Upgrading a single instance is typically the most efficient option in terms of both cost and setup complexity. It avoids the communication overhead associated with multi-instance setups (discussed below) and is well-suited for most small to medium-sized datasets.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ### Option 2: Use Multiple Instances for Distributed Training\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIf upgrading a single instance doesn’t sufficiently reduce training time, distributed training across multiple instances may be a viable alternative, particularly for larger datasets and complex models. SageMaker supports two primary distributed training techniques: **data parallelism** and **model parallelism**.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-If upgrading a single instance doesn’t sufficiently reduce training time, distributed training across multiple instances may be a viable alternative, particularly for larger datasets and complex models. In SageMaker, setting `instance_count > 1` enables data parallelism, where data is split across instances for simultaneous processing.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m#### Understanding Data Parallelism vs. Model Parallelism\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Data Parallelism**: This approach splits the dataset across multiple instances, allowing each instance to process a subset of the data independently. After each batch, gradients are synchronized across instances to ensure consistent updates to the model. Data parallelism is effective when the model itself fits within an instance’s memory, but the data size or desired training speed requires faster processing through multiple instances.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Model Parallelism**: Model parallelism divides the model itself across multiple instances, making it ideal for very large models (e.g., deep learning models in NLP or image processing) that cannot fit in memory on a single instance. Each instance processes a segment of the model, and results are combined during training. This approach is suitable for memory-intensive models that exceed the capacity of a single instance.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m#### How SageMaker Chooses Between Data and Model Parallelism\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIn SageMaker, the choice between data and model parallelism is not entirely automatic. Here’s how it typically works:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Data Parallelism (Automatic)**: When you set `instance_count > 1`, SageMaker will automatically apply data parallelism. This splits the dataset across instances, allowing each instance to process a subset independently and synchronize gradients after each batch. Data parallelism works well when the model can fit in the memory of a single instance, but the data size or processing speed needs enhancement with multiple instances.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Model Parallelism (Manual Setup)**: To enable model parallelism, you need to configure it explicitly using the **SageMaker Model Parallel Library**, suitable for deep learning models in frameworks like PyTorch or TensorFlow. Model parallelism splits the model itself across multiple instances, which is useful for memory-intensive models that exceed the capacity of a single instance. Configuring model parallelism requires setting up a distribution strategy in SageMaker’s Python SDK.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m- **Hybrid Parallelism (Manual Setup)**: For extremely large datasets and models, SageMaker can support both data and model parallelism together, but this setup requires manual configuration. Hybrid parallelism is beneficial for workloads that are both data- and memory-intensive, where both the model and the data need distributed processing.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-- **Data Parallelism**: This approach divides the dataset across instances, each instance processing a subset of the data independently. The gradients are then synchronized across instances, which helps reduce training time without requiring the dataset to fit entirely in memory on a single instance.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-- **Model Parallelism**: Useful for extremely large models that don’t fit in memory on a single instance, model parallelism divides the model itself across instances. This is particularly effective for memory-intensive models, such as those in natural language processing and large image processing networks.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       **When to Use Distributed Training with Multiple Instances**  \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       Consider multiple instances if:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -58,10 +71,10 @@\u001b[m \u001b[mLet’s break down some key points for deciding between **1 instance vs. multipl\u001b[m\n",
      "\u001b[31m-      \u001b[32m+            - **10 instances (parallel):** `(T / k) * (10 * $C)`, where `k` is the speedup factor (<10 due to overhead).\u001b[m\n",
      "\u001b[31m-      \u001b[32m+          - If the speedup is only about 5x instead of 10x due to communication overhead, then the cost difference may be minimal, with a slight edge to a single instance on total cost but at a higher wall-clock time.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m----\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-In summary:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-- **Start by upgrading to a more powerful instance (Option 1)** for datasets up to 10 GB and moderately complex models. A single, more powerful, instance is usually more cost-effective for smaller workloads and where time isn’t critical. Running initial tests with a single instance can also provide a benchmark. You can then experiment with small increases in instance count to find a balance between cost and time savings, particularly considering communication overheads that affect parallel efficiency.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-- **Consider distributed training across multiple instances (Option 2)** only when dataset size, model complexity, or training time demand it.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> In summary:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> - **Start by upgrading to a more powerful instance (Option 1)** for datasets up to 10 GB and moderately complex models. A single, more powerful, instance is usually more cost-effective for smaller workloads and where time isn’t critical. Running initial tests with a single instance can also provide a benchmark. You can then experiment with small increases in instance count to find a balance between cost and time savings, particularly considering communication overheads that affect parallel efficiency.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> - **Consider distributed training across multiple instances (Option 2)** only when dataset size, model complexity, or training time demand it.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ---\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/23/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,5 +1,10 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ## XGBoost's Distributed Training Mechanism\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-In the event that option 2 explained above really is better for your use-case (e.g., you have a very large dataset or model that takes a while to train even with high performance instances), the next example will demo setting this up. Before we do, though, we should ask what distributed computing really means for our specific model/setup.XGBoost’s distributed training relies on a data-parallel approach that divides the dataset across multiple instances (or workers), enabling each instance to work on a portion of the data independently. This strategy enhances efficiency, especially for large datasets and computationally intensive tasks. Here’s how distributed training in XGBoost works, particularly in the SageMaker environment:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mIn the event that option 2 explained above really is better for your use-case (e.g., you have a very large dataset or model that takes a while to train even with high performance instances), the next example will demo setting this up. Before we do, though, we should ask what distributed computing really means for our specific model/setup. XGBoost’s distributed training relies on a data-parallel approach that divides the dataset across multiple instances (or workers), enabling each instance to work on a portion of the data independently. This strategy enhances efficiency, especially for large datasets and computationally intensive tasks.\u001b[m\u001b[41m \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m> **What about a model parallelism approach?** Unlike deep learning models with vast neural network layers, XGBoost’s decision trees are usually small enough to fit in memory on a single instance, even when the dataset is large. Thus, model parallelism is rarely necessary.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mXGBoost does not inherently support model parallelism out of the box in SageMaker because the model architecture doesn’t typically exceed memory limits, unlike massive language or image models. Although model parallelism can be theoretically applied (e.g., splitting large tree structures across instances), it's generally not supported natively in SageMaker for XGBoost, as it would require a custom distribution framework to split the model itself.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mHere’s how distributed training in XGBoost works, particularly in the SageMaker environment:\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       ### Key Steps in Distributed Training with XGBoost\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/24/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -1,12 +1,16 @@\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       from sagemaker.inputs import TrainingInput\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       from sagemaker.estimator import Estimator\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m# Define instance type/count we'll use for training\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_type=\"ml.m5.large\"\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32minstance_count=1 # always start with 1. Rarely is parallelized training justified with data < 50 GB.\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Define the XGBoost estimator for distributed training\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator = Estimator(\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           image_uri=sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\"),\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           role=role,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    instance_count=1,  # Start with 1 instance for baseline\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-    instance_type=\"ml.m5.large\",\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_count=instance_count,  # Start with 1 instance for baseline\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32m    instance_type=instance_type,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           output_path=output_path,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+           sagemaker_session=session,\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       )\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -21,15 +25,16 @@\u001b[m \u001b[mxgboost_estimator.set_hyperparameters(\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       )\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Specify input data from S3\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-train_input = TrainingInput(f\"s3://{bucket}/train.csv\", content_type=\"csv\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mtrain_input = TrainingInput(train_s3_path, content_type=\"csv\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Run with 1 instance\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       start_time = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator.fit({\"train\": train_input})\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-print(f\"Training time with 1 instance: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {xgboost_estimator.instance_count}\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Now run with 2 instances to observe speedup\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator.instance_count = 2\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       start_time = t.time()\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       xgboost_estimator.fit({\"train\": train_input})\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-print(f\"Training time with 2 instances: {t.time() - start_time:.2f} seconds\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mprint(f\"Runtime for training on SageMaker: {end - start:.2f} seconds, instance_type: {instance_type}, instance_count: {xgboost_estimator.instance_count}\")\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/26:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: d2e722d7-93d0-456d-8a5b-3c3f9e89c5b7\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    execution_count: 51\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    metadata (known keys):\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      tags:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+        []\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    source:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+      print('yay, code this far up works')\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+  code cell:\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+    id: 802be838-91b5-43bb-b7e1-c3a70250d22d\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\u001b[34m\u001b[1m## modified /cells/27/source:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[36m@@ -4,7 +4,7 @@\u001b[m \u001b[mfrom sklearn.preprocessing import StandardScaler, LabelEncoder\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       import numpy as np\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Load and preprocess the Titanic dataset\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[31m-df = pd.read_csv('train.csv')\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \u001b[32m+\u001b[m\u001b[32mdf = pd.read_csv(train_filename)\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       \u001b[m\n",
      "\u001b[31m-      \u001b[32m+       # Encode categorical variables and normalize numerical ones\u001b[m\n",
      "\u001b[31m-      \u001b[32m+       df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\u001b[m\n",
      "\u001b[31m-      \u001b[32m+      \n",
      "\u001b[31m-      \u001b[32m+      \u001b[0m\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## replaced (type changed from NoneType to int) /cells/4/execution_count:\u001b[0m\n",
      "\u001b[31m-      \u001b[31m-  None\n",
      "\u001b[31m-      \u001b[32m+  5\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## added /cells/4/metadata/tags:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  []\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\u001b[34m\u001b[1m## inserted before /cells/4/outputs/0:\u001b[0m\n",
      "\u001b[31m-      \u001b[32m+  output:\n",
      "\u001b[31m-      \u001b[32m+    output_type: stream\n",
      "\u001b[31m-      \u001b[32m+    name: stdout\n",
      "\u001b[31m-      \u001b[32m+    text:\n",
      "\u001b[31m-      \u001b[32m+      [main cc909ef] Updates from Jupyter notebooks\n",
      "\u001b[31m-      \u001b[32m+       4 files changed, 525 insertions(+), 516 deletions(-)\n",
      "\u001b[31m-      \u001b[32m+       create mode 100644 scripts/__pycache__/train_xgboost.cpython-310.pyc\n",
      "\u001b[31m-      \u001b[32m+      From https://github.com/UW-Madison-DataScience/test_AWS\n",
      "\u001b[31m-      \u001b[32m+       * branch            main       -> FETCH_HEAD\n",
      "\u001b[31m-      \u001b[32m+         22d83d0..8102f2c  main       -> origin/main\n",
      "\u001b[31m-      \u001b[32m+      Already up to date.\n",
      "\u001b[31m-      \u001b[32m+      remote: Support for password authentication was removed on August 13, 2021.\n",
      "\u001b[31m-      \u001b[32m+      remote: Please see https://docs.github.com/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n",
      "\u001b[31m-      \u001b[32m+      fatal: Authentication failed for 'https://github.com/UW-Madison-DataScience/test_AWS.git/'\n",
      "\u001b[31m-      \n",
      "\u001b[31m-      \u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## replaced /cells/4/execution_count:\u001b[0m\n",
      "\u001b[31m-  10\n",
      "\u001b[32m+  16\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## inserted before /cells/4/outputs/0:\u001b[0m\n",
      "\u001b[32m+  output:\n",
      "\u001b[32m+    output_type: stream\n",
      "\u001b[32m+    name: stdout\n",
      "\u001b[32m+    text:\n",
      "\u001b[32m+      fatal: not a git repository (or any parent up to mount point /home/ec2-user)\n",
      "\u001b[32m+      Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "\u001b[32m+      fatal: not a git repository (or any parent up to mount point /home/ec2-user)\n",
      "\u001b[32m+      Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "\u001b[32m+      fatal: not in a git directory\n",
      "\u001b[32m+      fatal: not a git repository (or any parent up to mount point /home/ec2-user)\n",
      "\u001b[32m+      Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "\u001b[32m+      fatal: not a git repository (or any parent up to mount point /home/ec2-user)\n",
      "\u001b[32m+      Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## deleted /cells/4/outputs/0:\u001b[0m\n",
      "\u001b[31m-  output:\n",
      "\u001b[31m-    output_type: stream\n",
      "\u001b[31m-    name: stdout\n",
      "\u001b[31m-    text:\n",
      "\u001b[31m-      [main bf003ce] Updates from Jupyter notebooks\n",
      "\u001b[31m-       3 files changed, 3882 insertions(+), 1782 deletions(-)\n",
      "\u001b[31m-      From https://github.com/UW-Madison-DataScience/test_AWS\n",
      "\u001b[31m-       * branch            main       -> FETCH_HEAD\n",
      "\u001b[31m-         8102f2c..d6b124c  main       -> origin/main\n",
      "\u001b[31m-      Already up to date.\n",
      "\u001b[31m-      Enumerating objects: 11, done.\n",
      "\u001b[31m-      Counting objects: 100% (11/11), done.\n",
      "\u001b[31m-      Delta compression using up to 2 threads\n",
      "\u001b[31m-      Compressing objects: 100% (6/6), done.\n",
      "\u001b[31m-      Writing objects: 100% (6/6), 29.53 KiB | 1.48 MiB/s, done.\n",
      "\u001b[31m-      Total 6 (delta 4), reused 0 (delta 0), pack-reused 0\n",
      "\u001b[31m-      remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
      "\u001b[31m-      To https://github.com/UW-Madison-DataScience/test_AWS.git\n",
      "\u001b[31m-         d6b124c..bf003ce  main -> main\n",
      "\n",
      "\u001b[0m\u001b[34m\u001b[1m## modified /cells/5/source:\u001b[0m\n",
      "\u001b[36m@@ -0,0 +1 @@\u001b[m\n",
      "\u001b[32m+\u001b[m\u001b[32m%cd /home/ec2-user/SageMaker/\u001b[m\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !git diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a80ca60f-5e62-45e3-bfbb-9b62712688e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main e973358] Updates from Jupyter notebooks\n",
      " 7 files changed, 2058 insertions(+), 2809 deletions(-)\n",
      " create mode 100644 Check_running_resources.ipynb\n",
      " create mode 100644 scripts/AWS_helpers.py\n",
      " create mode 100644 titanic_test.csv\n",
      " create mode 100644 titanic_train.csv\n",
      " create mode 100644 xgboost-model\n",
      "From https://github.com/UW-Madison-DataScience/test_AWS\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   bf003ce..4a3af22  main       -> origin/main\n",
      "Already up to date.\n",
      "Enumerating objects: 14, done.\n",
      "Counting objects: 100% (14/14), done.\n",
      "Delta compression using up to 2 threads\n",
      "Compressing objects: 100% (10/10), done.\n",
      "Writing objects: 100% (10/10), 80.57 KiB | 3.50 MiB/s, done.\n",
      "Total 10 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To https://github.com/UW-Madison-DataScience/test_AWS.git\n",
      "   4a3af22..e973358  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add . # you may also add files one at a time, for further specificity over the associated commit message\n",
    "!git commit -m \"Updates from Jupyter notebooks\" # in general, your commit message should be more specific!\n",
    "\n",
    "!git config pull.rebase false # Combines the remote changes into your local branch as a merge commit.\n",
    "!git pull origin main\n",
    "\n",
    "!git push https://{username}:{token}@{github_url} main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1266fce9-b4b4-4134-88d6-7fdcd736a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ec2-user/SageMaker/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f6f61-28d3-4be4-9e93-f5bedcbd9780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cd0667-9ebd-418f-bafc-c8ff1126425a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbaca0-b333-4fc0-b185-d6b3a123e9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f88fc0-4dc5-43b8-aa1f-1faaed6ecd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34be5f6-f87d-43aa-8dc0-99030a92caf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f393cf-e481-4dfd-acbf-3862b1488e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bf6dd-d191-404d-9a27-35101349d400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
