{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33ab97a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Overview of RAG Workflows on AWS\"\n",
    "teaching: 10\n",
    "exercises: 0\n",
    "---\n",
    "\n",
    "## Retrieval-Augmented Generation (RAG) on AWS\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a pattern where you **retrieve** relevant context from your data and then **generate** an answer using that context. Unlike model training, a standard RAG workflow does **not** fine‑tune or train a model — it combines retrieval + inference only.\n",
    "\n",
    "This episode introduces the major ways to build RAG systems on AWS and prepares us for later episodes where we experiment with each approach.\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- What is Retrieval‑Augmented Generation (RAG)?\n",
    "- What are the main architectural options for running RAG on AWS?\n",
    "- When is each RAG workflow appropriate?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Understand that RAG does *not* require training or fine‑tuning a model.\n",
    "- Recognize the three major architectural patterns for RAG systems on AWS.\n",
    "- Understand the core trade‑offs that drive which approach to use.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "RAG combines two steps:\n",
    "\n",
    "1. **Retrieve**: Search your document store (vector DB or FAISS index) to find relevant text.\n",
    "2. **Generate**: Provide those retrieved chunks to a large language model (LLM) to answer a question.\n",
    "\n",
    "No model weights are updated. No backprop. No training job.  \n",
    "RAG is an inference‑only pattern that layers retrieval logic around an LLM.\n",
    "\n",
    "## Approaches to Running RAG on AWS\n",
    "\n",
    "Below are the three common patterns used across research and industry.\n",
    "\n",
    "### 1. **Notebook‑based RAG (single GPU instance)**\n",
    "Load embedding + generation models inside a GPU‑backed SageMaker notebook instance.\n",
    "\n",
    "**When this works well**\n",
    "- Small/medium models (< 8–12B). Large models also work, but be mindful of instance costs!\n",
    "- Workshops, demos, prototyping, exploratory RAG\n",
    "- RAG pipelines that only need to be run once (not live in production mode where you can constantly query your bot)\n",
    "- You want everything in one place with minimal architecture\n",
    "- You are feeling lazy and don't want to convert your code to a processing job or API calls\n",
    "\n",
    "**Trade‑offs**\n",
    "- Must shut down instance to avoid cost leakage\n",
    "- Limited by single‑GPU memory\n",
    "- Not all of the time spent in your notebook requires a GPU (wasteful for chunking + indexing)\n",
    "- If you want to use a better instance later, you have to stop your notebook, change the instance type, and restart it (annoying but doable)\n",
    "\n",
    "### 2. **Batch RAG with SageMaker Processing Jobs** (effecient!)\n",
    "Use short‑lived processing jobs for embedding corpora or offline batch generation.\n",
    "\n",
    "**When this works well**\n",
    "- Only pay for the GPU time that you actually need (embedding and generation steps)\n",
    "- \"Compute once, use many times\" tasks\n",
    "- Repeatable pipelines where you want clean logs + reproducibility\n",
    "\n",
    "**Trade‑offs**\n",
    "- Not suitable for *interactive* or per‑query RAG (startup time too slow)\n",
    "- Requires setting up processing jobs in SageMaker, similar to how we've done training jobs before. This isn't terribly difficult once you learn the basics, but it can slow progress the first time through.\n",
    "\n",
    "### 3. **Fully managed RAG using Amazon Bedrock APIs**\n",
    "Use Bedrock models for embedding + generation via API calls.\n",
    "\n",
    "**When this works well**\n",
    "- No desire to host or manage models\n",
    "- Need scalability without managing GPUs\n",
    "- Want access to the largest and greatest foundation models with guaranteed availability\n",
    "\n",
    "**Trade‑offs**\n",
    "- Per‑token cost may be higher than renting an instance/GPU if you have a large corpus or high query volume\n",
    "- Latency may be higher due to network calls\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: callout\n",
    "\n",
    "### RAG Pipeline Responsibilities\n",
    "Even with Bedrock, *you* still manage:\n",
    "- Chunking\n",
    "- Embedding storage\n",
    "- Vector search\n",
    "- Retrieval logic  \n",
    "\n",
    "Bedrock only handles the embedding + generation models.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "## When Do You Use Which Approach?\n",
    "\n",
    "**Notebook RAG**  \n",
    "Fastest to build. Great for learning, prototyping, and small‑scale research.\n",
    "\n",
    "**Processing‑job RAG**  \n",
    "Ideal for embedding large corpora and running periodic batch generation. Clean, reproducible, cost‑efficient.\n",
    "\n",
    "**Bedrock RAG**  \n",
    "Best for production or long‑term research tools that need scalability without hosting models.\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- RAG is an inference‑only workflow: no training or fine‑tuning required.\n",
    "- AWS supports three broad approaches: notebook RAG, batch RAG, and Bedrock‑managed RAG.\n",
    "- The right choice depends on latency needs, scale, cost sensitivity, and model‑management preferences.\n",
    "- Later episodes will walk through each pattern in depth using hands‑on examples.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
