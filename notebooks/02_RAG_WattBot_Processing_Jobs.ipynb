{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3300c01",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with Processing Jobs\"\n",
    "teaching: 30\n",
    "exercises: 20\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions\n",
    "\n",
    "- TODO\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- TODO\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "\n",
    "# RAG with Processing Jobs\n",
    "\n",
    "In the previous episode, we ran the entire WattBot RAG pipeline on a single GPU-backed SageMaker notebook. That was simple to teach, but the GPU sat idle while we downloaded PDFs, chunked text, and evaluated results.\n",
    "\n",
    "In this Episode 2 notebook, we will keep the same WattBot corpus and RAG logic, but restructure how we use AWS:\n",
    "\n",
    "- The notebook itself can run on a small CPU-only instance.\n",
    "- We regenerate pages and chunks locally, as before.\n",
    "- We save the chunks to S3.\n",
    "- We run two short-lived SageMaker Processing jobs on a GPU:\n",
    "  1. One job computes embeddings for all chunks.\n",
    "  2. A second job runs the full RAG loop (retrieval + Qwen) over all training questions.\n",
    "\n",
    "With this approach, we can more effectively use GPU resources only when needed, and we can scale out to larger corpora, models, and hardware more easily. The downside here is that you have to wait for processing jobs to spin up and run in batch mode on your queries. For many research applications of RAG, this is fine. However, if you want a near-real time chatbot you can have back and forth discussion with, this approach will not work. In the following episodes, we will discuss how we can use *Bedrock* or our own *model inference endpoints* to query models more rapidly.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll first need to clone in some .py files that contain helper functions for embedding and RAG processing jobs. Since we're using containerized Processing jobs, we can't just import local Python functions from the notebook. Instead, we create standalone scripts that the jobs can run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/carpentries-incubator/ML_with_AWS_SageMaker.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32295414",
   "metadata": {},
   "source": [
    "Create a /code directory and copy over the relevant scripts from ML_with_AWS_SageMaker/scripts into /code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e71b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "- `embedding_inference.py` – generic embedding script\n",
    "- `wattbot_rag_batch.py` – WattBot-specific RAG logic for batch processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcda752",
   "metadata": {},
   "source": [
    "Next, setup your AWS SDK, SageMaker session, and S3 bucket information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f40bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket_name = \"chris-rag-2\"          # reuse your bucket from Episode 1\n",
    "# bucket_region = \"us-east-1\"\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "local_data_dir = \"./data\"\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "corpus_dir = local_data_dir + \"/pdfs/\"\n",
    "os.makedirs(corpus_dir, exist_ok=True)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Bucket:\", bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a769e2",
   "metadata": {},
   "source": [
    "## Step 1 – Load WattBot metadata and training questions\n",
    "\n",
    "We reuse the same `metadata.csv` and `train_QA.csv` files from Episode 1. If they are not already on the notebook file system, we download them from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_read_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "\n",
    "metadata_path = os.path.join(local_data_dir, \"metadata.csv\")\n",
    "train_qa_path = os.path.join(local_data_dir, \"train_QA.csv\")\n",
    "corpus_path = os.path.join(corpus_dir, \"corpus.zip\")\n",
    "\n",
    "if not os.path.exists(metadata_path):\n",
    "    s3_client.download_file(bucket_name, \"metadata.csv\", metadata_path)\n",
    "if not os.path.exists(train_qa_path):\n",
    "    s3_client.download_file(bucket_name, \"train_QA.csv\", train_qa_path)\n",
    "if not os.path.exists(corpus_path):\n",
    "    s3_client.download_file(bucket_name, \"corpus.zip\", corpus_path)\n",
    "\n",
    "metadata_df = smart_read_csv(metadata_path)\n",
    "train_df = smart_read_csv(train_qa_path)\n",
    "\n",
    "print(\"Metadata rows:\", len(metadata_df))\n",
    "print(\"Train QAs:\", len(train_df))\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3063eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(corpus_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(corpus_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78135c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b43bce",
   "metadata": {},
   "source": [
    "## Step 2 – Verify chunks exist on S3 (from previous episode)\n",
    "\n",
    "For our processing job, we'll reuse the same  chunks generated in prev. episode. The code below just verifies you have this file available in S3 (for calling from the processing job)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e392019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chunks from s3\n",
    "chunks_s3_key = 'chunks.jsonl'\n",
    "chunks_s3_uri = f\"s3://{bucket_name}/{chunks_s3_key}\"\n",
    "local_chunks_path = os.path.join(local_data_dir, chunks_s3_key)\n",
    "if not os.path.exists(local_chunks_path):\n",
    "    s3_client.download_file(bucket_name, chunks_s3_key, local_chunks_path)\n",
    "with open(local_chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    chunked_docs = [json.loads(line) for line in f]\n",
    "print(f\"Loaded {len(chunked_docs)} chunks from {local_chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c3067",
   "metadata": {},
   "source": [
    "## Step 3 – Processing Job 1: embed all chunks on a GPU\n",
    "\n",
    "Now we launch a short-lived Hugging Face **Processing job** that:\n",
    "\n",
    "1. Downloads `chunks.jsonl` from S3.\n",
    "2. Loads `thenlper/gte-large` from Hugging Face.\n",
    "3. Encodes each chunk into an embedding vector.\n",
    "4. Saves the full matrix as `embeddings.npy` back to S3.\n",
    "\n",
    "We use the same `embedding_inference.py` script across projects; here it expects a JSONL file with a `text` field.\n",
    "\n",
    "\n",
    "### But first...\n",
    "we have to create a requirements.txt file that will add additional libraries to the HuggingFaceProcessor we use below, which builds the environment we'll run our embedding_inference.py script in. For the processing job to recognize this dependence, we'll add it to the source_dir (code/) referenced when we call embedding_processor.run() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd05f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [\n",
    "    \"sentence-transformers\",\n",
    "    # add more packages here if needed\n",
    "]\n",
    "\n",
    "req_path = \"code/requirements.txt\"\n",
    "with open(req_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(requirements))\n",
    "\n",
    "print(f\"Created requirements.txt at {req_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43977d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"thenlper/gte-large\"\n",
    "script_path = \"embedding_inference.py\"\n",
    "\n",
    "emb_output_prefix = \"embeddings\"\n",
    "emb_output_path = f\"s3://{bucket_name}/{emb_output_prefix}/\"\n",
    "\n",
    "embedding_processor = HuggingFaceProcessor(\n",
    "    base_job_name=\"WattBot-embed-gte-large\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.56\",\n",
    "    pytorch_version=\"2.8\",\n",
    "    py_version=\"py312\",\n",
    "    sagemaker_session=session,\n",
    "    max_runtime_in_seconds=2 * 60 * 60,\n",
    ")\n",
    "\n",
    "embedding_processor.run(\n",
    "    code=script_path,\n",
    "    source_dir=\"code/\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=chunks_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"embeddings\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=emb_output_path,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--model_id\", embedding_model_id,\n",
    "        \"--input_filename\", \"chunks.jsonl\",\n",
    "        \"--text_key\", \"text\",\n",
    "        \"--input_dir\", \"/opt/ml/processing/input\",\n",
    "        \"--output_dir\", \"/opt/ml/processing/output\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Embedding job complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7564342",
   "metadata": {},
   "source": [
    "### Check on running job in AWS Console\n",
    "To view the job running from the AWS Console, you can visit SageMaker AI, and then find the \"Data Preparation\" dropdown menu on the left side panel. Click that to find \"Processing jobs\". If you're in us-east-1, the following link should bring you there: [https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs)\n",
    "\n",
    "It may take ~5 minutes in total for the job to complete. This is the downside of launching jobs, but the good news is that we only need to launch one embedding job for our RAG pipeline. This strategy also ensures we're only paying for GPUs when we need them during the processing job.\n",
    "\n",
    "\n",
    "### Sanity-check the embeddings locally\n",
    "\n",
    "We can download `embeddings.npy` back into the notebook and inspect its shape to confirm the job ran successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_embeddings_path = os.path.join(local_data_dir, \"embeddings.npy\")\n",
    "embeddings_key = f\"{emb_output_prefix}/embeddings.npy\"\n",
    "\n",
    "s3_client.download_file(bucket_name, embeddings_key, local_embeddings_path)\n",
    "chunk_embeddings = np.load(local_embeddings_path)\n",
    "\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159b6c4",
   "metadata": {},
   "source": [
    "## Step 5 – Processing Job 2: full WattBot RAG over all questions\n",
    "\n",
    "For the second job, we pass four inputs:\n",
    "\n",
    "- `wattbot_chunks.jsonl` – serialized chunks\n",
    "- `embeddings.npy` – precomputed chunk embeddings\n",
    "- `train_QA.csv` – training questions (to compare with ground truth)\n",
    "- `metadata.csv` – to resolve `ref_id` → URL\n",
    "\n",
    "The script `wattbot_rag_batch.py` reuses the RAG helpers from Episode 1:\n",
    "\n",
    "- cosine similarity + `retrieve_top_k`\n",
    "- `retrieve_context_for_question`\n",
    "- `answer_phase_for_question` (Qwen answer, answer_value, ref_ids, is_blank)\n",
    "- `explanation_phase_for_question`\n",
    "- `run_single_qa` (hybrid unanswerable logic: retrieval threshold + LLM is_blank)\n",
    "\n",
    "The job writes out `wattbot_solutions.csv` in the WattBot submission format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6490543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload CSVs so the job can read them\n",
    "train_qa_key = \"train_QA.csv\"\n",
    "metadata_key = \"metadata.csv\"\n",
    "\n",
    "train_qa_s3 = f\"s3://{bucket_name}/{train_qa_key}\"\n",
    "metadata_s3 = f\"s3://{bucket_name}/{metadata_key}\"\n",
    "emb_output_s3 = f\"s3://{bucket_name}/{emb_output_prefix}/embeddings.npy\"\n",
    "\n",
    "print(\"train_QA:\", train_qa_s3)\n",
    "print(\"metadata:\", metadata_s3)\n",
    "print(\"embeddings:\", emb_output_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_script = \"wattbot_rag_batch.py\"\n",
    "\n",
    "rag_output_prefix = \"solutions\"\n",
    "rag_output_path = f\"s3://{bucket_name}/{rag_output_prefix}/\"\n",
    "\n",
    "rag_processor = HuggingFaceProcessor(\n",
    "    base_job_name=\"WattBot-rag-batch\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.56\",\n",
    "    pytorch_version=\"2.8\",\n",
    "    py_version=\"py312\",\n",
    "    sagemaker_session=session,\n",
    "    max_runtime_in_seconds=4 * 60 * 60,\n",
    ")\n",
    "\n",
    "rag_processor.run(\n",
    "    code=rag_script,\n",
    "    source_dir=\"code/\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=chunks_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/chunks\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=emb_output_s3,\n",
    "            destination=\"/opt/ml/processing/input/embeddings\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=train_qa_s3,\n",
    "            destination=\"/opt/ml/processing/input/train\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=metadata_s3,\n",
    "            destination=\"/opt/ml/processing/input/metadata\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"solutions\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=rag_output_path,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--input_dir\", \"/opt/ml/processing/input\",\n",
    "        \"--output_dir\", \"/opt/ml/processing/output\",\n",
    "        \"--embedding_model_id\", embedding_model_id,\n",
    "        \"--top_k\", \"8\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"RAG batch job complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff082c",
   "metadata": {},
   "source": [
    "## Step 6 – Download predictions and evaluate\n",
    "\n",
    "Finally, we download `wattbot_solutions.csv` from S3, inspect a few rows, and (optionally) compute the WattBot score against `train_QA.csv` using the `Score.py` logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_key = f\"{rag_output_prefix}/wattbot_solutions.csv\"\n",
    "local_solutions_path = os.path.join(local_data_dir, \"wattbot_solutions.csv\")\n",
    "\n",
    "s3_client.download_file(bucket_name, solutions_key, local_solutions_path)\n",
    "solutions_df = pd.read_csv(local_solutions_path)\n",
    "solutions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"True\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"False\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",   # can also pass \"is_blank\" or None\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,    # can pass \"is_blank\", or leave None to auto\n",
    "    n_examples=10,          # how many incorrect examples to print\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "\n",
    "    NA logic:\n",
    "    - If an explicit NA column is found/used (e.g. is_NA), we use it via _to_bool_flag.\n",
    "    - If you pass gt_is_na_col=\"is_blank\" or pred_is_na_col=\"is_blank\",\n",
    "      we *derive* NA from answer_value == \"is_blank\" instead of expecting a real column.\n",
    "    - If no NA column is available at all, we derive from answer_value == \"is_blank\".\n",
    "\n",
    "    Also prints up to `n_examples` rows where the model is not perfect\n",
    "    (answer_score < 1, ref_id_score < 1, or is_NA_score < 1).\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "\n",
    "    # ----- ground truth NA flags -----\n",
    "    if gt_is_na_col is not None and gt_is_na_col in merged.columns:\n",
    "        # Use explicit column (e.g. \"is_NA\")\n",
    "        gt_is_na_series = merged[gt_is_na_col].map(_to_bool_flag)\n",
    "    elif gt_is_na_col is not None and gt_is_na_col.lower() == \"is_blank\":\n",
    "        # Special meaning: derive NA from answer_value_gt == \"is_blank\"\n",
    "        gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "    else:\n",
    "        # Fallback: if we have is_NA or is_blank col, use it; else derive\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            gt_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            gt_is_na_series = merged[f\"{gt_answer_col}_gt\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"gt_is_blank_flag\"] = gt_is_na_series\n",
    "\n",
    "    # ----- prediction NA flags -----\n",
    "    if pred_is_na_col is not None and pred_is_na_col in merged.columns:\n",
    "        pred_is_na_series = merged[pred_is_na_col].map(_to_bool_flag)\n",
    "    elif pred_is_na_col is not None and pred_is_na_col.lower() == \"is_blank\":\n",
    "        # Same convention: derive from answer_value_pred\n",
    "        pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "        merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "    else:\n",
    "        # Auto-detect or derive if no NA column in preds\n",
    "        if \"is_NA\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_NA\"].map(_to_bool_flag)\n",
    "        elif \"is_blank\" in merged.columns:\n",
    "            pred_is_na_series = merged[\"is_blank\"].map(_to_bool_flag)\n",
    "        else:\n",
    "            pred_is_na_series = merged[f\"{pred_answer_col}_pred\"].astype(str).str.lower().eq(\"is_blank\")\n",
    "            merged[\"pred_is_blank_flag\"] = pred_is_na_series\n",
    "\n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for idx, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = bool(gt_is_na_series.iloc[idx])\n",
    "        pred_is_na = bool(pred_is_na_series.iloc[idx])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    # ----- Show some incorrect examples -----\n",
    "    incorrect = merged[\n",
    "        (merged[\"answer_score\"] < 1.0)\n",
    "        | (merged[\"ref_id_score\"] < 1.0)\n",
    "        | (merged[\"is_NA_score\"] < 1.0)\n",
    "    ]\n",
    "    \n",
    "    if not incorrect.empty and n_examples > 0:\n",
    "        print(\"\\nExamples of incorrect / partially correct responses \"\n",
    "              f\"(up to {n_examples} rows):\\n\")\n",
    "        # Grab up to n_examples \"worst\" rows by wattbot_score\n",
    "        for _, row in incorrect.sort_values(\"wattbot_score\").head(n_examples).iterrows():\n",
    "            q = row[\"question_gt\"] if \"question_gt\" in row.index else None\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"id: {row[id_col]}\")\n",
    "            if q is not None:\n",
    "                print(f\"Question: {q}\")\n",
    "            print(f\"GT answer_value:   {row[f'{gt_answer_col}_gt']}\")\n",
    "            print(f\"Pred answer_value: {row[f'{pred_answer_col}_pred']}\")\n",
    "            print(f\"GT ref_id:         {row[f'{gt_ref_col}_gt']}\")\n",
    "            print(f\"Pred ref_id:       {row[f'{pred_ref_col}_pred']}\")\n",
    "            print(f\"answer_score: {row['answer_score']:.3f}, \"\n",
    "                  f\"ref_id_score: {row['ref_id_score']:.3f}, \"\n",
    "                  f\"is_NA_score: {row['is_NA_score']:.3f}, \"\n",
    "                  f\"wattbot_score: {row['wattbot_score']:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac20cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=\"./data/wattbot_solutions.csv\",\n",
    "    gt_is_na_col=\"is_blank\",\n",
    "    pred_is_na_col=\"is_blank\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec815ae7",
   "metadata": {},
   "source": [
    "::::::::::::::::::::::::::::::::::::: keypoints\n",
    "\n",
    "- TODO\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
