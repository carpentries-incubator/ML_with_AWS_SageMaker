{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d94a92c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"RAG with Processing Jobs: WattBot 2025\"\n",
    "teaching: 30\n",
    "exercises: 20\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd1f6f",
   "metadata": {},
   "source": [
    "\n",
    "# RAG with Processing Jobs: WattBot 2025\n",
    "\n",
    "In the previous episode, we ran the entire WattBot RAG pipeline on a single GPU-backed SageMaker notebook. That was simple to teach, but the GPU sat idle while we downloaded PDFs, chunked text, and evaluated results.\n",
    "\n",
    "In this Episode 2 notebook, we will keep the same WattBot corpus and RAG logic, but restructure how we use AWS:\n",
    "\n",
    "- The notebook itself can run on a small CPU-only instance.\n",
    "- We regenerate pages and chunks locally, as before.\n",
    "- We save the chunks to S3.\n",
    "- We run two short-lived SageMaker Processing jobs on a GPU:\n",
    "  1. One job computes embeddings for all chunks.\n",
    "  2. A second job runs the full RAG loop (retrieval + Qwen) over all training questions.\n",
    "\n",
    "With this approach, we can more effectively use GPU resources only when needed, and we can scale out to larger corpora, models, and hardware more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21e5ba",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48af9455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-east-1\n",
      "Role: arn:aws:iam::183295408236:role/ml-sagemaker-use\n",
      "Bucket: chris-rag\n",
      "Prefix: wattbot-episode-02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role()\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "bucket_name = \"chris-rag\"          # reuse your bucket from Episode 1\n",
    "\n",
    "local_data_dir = \"./data\"\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "corpus_dir = local_data_dir + \"/pdfs/\"\n",
    "os.makedirs(corpus_dir, exist_ok=True)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Bucket:\", bucket_name)\n",
    "print(\"Prefix:\", prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1338f6",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 – Load WattBot metadata and training questions\n",
    "\n",
    "We reuse the same `metadata.csv` and `train_QA.csv` files from Episode 1. If they are not already on the notebook file system, we download them from S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7a377-732c-4fbc-8d31-fc92cd87d216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0eb366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 32\n",
      "Train QAs: 41\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_value</th>\n",
       "      <th>answer_unit</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>ref_url</th>\n",
       "      <th>supporting_materials</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q003</td>\n",
       "      <td>What is the name of the benchmark suite presen...</td>\n",
       "      <td>The ML.ENERGY Benchmark</td>\n",
       "      <td>ML.ENERGY Benchmark</td>\n",
       "      <td>is_blank</td>\n",
       "      <td>['chung2025']</td>\n",
       "      <td>['https://arxiv.org/pdf/2505.06371']</td>\n",
       "      <td>We present the ML.ENERGY Benchmark, a benchmar...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q009</td>\n",
       "      <td>What were the net CO2e emissions from training...</td>\n",
       "      <td>4.3 tCO2e</td>\n",
       "      <td>4.3</td>\n",
       "      <td>tCO2e</td>\n",
       "      <td>['patterson2021']</td>\n",
       "      <td>['https://arxiv.org/pdf/2104.10350']</td>\n",
       "      <td>\"Training GShard-600B used 24 MWh and produced...</td>\n",
       "      <td>Quote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q054</td>\n",
       "      <td>What is the model size in gigabytes (GB) for t...</td>\n",
       "      <td>64.7 GB</td>\n",
       "      <td>64.7</td>\n",
       "      <td>GB</td>\n",
       "      <td>['chen2024']</td>\n",
       "      <td>['https://arxiv.org/pdf/2405.01814']</td>\n",
       "      <td>Table 3: Large language models used for evalua...</td>\n",
       "      <td>Table 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question  \\\n",
       "0  q003  What is the name of the benchmark suite presen...   \n",
       "1  q009  What were the net CO2e emissions from training...   \n",
       "2  q054  What is the model size in gigabytes (GB) for t...   \n",
       "\n",
       "                    answer         answer_value answer_unit  \\\n",
       "0  The ML.ENERGY Benchmark  ML.ENERGY Benchmark    is_blank   \n",
       "1                4.3 tCO2e                  4.3       tCO2e   \n",
       "2                  64.7 GB                 64.7          GB   \n",
       "\n",
       "              ref_id                               ref_url  \\\n",
       "0      ['chung2025']  ['https://arxiv.org/pdf/2505.06371']   \n",
       "1  ['patterson2021']  ['https://arxiv.org/pdf/2104.10350']   \n",
       "2       ['chen2024']  ['https://arxiv.org/pdf/2405.01814']   \n",
       "\n",
       "                                supporting_materials explanation  \n",
       "0  We present the ML.ENERGY Benchmark, a benchmar...       Quote  \n",
       "1  \"Training GShard-600B used 24 MWh and produced...       Quote  \n",
       "2  Table 3: Large language models used for evalua...     Table 3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def smart_read_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "\n",
    "metadata_path = os.path.join(local_data_dir, \"metadata.csv\")\n",
    "train_qa_path = os.path.join(local_data_dir, \"train_QA.csv\")\n",
    "corpus_path = os.path.join(corpus_dir, \"corpus.zip\")\n",
    "\n",
    "if not os.path.exists(metadata_path):\n",
    "    s3_client.download_file(bucket_name, \"metadata.csv\", metadata_path)\n",
    "if not os.path.exists(train_qa_path):\n",
    "    s3_client.download_file(bucket_name, \"train_QA.csv\", train_qa_path)\n",
    "if not os.path.exists(corpus_path):\n",
    "    s3_client.download_file(bucket_name, \"corpus.zip\", corpus_path)\n",
    "\n",
    "metadata_df = smart_read_csv(metadata_path)\n",
    "train_df = smart_read_csv(train_qa_path)\n",
    "\n",
    "print(\"Metadata rows:\", len(metadata_df))\n",
    "print(\"Train QAs:\", len(train_df))\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3242c5c-ccf4-4e83-937e-f0433af4ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(corpus_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(corpus_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddca381-2a08-4b24-b9c9-1aa44dabc36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/pdfs/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d2e3c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 – Regenerate pages and chunks (local)\n",
    "\n",
    "We reuse the same PDF → pages → overlapping chunks pipeline from Episode 1. For clarity, we keep this logic in the notebook so learners can see exactly how context is constructed.\n",
    "\n",
    "Below is a *lightly abbreviated* version of the same code you used previously. If you update the Episode 1 pipeline, you should mirror those changes here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0da90a-32b8-4933-9f80-884ed69036ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pypdf) (4.15.0)\n",
      "Downloading pypdf-6.4.0-py3-none-any.whl (329 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-6.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d0f9f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 639 page-level records from 32 PDFs.\n",
      "Raw pages: 639\n",
      "Chunked docs: 2874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Amazon \\nSustainability \\nReport\\n2023',\n",
       " 'doc_id': 'amazon2023',\n",
       " 'title': '2023 Amazon Sustainability Report',\n",
       " 'url': 'https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf',\n",
       " 'page_num': 0,\n",
       " 'page_label': '1',\n",
       " 'total_pages': 98,\n",
       " 'chunk_idx_in_page': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def pdfs_to_page_docs(metadata: pd.DataFrame, pdf_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load each PDF into a list of page-level dictionaries.\n",
    "\n",
    "    Each dict has keys: text, doc_id, title, url, page_num, page_label, total_pages.\n",
    "    \"\"\"\n",
    "    page_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for _, row in metadata.iterrows():\n",
    "        doc_id = str(row[\"id\"]).strip()\n",
    "        title = str(row.get(\"title\", \"\")).strip()\n",
    "        url = str(row.get(\"url\", \"\")).strip()\n",
    "\n",
    "        pdf_path = os.path.join(pdf_dir, f\"{doc_id}.pdf\")\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"Missing PDF for {doc_id}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {pdf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        total_pages = len(reader.pages)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                text = page.extract_text() or \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract text from {doc_id} page {i}: {e}\")\n",
    "                text = \"\"\n",
    "\n",
    "            text = text.strip()\n",
    "            if not text:\n",
    "                # Still keep the page so we know it exists, but mark it as empty\n",
    "                text = \"[[EMPTY PAGE TEXT – see original PDF for tables/figures]]\"\n",
    "\n",
    "            page_docs.append(\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"page_num\": i,\n",
    "                    \"page_label\": str(i + 1),\n",
    "                    \"total_pages\": total_pages,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return page_docs\n",
    "\n",
    "\n",
    "page_docs = pdfs_to_page_docs(metadata_df, corpus_dir)\n",
    "print(f\"Loaded {len(page_docs)} page-level records from {len(metadata_df)} PDFs.\")\n",
    "page_docs[0] if page_docs else None\n",
    "\n",
    "\n",
    "def split_text_into_chunks(\n",
    "    text: str,\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[str]:\n",
    "    \"\"\"Split `text` into overlapping character-based chunks.\n",
    "\n",
    "    This is a simple baseline; more advanced versions might:\n",
    "    - split on sentence boundaries, or\n",
    "    - merge short paragraphs and respect section headings.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    while start < text_len:\n",
    "        end = min(start + chunk_size_chars, text_len)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == text_len:\n",
    "            break\n",
    "        # Move the window forward, keeping some overlap\n",
    "        start = end - chunk_overlap_chars\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def make_chunked_docs(\n",
    "    page_docs: List[Dict[str, Any]],\n",
    "    chunk_size_chars: int = 1200,\n",
    "    chunk_overlap_chars: int = 200,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Turn page-level records into smaller overlapping text chunks.\n",
    "\n",
    "    Each chunk keeps a pointer back to its document and page metadata.\n",
    "    \"\"\"\n",
    "    chunked: List[Dict[str, Any]] = []\n",
    "    for page in page_docs:\n",
    "        page_text = page[\"text\"]\n",
    "        chunks = split_text_into_chunks(\n",
    "            page_text,\n",
    "            chunk_size_chars=chunk_size_chars,\n",
    "            chunk_overlap_chars=chunk_overlap_chars,\n",
    "        )\n",
    "        for idx, chunk_text in enumerate(chunks):\n",
    "            chunked.append(\n",
    "                {\n",
    "                    \"text\": chunk_text,\n",
    "                    \"doc_id\": page[\"doc_id\"],\n",
    "                    \"title\": page[\"title\"],\n",
    "                    \"url\": page[\"url\"],\n",
    "                    \"page_num\": page[\"page_num\"],\n",
    "                    \"page_label\": page[\"page_label\"],\n",
    "                    \"total_pages\": page[\"total_pages\"],\n",
    "                    \"chunk_idx_in_page\": idx,\n",
    "                }\n",
    "            )\n",
    "    return chunked\n",
    "\n",
    "\n",
    "chunked_docs = make_chunked_docs(page_docs)\n",
    "print(\"Raw pages:\", len(page_docs))\n",
    "print(\"Chunked docs:\", len(chunked_docs))\n",
    "chunked_docs[0] if chunked_docs else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91323aa",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3 – Serialize chunks to JSONL and upload to S3\n",
    "\n",
    "The Processing jobs will not have access to your Python variables. Instead, we serialize `chunked_docs` to `wattbot_chunks.jsonl` and upload it to S3 under this episode's prefix.\n",
    "\n",
    "Each line is one JSON object representing a chunk, including its text and metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "580c782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2874 chunks to ./data/wattbot_chunks.jsonl\n",
      "Chunks JSONL in S3: s3://chris-rag/wattbot_chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks_jsonl_path = os.path.join(local_data_dir, \"wattbot_chunks.jsonl\")\n",
    "\n",
    "with open(chunks_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ch in chunked_docs:\n",
    "        f.write(json.dumps(ch, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote {len(chunked_docs)} chunks to {chunks_jsonl_path}\")\n",
    "\n",
    "chunks_key = \"wattbot_chunks.jsonl\"\n",
    "s3_client.upload_file(chunks_jsonl_path, bucket_name, chunks_key)\n",
    "\n",
    "chunks_s3_uri = f\"s3://{bucket_name}/{chunks_key}\"\n",
    "print(\"Chunks JSONL in S3:\", chunks_s3_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7283ccf",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4 – Processing Job 1: embed all chunks on a GPU\n",
    "\n",
    "Now we launch a short-lived Hugging Face **Processing job** that:\n",
    "\n",
    "1. Downloads `wattbot_chunks.jsonl` from S3.\n",
    "2. Loads `thenlper/gte-large` from Hugging Face.\n",
    "3. Encodes each chunk into an embedding vector.\n",
    "4. Saves the full matrix as `embeddings.npy` back to S3.\n",
    "\n",
    "We use the same `embedding_inference.py` script across projects; here it expects a JSONL file with a `text` field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef120f-b492-4bdd-96f8-26f54ff7a945",
   "metadata": {},
   "source": [
    "### But first...\n",
    "we have to create a requirements.txt file that will add additional libraries to the HuggingFaceProcessor we use below, which builds the environment we'll run our embedding_inference.py script in. For the processing job to recognize this dependence, we'll add it to the source_dir (code/) referenced when we call embedding_processor.run() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d721b-521b-4717-bdde-d30f01a671a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [\n",
    "    \"sentence-transformers\",\n",
    "    # add more packages here if needed\n",
    "]\n",
    "\n",
    "req_path = \"code/requirements.txt\"\n",
    "with open(req_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(requirements))\n",
    "\n",
    "print(f\"Created requirements.txt at {req_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51ce6a-2019-434d-8313-560a51693086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bdfa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.processing:Uploaded code/ to s3://sagemaker-us-east-1-183295408236/WattBot-embed-gte-large-2025-11-26-04-19-52-637/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-183295408236/WattBot-embed-gte-large-2025-11-26-04-19-52-637/source/runproc.sh\n",
      "INFO:sagemaker:Creating processing-job with name WattBot-embed-gte-large-2025-11-26-04-19-52-637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................\u001b[34mCodeArtifact repository not specified. Skipping login.\u001b[0m\n",
      "\u001b[34mFound existing installation: typing 3.7.4.3\u001b[0m\n",
      "\u001b[34mUninstalling typing-3.7.4.3:\n",
      "  Successfully uninstalled typing-3.7.4.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34mCollecting sentence_transformers (from -r requirements.txt (line 1))\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (4.56.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (4.67.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (2.8.0+cu129)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (1.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (0.35.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (11.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/site-packages (from sentence_transformers->-r requirements.txt (line 1)) (4.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (2025.9.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (2.32.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (0.22.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (0.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirements.txt (line 1)) (2025.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers->-r requirements.txt (line 1)) (1.1.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (80.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (1.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (3.1.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.9.86)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.9.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.9.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (9.10.2.21)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.9.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (11.4.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (10.3.10.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (11.7.5.82)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.5.10.65)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (0.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (2.27.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.9.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (12.9.86)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (1.14.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 1)) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (3.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (3.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (2.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 1)) (2025.8.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 1)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 1)) (3.6.0)\u001b[0m\n",
      "\u001b[34mDownloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentence_transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed sentence_transformers-5.1.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.2 -> 25.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\u001b[0m\n",
      "\u001b[34mLoaded 2874 chunks\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_model_id = \"thenlper/gte-large\"\n",
    "script_path = \"embedding_inference.py\"\n",
    "\n",
    "emb_output_prefix = \"embeddings\"\n",
    "emb_output_path = f\"s3://{bucket_name}/{emb_output_prefix}/\"\n",
    "\n",
    "embedding_processor = HuggingFaceProcessor(\n",
    "    base_job_name=\"WattBot-embed-gte-large\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.56\",\n",
    "    pytorch_version=\"2.8\",\n",
    "    py_version=\"py312\",\n",
    "    sagemaker_session=session,\n",
    "    max_runtime_in_seconds=2 * 60 * 60,\n",
    ")\n",
    "\n",
    "embedding_processor.run(\n",
    "    code=script_path,\n",
    "    source_dir=\"code/\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=chunks_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"embeddings\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=emb_output_path,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--model_id\", embedding_model_id,\n",
    "        \"--input_filename\", \"wattbot_chunks.jsonl\",\n",
    "        \"--text_key\", \"text\",\n",
    "        \"--input_dir\", \"/opt/ml/processing/input\",\n",
    "        \"--output_dir\", \"/opt/ml/processing/output\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Embedding job complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56b5f9",
   "metadata": {},
   "source": [
    "\n",
    "### Sanity-check the embeddings locally\n",
    "\n",
    "We can download `embeddings.npy` back into the notebook and inspect its shape to confirm the job ran successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369db985",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_embeddings_path = os.path.join(local_data_dir, \"embeddings.npy\")\n",
    "embeddings_key = f\"{emb_output_prefix}/embeddings.npy\"\n",
    "\n",
    "s3_client.download_file(bucket_name, embeddings_key, local_embeddings_path)\n",
    "chunk_embeddings = np.load(local_embeddings_path)\n",
    "\n",
    "print(\"Embeddings shape:\", chunk_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65fdd3",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5 – Processing Job 2: full WattBot RAG over all questions\n",
    "\n",
    "For the second job, we pass four inputs:\n",
    "\n",
    "- `wattbot_chunks.jsonl` – serialized chunks\n",
    "- `embeddings.npy` – precomputed chunk embeddings\n",
    "- `train_QA.csv` – training questions (to compare with ground truth)\n",
    "- `metadata.csv` – to resolve `ref_id` → URL\n",
    "\n",
    "The script `wattbot_rag_batch.py` reuses the RAG helpers from Episode 1:\n",
    "\n",
    "- cosine similarity + `retrieve_top_k`\n",
    "- `retrieve_context_for_question`\n",
    "- `answer_phase_for_question` (Qwen answer, answer_value, ref_ids, is_blank)\n",
    "- `explanation_phase_for_question`\n",
    "- `run_single_qa` (hybrid unanswerable logic: retrieval threshold + LLM is_blank)\n",
    "\n",
    "The job writes out `wattbot_solutions.csv` in the WattBot submission format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8719c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upload CSVs so the job can read them\n",
    "train_qa_key = \"train_QA.csv\"\n",
    "metadata_key = \"metadata.csv\"\n",
    "\n",
    "train_qa_s3 = f\"s3://{bucket_name}/{train_qa_key}\"\n",
    "metadata_s3 = f\"s3://{bucket_name}/{metadata_key}\"\n",
    "emb_output_s3 = f\"s3://{bucket_name}/{emb_output_prefix}/embeddings.npy\"\n",
    "\n",
    "print(\"train_QA:\", train_qa_s3)\n",
    "print(\"metadata:\", metadata_s3)\n",
    "print(\"embeddings:\", emb_output_s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_script = \"wattbot_rag_batch.py\"\n",
    "\n",
    "rag_output_prefix = \"solutions\"\n",
    "rag_output_path = f\"s3://{bucket_name}/{rag_output_prefix}/\"\n",
    "\n",
    "rag_processor = HuggingFaceProcessor(\n",
    "    base_job_name=\"WattBot-rag-batch\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    instance_count=1,\n",
    "    transformers_version=\"4.56\",\n",
    "    pytorch_version=\"2.8\",\n",
    "    py_version=\"py312\",\n",
    "    sagemaker_session=session,\n",
    "    max_runtime_in_seconds=4 * 60 * 60,\n",
    ")\n",
    "\n",
    "rag_processor.run(\n",
    "    code=rag_script,\n",
    "    source_dir=\"code/\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=chunks_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=emb_output_s3,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=train_qa_s3,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=metadata_s3,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"solutions\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=rag_output_path,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--input_dir\", \"/opt/ml/processing/input\",\n",
    "        \"--output_dir\", \"/opt/ml/processing/output\",\n",
    "        \"--embedding_model_id\", embedding_model_id,\n",
    "        \"--top_k\", \"8\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"RAG batch job complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e017f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6 – Download predictions and evaluate\n",
    "\n",
    "Finally, we download `wattbot_solutions.csv` from S3, inspect a few rows, and (optionally) compute the WattBot score against `train_QA.csv` using the `Score.py` logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solutions_key = f\"{rag_output_prefix}/wattbot_solutions.csv\"\n",
    "local_solutions_path = os.path.join(local_data_dir, \"wattbot_solutions.csv\")\n",
    "\n",
    "s3_client.download_file(bucket_name, solutions_key, local_solutions_path)\n",
    "solutions_df = pd.read_csv(local_solutions_path)\n",
    "solutions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _to_bool_flag(x):\n",
    "    \"\"\"Convert typical truthy/falsey strings to bool.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        if s in {\"1\", \"true\", \"yes\"}:\n",
    "            return True\n",
    "        if s in {\"0\", \"false\", \"no\"}:\n",
    "            return False\n",
    "    return bool(x)\n",
    "\n",
    "def _parse_float_or_none(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _answer_value_correct(gt_val, pred_val, rel_tol=1e-3):\n",
    "    \"\"\"\n",
    "    gt_val, pred_val: values from answer_value columns.\n",
    "    rel_tol = 0.001 => 0.1% relative tolerance.\n",
    "    \"\"\"\n",
    "    # Normalize to string for special tokens\n",
    "    gt_str = str(gt_val).strip()\n",
    "    pred_str = str(pred_val).strip()\n",
    "    \n",
    "    # If either is 'is_blank', treat as categorical\n",
    "    if gt_str.lower() == \"is_blank\" or pred_str.lower() == \"is_blank\":\n",
    "        return gt_str.lower() == pred_str.lower()\n",
    "    \n",
    "    gt_num = _parse_float_or_none(gt_val)\n",
    "    pred_num = _parse_float_or_none(pred_val)\n",
    "    \n",
    "    # If both numeric, use relative tolerance\n",
    "    if gt_num is not None and pred_num is not None:\n",
    "        if gt_num == 0:\n",
    "            return abs(pred_num - gt_num) <= rel_tol  # small absolute tolerance around 0\n",
    "        rel_err = abs(pred_num - gt_num) / max(abs(gt_num), 1e-12)\n",
    "        return rel_err <= rel_tol\n",
    "    \n",
    "    # Otherwise, fall back to normalized string match\n",
    "    return gt_str.lower() == pred_str.lower()\n",
    "\n",
    "def _ref_id_jaccard(gt_ref, pred_ref):\n",
    "    \"\"\"\n",
    "    Jaccard overlap between sets of ref_ids.\n",
    "    Strings may contain semicolon-separated IDs, or 'is_blank'.\n",
    "    Case-insensitive.\n",
    "    \"\"\"\n",
    "    def to_set(s):\n",
    "        if s is None:\n",
    "            return set()\n",
    "        s = str(s).strip()\n",
    "        if not s or s.lower() == \"is_blank\":\n",
    "            return set()\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        return set(parts)\n",
    "    \n",
    "    gt_set = to_set(gt_ref)\n",
    "    pred_set = to_set(pred_ref)\n",
    "    \n",
    "    if not gt_set and not pred_set:\n",
    "        return 1.0\n",
    "    union = gt_set | pred_set\n",
    "    if not union:\n",
    "        return 0.0\n",
    "    inter = gt_set & pred_set\n",
    "    return len(inter) / len(union)\n",
    "\n",
    "def compute_wattbot_score(\n",
    "    train_qa_path=\"train_QA.csv\",\n",
    "    preds_path=\"train_solutions_qwen.csv\",\n",
    "    id_col=\"id\",\n",
    "    gt_answer_col=\"answer_value\",\n",
    "    gt_ref_col=\"ref_id\",\n",
    "    gt_is_na_col=\"is_NA\",\n",
    "    pred_answer_col=\"answer_value\",\n",
    "    pred_ref_col=\"ref_id\",\n",
    "    pred_is_na_col=None,   # if None, will auto-detect is_NA or is_blank\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare your solutions to train_QA.csv using a WattBot-style score.\n",
    "    Returns a DataFrame with per-row scores and prints summary stats.\n",
    "    \"\"\"\n",
    "    gt = pd.read_csv(train_qa_path)\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    \n",
    "    # If predictions column name for NA isn't given, auto-detect\n",
    "    if pred_is_na_col is None:\n",
    "        if \"is_NA\" in preds.columns:\n",
    "            pred_is_na_col = \"is_NA\"\n",
    "        elif \"is_blank\" in preds.columns:\n",
    "            pred_is_na_col = \"is_blank\"\n",
    "        else:\n",
    "            raise ValueError(\"Could not find is_NA or is_blank column in predictions.\")\n",
    "    \n",
    "    # Inner join on id to be strict\n",
    "    merged = gt.merge(preds, on=id_col, suffixes=(\"_gt\", \"_pred\"))\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"No overlapping ids between ground truth and predictions.\")\n",
    "    \n",
    "    ans_scores = []\n",
    "    ref_scores = []\n",
    "    na_scores = []\n",
    "    \n",
    "    for _, row in merged.iterrows():\n",
    "        gt_ans = row[f\"{gt_answer_col}_gt\"]\n",
    "        pred_ans = row[f\"{pred_answer_col}_pred\"]\n",
    "        gt_ref = row[f\"{gt_ref_col}_gt\"]\n",
    "        pred_ref = row[f\"{pred_ref_col}_pred\"]\n",
    "        \n",
    "        gt_is_na = _to_bool_flag(row[gt_is_na_col])\n",
    "        pred_is_na = _to_bool_flag(row[pred_is_na_col])\n",
    "        \n",
    "        # 1. answer_value component\n",
    "        ans_correct = _answer_value_correct(gt_ans, pred_ans)\n",
    "        ans_scores.append(1.0 * ans_correct)\n",
    "        \n",
    "        # 2. ref_id Jaccard\n",
    "        ref_j = _ref_id_jaccard(gt_ref, pred_ref)\n",
    "        ref_scores.append(ref_j)\n",
    "        \n",
    "        # 3. is_NA component (simple: must match ground truth flag)\n",
    "        na_scores.append(1.0 if gt_is_na == pred_is_na else 0.0)\n",
    "    \n",
    "    merged[\"answer_score\"] = ans_scores\n",
    "    merged[\"ref_id_score\"] = ref_scores\n",
    "    merged[\"is_NA_score\"] = na_scores\n",
    "    \n",
    "    merged[\"wattbot_score\"] = (\n",
    "        0.75 * merged[\"answer_score\"]\n",
    "        + 0.15 * merged[\"ref_id_score\"]\n",
    "        + 0.10 * merged[\"is_NA_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Rows compared: {len(merged)}\")\n",
    "    print(f\"Mean answer_value score: {merged['answer_score'].mean():.4f}\")\n",
    "    print(f\"Mean ref_id score:       {merged['ref_id_score'].mean():.4f}\")\n",
    "    print(f\"Mean is_NA score:        {merged['is_NA_score'].mean():.4f}\")\n",
    "    print(f\"Overall WattBot score:   {merged['wattbot_score'].mean():.4f}\")\n",
    "    \n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411359ce-c707-4c09-8699-639b859a5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = compute_wattbot_score(\n",
    "    train_qa_path=\"./data/train_QA.csv\",\n",
    "    preds_path=\"./data/train_solutions_qwen.csv\",\n",
    "    gt_is_na_col=\"is_blank\",\n",
    "    pred_is_na_col=\"is_blank\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
