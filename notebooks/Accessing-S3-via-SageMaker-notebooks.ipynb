{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bb306e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Accessing and Managing Data in S3 with SageMaker Notebooks\"\n",
    "teaching: 20\n",
    "exercises: 10\n",
    "---\n",
    "\n",
    ":::::::::::::::::::::::::::::::::::::: questions \n",
    "\n",
    "- How can I load data from S3 into a SageMaker notebook?\n",
    "- How do I monitor storage usage and costs for my S3 bucket?\n",
    "- What steps are involved in pushing new data back to S3 from a notebook?\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::: objectives\n",
    "\n",
    "- Read data directly from an S3 bucket into memory in a SageMaker notebook.\n",
    "- Check storage usage and estimate costs for data in an S3 bucket.\n",
    "- Upload new files from the SageMaker environment back to the S3 bucket.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::\n",
    "\n",
    "#### Set up AWS environment\n",
    "To begin each notebook, it's important to set up an AWS environment that will allow seamless access to the necessary cloud resources. Here's what we'll do to get started:\n",
    "\n",
    "1. **Define the Role**: We'll use `get_execution_role()` to retrieve the IAM role associated with the SageMaker instance. This role specifies the permissions needed for interacting with AWS services like S3, which allows SageMaker to securely read from and write to storage buckets.\n",
    "\n",
    "2. **Initialize the SageMaker Session**: Next, we'll create a `sagemaker.Session()` object, which will help manage and track the resources and operations we use in SageMaker, such as training jobs and model artifacts. The session acts as a bridge between the SageMaker SDK commands in our notebook and AWS services.\n",
    "\n",
    "3. **Set Up an S3 Client using boto3**: Using `boto3`, we'll initialize an S3 client for accessing S3 buckets directly. Boto3 is the official AWS SDK for Python, allowing developers to interact programmatically with AWS services like S3, EC2, and Lambda.\n",
    "\n",
    "Starting with these initializations prepares our notebook environment to efficiently interact with AWS resources for model development, data management, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c79f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Initialize the SageMaker role, session, and s3 client\n",
    "role = sagemaker.get_execution_role() # specifies your permissions to use AWS tools\n",
    "session = sagemaker.Session() \n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08422c38",
   "metadata": {},
   "source": [
    "Preview variable details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adac578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print relevant details \n",
    "print(f\"Execution Role: {role}\")  # Displays the IAM role being used\n",
    "bucket_names = [bucket[\"Name\"] for bucket in s3.list_buckets()[\"Buckets\"]]\n",
    "print(f\"Available S3 Buckets: {bucket_names}\")  # Shows the default S3 bucket assigned to SageMaker\n",
    "print(f\"AWS Region: {session.boto_region_name}\")  # Prints the region where the SageMaker session is running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9cb6a",
   "metadata": {},
   "source": [
    "## Reading data from S3\n",
    "\n",
    "You can either (A) read data from S3 into memory or (B) download a copy of your S3 data into your notebook instance. Since we are using SageMaker notebooks as controllers—rather than performing training or tuning directly in the notebook—the best practice is to **read data directly from S3** whenever possible. However, there are cases where downloading a local copy may be useful. We'll show you both strategies.\n",
    "\n",
    "### A) Reading data directly from S3 into memory  \n",
    "This is the recommended approach for most workflows. By keeping data in S3 and reading it into memory when needed, we avoid local storage constraints and ensure that our data remains accessible for SageMaker training and tuning jobs.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- **Scalability**: Data remains in S3, allowing multiple training/tuning jobs to access it without duplication.\n",
    "- **Efficiency**: No need to manage local copies or manually clean up storage.\n",
    "- **Cost-effective**: Avoids unnecessary instance storage usage.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- **Network dependency**: Requires internet access to S3.\n",
    "- **Potential latency**: Reading large datasets repeatedly from S3 may introduce small delays. This approach works best if you only need to load data once or infrequently.\n",
    "\n",
    "#### Example: Reading data from S3 into memory\n",
    "Our data is stored on an S3 bucket called 'teamname-name-dataname' (e.g., sinkorswim-doejohn-titanic). We can use the following code to read data directly from S3 into memory in the Jupyter notebook environment, without actually downloading a copy of train.csv as a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Define the S3 bucket and object key\n",
    "bucket_name = 'sinkorswim-doejohn-titanic'  # replace with your S3 bucket name\n",
    "\n",
    "# Read the train data from S3\n",
    "key = 'titanic_train.csv'  # replace with your object key\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "train_data = pd.read_csv(response['Body'])\n",
    "\n",
    "# Read the test data from S3\n",
    "key = 'titanic_test.csv'  # replace with your object key\n",
    "response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "test_data = pd.read_csv(response['Body'])\n",
    "\n",
    "# check shape\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "# Inspect the first few rows of the DataFrame\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82e678",
   "metadata": {},
   "source": [
    "### B) Download copy into notebook environment\n",
    "In some cases, downloading a local copy of the dataset may be useful, such as when performing repeated reads in an interactive notebook session.\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- **Faster access for repeated operations**: Avoids repeated S3 requests.\n",
    "- **Works offline**: Useful if running in an environment with limited network access.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- **Consumes instance storage**: Notebook instances have limited space.\n",
    "- **Requires manual cleanup**: Downloaded files remain until deleted.\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e817f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b4e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket and file location\n",
    "key = \"titanic_train.csv\"  # Path to your file in the S3 bucket\n",
    "local_file_path = \"/home/ec2-user/SageMaker/titanic_train.csv\"  # Local path to save the file\n",
    "\n",
    "# Initialize the S3 client and download the file\n",
    "s3.download_file(bucket_name, key, local_file_path)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613b15b",
   "metadata": {},
   "source": [
    "**Note**: You may need to hit refresh on the file explorer panel to the left to see this file. If you get any permission issues...\n",
    "\n",
    "* check that you have selected the appropriate policy for this notebook\n",
    "* check that your bucket has the appropriate policy permissions\n",
    "\n",
    "#### Check the current size and storage costs of bucket\n",
    "It's a good idea to periodically check how much storage you have used in your bucket. You can do this from a Jupyter notebook in SageMaker by using the **Boto3** library, which is the AWS SDK for Python. This will allow you to calculate the total size of objects within a specified bucket. \n",
    "\n",
    "The code below will calculate your bucket size for you. Here is a breakdown of the important pieces in the next code section:\n",
    "\n",
    "1. **Paginator**: Since S3 buckets can contain many objects, we use a paginator to handle large listings.\n",
    "2. **Size calculation**: We sum the `Size` attribute of each object in the bucket.\n",
    "3. **Unit conversion**: The size is given in bytes, so dividing by `1024 ** 2` converts it to megabytes (MB).\n",
    "\n",
    "> **Note**: If your bucket has very large objects or you want to check specific folders within a bucket, you may want to refine this code to only fetch certain objects or folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the total size counter (bytes)\n",
    "total_size_bytes = 0\n",
    "\n",
    "# Use a paginator to handle large bucket listings\n",
    "# This ensures that even if the bucket contains many objects, we can retrieve all of them\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "# Iterate through all pages of object listings\n",
    "for page in paginator.paginate(Bucket=bucket_name):\n",
    "    # 'Contents' contains the list of objects in the current page, if available\n",
    "    for obj in page.get(\"Contents\", []):  \n",
    "        total_size_bytes += obj[\"Size\"]  # Add each object's size to the total\n",
    "\n",
    "# Convert the total size to gigabytes for cost estimation\n",
    "total_size_gb = total_size_bytes / (1024 ** 3)\n",
    "\n",
    "# Convert the total size to megabytes for easier readability\n",
    "total_size_mb = total_size_bytes / (1024 ** 2)\n",
    "\n",
    "# Print the total size in MB\n",
    "print(f\"Total size of bucket '{bucket_name}': {total_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the total size in GB\n",
    "#print(f\"Total size of bucket '{bucket_name}': {total_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfd3a7",
   "metadata": {},
   "source": [
    "### Using helper functions from lesson repo\n",
    "We have added code to calculate bucket size to a helper function called `get_s3_bucket_size(bucket_name)` for your convenience. There are also some other helper functions in that repo to assist you with common AWS/SageMaker workflows. We'll show you how to clone this code into your notebook environment.\n",
    "\n",
    "**Note**: If you haven't already, make sure to fork the AWS_helpers repo as described on the [setup page](https://uw-madison-datascience.github.io/ML_with_Amazon_SageMaker/#workshop-repository-setup). Replace \"username\" below with your GitHub username.\n",
    "\n",
    "#### Directory setup\n",
    "Let's make sure we're starting in the root directory of this instance, so that we all have our AWS_helpers.py file located in the same path (/test_AWS/scripts/AWS_helpers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4984e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2e796",
   "metadata": {},
   "source": [
    "**Before cloning, visit your fork to make sure there aren't any changes that need to be synced**.\n",
    "\n",
    "Adjust the following path to find your fork : [`https://github.com/username/AWS_helpers.git](https://github.com/username/AWS_helpers.git). If you still need to fork the original repo, visit [https://github.com/UW-Madison-DataScience/AWS_helpers](https://github.com/UW-Madison-DataScience/AWS_helpers) and create a fork.\n",
    "\n",
    "To clone the repo to our Jupyter notebook, use the following code, adjusting username to your GitHub username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79577de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/username/AWS_helpers.git # downloads AWS_helpers folder/repo (refresh file explorer to see)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7be5cc",
   "metadata": {},
   "source": [
    "Our AWS_helpers.py file can be found in `AWS_helpers/helpers.py`. With this file downloaded, you can call this function via..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a930cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AWS_helpers.helpers as helpers\n",
    "helpers.get_s3_bucket_size(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a885e",
   "metadata": {},
   "source": [
    "### Check storage costs of bucket\n",
    "To estimate the storage cost of your Amazon S3 bucket directly from a Jupyter notebook in SageMaker, you can use the following approach. This method calculates the total size of the bucket and estimates the monthly storage cost based on AWS S3 pricing.\n",
    "\n",
    "**Note**: AWS S3 pricing varies by region and storage class. The example below uses the S3 Standard storage class pricing for the US East (N. Virginia) region as of November 1, 2024. Please verify the current pricing for your specific region and storage class on the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 Standard Storage pricing for US East (N. Virginia) region\n",
    "# Pricing tiers as of November 1, 2024\n",
    "first_50_tb_price_per_gb = 0.023  # per GB for the first 50 TB\n",
    "next_450_tb_price_per_gb = 0.022  # per GB for the next 450 TB\n",
    "over_500_tb_price_per_gb = 0.021  # per GB for storage over 500 TB\n",
    "\n",
    "# Calculate the cost based on the size\n",
    "if total_size_gb <= 50 * 1024:\n",
    "    # Total size is within the first 50 TB\n",
    "    cost = total_size_gb * first_50_tb_price_per_gb\n",
    "elif total_size_gb <= 500 * 1024:\n",
    "    # Total size is within the next 450 TB\n",
    "    cost = (50 * 1024 * first_50_tb_price_per_gb) + \\\n",
    "           ((total_size_gb - 50 * 1024) * next_450_tb_price_per_gb)\n",
    "else:\n",
    "    # Total size is over 500 TB\n",
    "    cost = (50 * 1024 * first_50_tb_price_per_gb) + \\\n",
    "           (450 * 1024 * next_450_tb_price_per_gb) + \\\n",
    "           ((total_size_gb - 500 * 1024) * over_500_tb_price_per_gb)\n",
    "\n",
    "print(f\"Estimated monthly storage cost: ${cost:.5f}\")\n",
    "print(f\"Estimated annual storage cost: ${cost*12:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b75fb6",
   "metadata": {},
   "source": [
    "For your convenience, we have also added this code to a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2929739",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_cost, storage_size_gb = helpers.calculate_s3_storage_cost(bucket_name)\n",
    "print(f\"Estimated monthly cost ({storage_size_gb:.4f} GB): ${monthly_cost:.5f}\")\n",
    "print(f\"Estimated annual cost ({storage_size_gb:.4f} GB): ${monthly_cost*12:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a203e",
   "metadata": {},
   "source": [
    "**Important Considerations**:\n",
    "\n",
    "- **Pricing Tiers**: AWS S3 pricing is tiered. The first 50 TB per month is priced at `$0.023 per GB`, the next 450 TB at `$0.022 per GB`, and storage over 500 TB at `$0.021 per GB`. Ensure you apply the correct pricing tier based on your total storage size.\n",
    "- **Region and Storage Class**: Pricing varies by AWS region and storage class. The example above uses the S3 Standard storage class pricing for the US East (N. Virginia) region. Adjust the pricing variables if your bucket is in a different region or uses a different storage class.\n",
    "- **Additional Costs**: This estimation covers storage costs only. AWS S3 may have additional charges for requests, data retrievals, and data transfers. For a comprehensive cost analysis, consider these factors as well.\n",
    "\n",
    "For detailed and up-to-date information on AWS S3 pricing, please refer to the [AWS S3 Pricing page](https://aws.amazon.com/s3/pricing/).\n",
    "\n",
    "## Writing output files to S3\n",
    "As your analysis generates new files or demands additional documentation, you can upload files to your bucket as demonstrated below. For this demo, you can create a blank `Notes.txt` file to upload to your bucket. To do so, go to **File** -> **New** -> **Text file**, and save it out as `Notes.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket name and the file paths\n",
    "notes_file_path = \"Notes.txt\" # assuming your file is in root directory of jupyter notebook (check file explorer tab)\n",
    "\n",
    "# Upload the training file to a new folder called \"docs\". You can also just place it in the bucket's root directory if you prefer (remove docs/ in code below).\n",
    "s3.upload_file(notes_file_path, bucket_name, \"docs/Notes.txt\")\n",
    "\n",
    "print(\"Files uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b647d",
   "metadata": {},
   "source": [
    "After uploading, we can view the objects/files available on our bucket using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and print all objects in the bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "# Check if there are objects in the bucket\n",
    "if 'Contents' in response:\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])  # Print the object's key (its path in the bucket)\n",
    "else:\n",
    "    print(\"The bucket is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c44d53",
   "metadata": {},
   "source": [
    "Alternatively, we can substitute this for a helper function call as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf209945",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = helpers.list_S3_objects(bucket_name)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985cbd8",
   "metadata": {},
   "source": [
    ":::::::::::::::::::::::::::::::::::::: keypoints \n",
    "\n",
    "- Load data from S3 into memory for efficient storage and processing.\n",
    "- Periodically check storage usage and costs to manage S3 budgets.\n",
    "- Use SageMaker to upload analysis results and maintain an organized workflow.\n",
    "\n",
    "::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
