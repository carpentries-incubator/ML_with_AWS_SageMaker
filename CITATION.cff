cff-version: 1.2.0
title: Intro to AWS SageMaker for Predictive ML/AI
message: >-
  Please cite this lesson using the information in this file
  when you refer to it in publications, and/or if you
  re-use, adapt, or expand on the content in your own
  training material.
type: dataset
authors:
  - given-names: Christopher
    family-names: Endemann
    email: endemann@wisc.edu
    affiliation: University of Wisconsin-Madison
    orcid: 'https://orcid.org/0000-0002-7357-6129'
repository-code: >-
  https://github.com/carpentries-incubator/ML_with_AWS_SageMaker
url: >-
  https://carpentries-incubator.github.io/ML_with_AWS_SageMaker/index.html
abstract: >-
  This workshop introduces foundational workflows in AWS
  SageMaker, focusing on data setup, repository management,
  model training, and hyperparameter tuning within AWS's
  managed environment. Participants will learn to use
  SageMaker notebooks to orchestrate data pipelines, launch
  training and tuning jobs, and assess model performance.
  The session will also cover strategies for scaling ML
  workflows efficiently, including guidance on selecting
  between CPU and GPU instances and leveraging parallelized
  workflows across multiple instances. To support
  cost-effective experimentation, the workshop provides best
  practices for tracking and managing AWS expenses. While
  AWS usage incurs costs, it can be an affordable solution
  for research-oriented ML workflows. For example, training
  approximately 100 small to medium-sized models (e.g.,
  logistic regression, random forests, or lightweight deep
  learning models with a few million parameters) on datasets
  under 10GB can often be achieved for under $20. This
  workshop is designed for researchers and practitioners
  looking to implement scalable, cost-conscious ML workflows
  using cloud-based infrastructure.
keywords:
  - AWS
  - Cloud Computing
  - Machine Learning
  - Artificial Intelligence
  - SageMaker
  - GPU
  - Open-Source
  - Carpentries Incubator
  - Python
license: CC-BY-4.0
